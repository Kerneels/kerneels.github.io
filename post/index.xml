<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Good Fast</title>
    <link>https://blog.goodfast.info/post/</link>
    <description>Recent content in Posts on Good Fast</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Jan 2021 21:26:20 +0000</lastBuildDate>
    <atom:link href="https://blog.goodfast.info/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Finding what you are looking for, faster</title>
      <link>https://blog.goodfast.info/post/find-differences-fast/</link>
      <pubDate>Sun, 31 Jan 2021 21:26:20 +0000</pubDate>
      
      <guid>https://blog.goodfast.info/post/find-differences-fast/</guid>
      <description>

&lt;p&gt;Sometimes we need to  show that the exact same data is produced by two different processes.
We might be replacing one implementation of an algorithm with a more efficient one, or we might be running the same algorithm on a different technology.
We want to  show that the data in &lt;code&gt;table_a&lt;/code&gt; and &lt;code&gt;table_b&lt;/code&gt; is identical, or if it is not identical, we want to identify all differences.
In this article, I want to share with you a method that helped me do this.
SQL, being set-based, help us do this fairly elegantly, but getting to the bottom of what causes
the differences requires a bit of careful and systematic work on our part too.&lt;/p&gt;

&lt;h1 id=&#34;comparing-on-two-dimensions:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Comparing on two dimensions&lt;/h1&gt;

&lt;p&gt;Assuming that the &amp;ldquo;shape&amp;rdquo; or schema of the two  tables is equivalent (see notes below on how to determine this),
we can consider how similar the contained data is by looking at the vertical (rows), and horizontal (columns) dimensions.&lt;/p&gt;

&lt;h1 id=&#34;vertical-or-row-difference:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Vertical, or row difference&lt;/h1&gt;

&lt;p&gt;these possibilities exist:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;full disjunction : &lt;code&gt;table_a&lt;/code&gt; and &lt;code&gt;table_b&lt;/code&gt; differ completely and do not overlap at all&lt;/li&gt;
&lt;li&gt;non empty union : &lt;code&gt;table_a&lt;/code&gt; and &lt;code&gt;table_b&lt;/code&gt; partially overlap, but differ in all other rows&lt;/li&gt;
&lt;li&gt;&lt;code&gt;table_a&lt;/code&gt; is entirely contained in &lt;code&gt;table_b&lt;/code&gt; : &lt;code&gt;table_a&lt;/code&gt; is a subset of &lt;code&gt;table_b&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;table_b&lt;/code&gt; is entirely contained in &lt;code&gt;table_a&lt;/code&gt; : &lt;code&gt;table_b&lt;/code&gt; is a subset of &lt;code&gt;table_a&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;set equivalence : &lt;code&gt;table_a&lt;/code&gt; and &lt;code&gt;table_b&lt;/code&gt; overlap completely and do not differ at all&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is worth noting that:&lt;/p&gt;

&lt;p&gt;a) We have set equivalence if we have both &lt;code&gt;table_a&lt;/code&gt; is a subset of &lt;code&gt;table_b&lt;/code&gt;, AND &lt;code&gt;table_b&lt;/code&gt; is a subset of &lt;code&gt;table_a&lt;/code&gt;.
a) If row counts for the tables differ we can never have set equivalence.
a) Even if row counts for the tables differ, we can still potentially have non empty union or a subset scenario.
a) If row counts for the two tables are both &lt;code&gt;N&lt;/code&gt;, , and the row count for the set intersection between the two tables is also  &lt;code&gt;N&lt;/code&gt;, then we have set equivalence.&lt;/p&gt;

&lt;p&gt;To find differing/similar rows, you might be tempted to construct elaborate joins involving all the columns, perhaps using FULL JOIN, but there exist a more elegant approach which involves some of SQL&amp;rsquo;s set operators.&lt;/p&gt;

&lt;h2 id=&#34;sql-set-operators-intersect-and-except:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;SQL set operators INTERSECT and EXCEPT&lt;/h2&gt;

&lt;p&gt;Most people are familiar with the &lt;code&gt;UNION&lt;/code&gt; and &lt;code&gt;UNION ALL&lt;/code&gt; set operators, but &lt;code&gt;INTERSECT&lt;/code&gt; and &lt;code&gt;EXCEPT&lt;/code&gt; is perhaps a little less known.
As a reminder, &lt;code&gt;UNION&lt;/code&gt; between two queries will produce the set union, with all duplicates removed, while &lt;code&gt;UNION ALL&lt;/code&gt; simply unifies the two result sets into one, where there might exist duplicates.
&lt;code&gt;UNION&lt;/code&gt; is more expensive than &lt;code&gt;UNION ALL&lt;/code&gt; because &lt;code&gt;UNION&lt;/code&gt; performs a sort in order to do the implicit &lt;code&gt;DISTINCT&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;intersect:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;INTERSECT&lt;/h3&gt;

&lt;p&gt;For obtaining a possible set intersection between two tables, we can use the &lt;code&gt;INTERSECT&lt;/code&gt; operator:&lt;/p&gt;
select * from table_a
intersect
select * from table_b
;

&lt;p&gt;As pointed out earlier, if the row count for the intersection is the same as the row counts for the two tables, we have set equivalence:&lt;/p&gt;
select sum(row_count) = 0 as set_equivalence from (
select count(1) as row_count from (
select * from table_a
intersect
select * from table_b
)
union all
select count(1) as row_count from table_a
union all
select -2 * count(1) as row_count from table_b
);

&lt;h3 id=&#34;except:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;EXCEPT&lt;/h3&gt;

&lt;p&gt;When we use the &lt;code&gt;EXCEPT&lt;/code&gt; set operator, for example, &lt;code&gt;query_1 EXCEPT query_2&lt;/code&gt;, we are left with all rows from &lt;code&gt;query_1&lt;/code&gt; that do not occur in &lt;code&gt;query_2&lt;/code&gt;.
Note that &lt;code&gt;query_2&lt;/code&gt; might contain rows that are not present in &lt;code&gt;query_1&lt;/code&gt;, but because &lt;code&gt;EXCEPT&lt;/code&gt; is not commutative, similar to how minus is not commutative, &lt;code&gt;query_1 EXCEPT query_2&lt;/code&gt; is not the same as &lt;code&gt;query_2 EXCEPT 
query_1&lt;/code&gt;, and we are only left with non occurring rows in &lt;code&gt;query_1&lt;/code&gt;.&lt;/p&gt;
select * from table_a
except
select * from table_b;

&lt;p&gt;&amp;hellip;is not the same as:&lt;/p&gt;
select * from table_b
except
select * from table_a;

&lt;p&gt;Some points to note about &lt;code&gt;EXCEPT&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;if the row counts for both tables are the same, then the row count for the result of applying the &lt;code&gt;EXCEPT&lt;/code&gt; operator will always be equal to, or less than the row count for the tables.&lt;/li&gt;
&lt;li&gt;If the row counts for the tables are equal, and the row count for the &lt;code&gt;EXCEPT&lt;/code&gt; query is equal to that number, then the two tables differ completely.&lt;/li&gt;
&lt;li&gt;If the row counts for the tables are equal), then the  row count for an &lt;code&gt;EXCEPT&lt;/code&gt; query between the two tables gives us the number of different rows between the two tables.&lt;/li&gt;
&lt;li&gt;If the row counts for the tables are equal, and the row count for the &lt;code&gt;table_a EXCEPT table_b&lt;/code&gt; query is equal to zero, then it means that &lt;code&gt;table_a&lt;/code&gt; is completely contained in &lt;code&gt;table_b&lt;/code&gt;, but because the tables have the same number of rows, it must be that &lt;code&gt;table_b&lt;/code&gt; and &lt;code&gt;table_a&lt;/code&gt;  are set equivalent.
Obtaining the query results for both &lt;code&gt;table_a EXCEPT table_b&lt;/code&gt; and &lt;code&gt;table_b EXCEPT table_a&lt;/code&gt; is especially useful when the row counts for &lt;code&gt;table_a&lt;/code&gt; and &lt;code&gt;table_b&lt;/code&gt; are equivalent.
Armed with those two query results, we can now move on to the task of determining in which columns the differences occur.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;horizontal-or-column-differences:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Horizontal, or column differences&lt;/h1&gt;

&lt;p&gt;When we have both &lt;code&gt;table_a EXCEPT table_b&lt;/code&gt; and &lt;code&gt;table_b EXCEPT table_a&lt;/code&gt; result sets, we can do a further &lt;code&gt;EXCEPT&lt;/code&gt; query on them, in a special regime of commenting out columns, in order to determine which columns differ.&lt;/p&gt;

&lt;h2 id=&#34;setting-things-up-for-horizontal-comparisons:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Setting things up for horizontal comparisons&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s call the result of &lt;code&gt;table_a EXCEPT table_b&lt;/code&gt; view &lt;code&gt;except_ab&lt;/code&gt;, and  similarly &lt;code&gt;table_b EXCEPT table_a&lt;/code&gt; we call view &lt;code&gt;except_ba&lt;/code&gt;.
&lt;code&gt;except_ab&lt;/code&gt; and &lt;code&gt;except_ba&lt;/code&gt; could be actual views, or results persisted in new tables.
Persisting the result sets to actual tables is a better choice when the &lt;code&gt;EXCEPT&lt;/code&gt; opperator has to work through large data sets.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s further say that both &lt;code&gt;table_a&lt;/code&gt; and &lt;code&gt;table_b&lt;/code&gt; have columns &lt;code&gt;column_001&lt;/code&gt; through &lt;code&gt;column_100&lt;/code&gt; - not an uncommonly large number of columns when dealing with analytics type, column store data sets.&lt;/p&gt;

&lt;p&gt;We want to find out what subset of columns cause &lt;code&gt;except_ab&lt;/code&gt; and &lt;code&gt;except_ba&lt;/code&gt; to differ.
We start out by crafting a query of the form:&lt;/p&gt;
select count(1) from (
select
column_001
,column_002
,column_003
...
,column_100
from except_ab
except
select
column_001
,column_002
,column_003
...
,column_100
from except_ba
);

&lt;p&gt;Of course, in practice, we  rarely have such clean column names, but rather have all sorts of interesting and creative names to deal with.
When we execute this query, we get the number of different rows - no surprises.&lt;/p&gt;

&lt;h2 id=&#34;performing-a-binary-search-over-the-column-names:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Performing a binary search over the column names&lt;/h2&gt;

&lt;p&gt;It is tempting at this point to start commenting out any column that appears suspicious&amp;hellip;. &amp;ldquo;That &lt;code&gt;column_066&lt;/code&gt; looks like a problem, let&amp;rsquo;s comment it out and run the query again&amp;rdquo; , you might reason:&lt;/p&gt;
select count(1) from (
select
column_001
,column_002
,column_003
...
--,column_066
...
,column_100
from except_ab
except
select
column_001
,column_002
,column_003
...
--,column_066
...
,column_100
from except_ba
);

&lt;p&gt;Resist this urge in favor of a more systematic approach, which will lead you to the offending column(s) in the least amount of time.
The basic idea of the systematic approach is not to try and identify individual columns, but rather finding regions of columns that contain differing column values, and then to shrink them one by one.
Once we have identified all the column regions that are causing the count to be greater than zero, we can work on each of the regions in turn, to narrow down to the individual column level, which columns differ in values.
When we found the regions that cause the two result sets to differ, we reduce those regions by halving them each time, until we end up with the smallest (in number of columns) sized regions that accounts for all the differences.&lt;/p&gt;

&lt;p&gt;It is kind of awkward to describe the algorithm here in plain English, but hopefully you get the idea:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Run the query and take note of the result - let&amp;rsquo;s call it $D$.&lt;/li&gt;
&lt;li&gt;Comment out half the columns of all currently uncommented columns. At the start, this would be from &lt;code&gt;column_001&lt;/code&gt; to &lt;code&gt;column_050&lt;/code&gt;. Do this in both branches of the query.&lt;/li&gt;
&lt;li&gt;Run the query and take note of the new result - let&amp;rsquo;s call it $E$.&lt;/li&gt;
&lt;li&gt;If the count is the same as before, if $D = E$, it means that the offending columns definitely exist in the uncommented list of columns (and potentially also exist in the commented out ones).

&lt;ol&gt;
&lt;li&gt;You can now try to invert the commented regions, and run the query again - start the process from step 1.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;If instead the count reduces to a number greater than zero, if $E &amp;gt; 0$,  it means that you have found some of the differing columns: some of them are in the commented out list of columns.

&lt;ol&gt;
&lt;li&gt;Now comment half of the previously uncommented region - &lt;code&gt;column_051&lt;/code&gt; through &lt;code&gt;column_075&lt;/code&gt;, and run the query again (go to step 2.).&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;If the count drops all the way to zero, then you know that all of the differing columns exist in the commented list of columns - you have found all of them.  This should be your goal in the first part of the process.&lt;/li&gt;
&lt;li&gt;Never start reducing the commented regions until you get a count of zero differences.&lt;/li&gt;
&lt;li&gt;Once you have a count of zero for the differences, start to work on each region in turn, by uncommenting half of it, and checking if the difference count remains zero.&lt;/li&gt;
&lt;li&gt;At the end of the process you want to be left with one or more regions of commented columns such that, uncommenting any column currently commented will make the query return a number greater than zero.&lt;/li&gt;
&lt;li&gt;If you reach this point, you have found the smallest number of columns that is responsible for the differences between the two result sets.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Most databases provide the multi-line commenting so that you can comment  like this:&lt;/p&gt;
select count(1) from (
select
/*column_001
,column_002
,column_003
...
,*/column_051
,column_052
,column_053
...
,column_100
from except_ab
except
select
/*column_001
,column_002
,column_003
...
,*/column_051
,column_052
,column_053
...
,column_100
from except_ba
);

&lt;h1 id=&#34;next-steps:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Next steps&lt;/h1&gt;

&lt;p&gt;Knowing which rows and columns differ, we can now more closely inspect the nature of the differences.
One approach we can try is to aggregate over various sub groups in the data - we group over columns that do not differ, and we aggregate over values in columns that do differ.
The cause of the differences might be due to something simple like a sign reversal, or a incorrect scaling operation, or it might be far more involved, but knowing which columns to inspect in the algorithm will help you a lot.&lt;/p&gt;

&lt;h1 id=&#34;establishing-shape-or-schema-equivalence:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Establishing shape, or schema equivalence&lt;/h1&gt;

&lt;p&gt;As a start, we have to establish schema equivalence - the two data sets have to have the same &amp;ldquo;shape&amp;rdquo; else they cannot be equivalent.
In the context of a database, this would be the number of columns and their names and data types.
Even for very wide tables, we can  fairly easily establish this by looking at the schema definition.
One approach would be to get the DDL for both data sets, the &lt;code&gt;CREATE TABLE&lt;/code&gt; statements, and compare the code side-by-side, or line-by-line.
Such a manual approach is only applicable if the schema is relatively small;
for larger schemas that would take too long to compare manually we can use diff tools.&lt;/p&gt;

&lt;p&gt;When we know the schemas are equivalent, the next logical step would be to count rows - something as simple as:&lt;/p&gt;
select count(1) from table_a
union all
select count(1) from table_b;

&lt;h1 id=&#34;conclusion:b0fe8f0bcf3b6892f71d222f4dd92b20&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It can be disheartening when, after a lot of hard work crafting a new algorithm or process, only to be greeted with differences of hundreds of thousands of rows.
Fortunately though, my experience has been that in a big data analytics context, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareto_principle&#34;&gt;pareto principal&lt;/a&gt;
tends to hold in that, and that a minority of columns tend to be the cause of the majority of differences.
In big data settings we often cannot simply eyeball the differences between data sets, mainly because of the ever-widening table schemas, and ever-growing data sizes.
What we need is approaches that can scale, and that utilise the excellent features of the storage technology the data resides in, and this is probably some sort of stack that allows for set-based, SQL-like queries.&lt;/p&gt;

&lt;p&gt;I hope that the method I shared here can help you find what you are looking for, faster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two big ideas of SQL: The What, not How, and querying with ease</title>
      <link>https://blog.goodfast.info/post/what-not-how/</link>
      <pubDate>Wed, 24 Jul 2019 21:45:07 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/what-not-how/</guid>
      <description>

&lt;p&gt;There are many factors that have contributed to the slow but steady rise of the popularity and ubiquity of SQL as a way to work with data.
Among all these factors, the most significant one in my opinion is the concept of &amp;ldquo;what&amp;rdquo; rather than &amp;ldquo;how&amp;rdquo;.
The second factor of greatness of SQL for me is the ability to quickly and easily answer questions about data.&lt;/p&gt;

&lt;p&gt;I choose to start out with these two &lt;em&gt;big ideas&lt;/em&gt; that to me, is fundamental to the language.&lt;/p&gt;

&lt;h2 id=&#34;the-what-rather-than-the-how:7167d0e820c7576947c6f743fb434a0c&#34;&gt;The &amp;ldquo;what&amp;rdquo; rather than the &amp;ldquo;how&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;The &amp;ldquo;what&amp;rdquo; rather than &amp;ldquo;how&amp;rdquo; idea seems simple, but it contains within it far reaching implications.
It is the idea of expressing &amp;ldquo;what&amp;rdquo; you want to achieve, rather than stipulating exactly &amp;ldquo;how&amp;rdquo; this should be achieved, leaving the &amp;ldquo;how&amp;rdquo; up to the system to decide.&lt;/p&gt;

&lt;p&gt;If you have had some experience solving problems by programming in a general purpose language such as C, C#, Java or Python, you should be familiar with the necessity to explain to the computer exactly how to do what you want it to do.
Failing to correctly specify every step the computer should take can lead to the inability to compile your program, or the wrong output being produced, or no output at all, or at best the correct output but too late.
This need for verbosity seems like a reasonable requirement, after all, the computer cannot guess what needs to happen, and you should find it natural that you have to tell it exactly what to do as well as how to do it.&lt;/p&gt;

&lt;p&gt;The need for total control and complete verbosity is perhaps most evident in  the domain of real time systems, where it is vital for the software not only to produce the correct result, but this result also needs to be produced at the exact correct time.
In these real time systems, it can be that the right result at the wrong time would be as disastrous as the wrong result or total failure.
A common programming language for building real time applications is C, because C allows the developer to specify the what and how very precisely, making it possible to calculate the exact time each step would take to execute.
So I got it into my head to try to do some things that are easy with SQL, in none other than C language. It turned out much harder than I thought, painful really, so read on for the gory details.&lt;/p&gt;

&lt;h2 id=&#34;the-ability-to-ask-easily:7167d0e820c7576947c6f743fb434a0c&#34;&gt;The ability to ask easily&lt;/h2&gt;

&lt;p&gt;The next great idea of SQL is the ability to quickly and easily craft questions, or queries on data.
I understand that SQL might look foreign and strange to someone who has never worked with it.
I do invite that group of people to consider how much stranger, harder and more foreign other ways of answering
this kind of questions can be if you have to use general purpose programming languages;
this post being  a case in point.&lt;/p&gt;

&lt;h3 id=&#34;data-example-from-the-gdelt-data-set:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Data :   Example from the GDELT data set&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;gdelt&#34;&gt;GDELT data set&lt;/a&gt; is a very high resolution global collection of events, gathered from numerous online news agencies and publications.
For &lt;a href=&#34;gdelt1oneday&#34;&gt;one days worth of data&lt;/a&gt; from the GDELT project, we would simply like to know:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How many data points, or events, is contained in the sample file.&lt;/li&gt;
&lt;li&gt;How many events had the actor 1 code of &amp;lsquo;AFR&amp;rsquo;, the code for Africa.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;a href=&#34;gdelt1header&#34;&gt;header columns&lt;/a&gt; is not part of the data file.
We place the data and header file in a &lt;code&gt;./gdelt&lt;/code&gt; directory after download.&lt;/p&gt;

&lt;h4 id=&#34;nix-solution:7167d0e820c7576947c6f743fb434a0c&#34;&gt;*Nix Solution&lt;/h4&gt;

&lt;p&gt;While already well away cooking  up a C language solution, I remembered AWK, and the wc program.&lt;/p&gt;
 $ wc -l gdelt/20190531.export.CSV
 176780

  $ gawk &#39;{ if ($6 == &#34;AFR&#34;) sum += 1; } END { print sum; }&#39; gdelt/20190531.export.CSV
 1184

&lt;p&gt;Here I used the &lt;code&gt;wc&lt;/code&gt; or word count program, with the &lt;code&gt;-l&lt;/code&gt; flag to count lines.
For the count of &amp;lsquo;AFR&amp;rsquo; valued actor codes I used the GNU version of &lt;code&gt;awk&lt;/code&gt;, together with a tiny awk program for a conditional incrementing of a counter.
We might want to stop here and conclude that we need go no further since we&amp;rsquo;ve gotten our answer,
but the truth is that we were quite fortunate in that there even exists a tool like &lt;code&gt;wc&lt;/code&gt; and that
conditionally counting the occurances of a particular value in a particular column is fairly straightforward with &lt;code&gt;awk&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Raising the bar ever so slightly by, for example wanting to know all the actor codes that appear in the file, along with the counts for each becomes tricky using awk.
Furthermore, we need to  verify our answer above,  by computing  the same in a few more ways.&lt;/p&gt;

&lt;h4 id=&#34;c-solution:7167d0e820c7576947c6f743fb434a0c&#34;&gt;C Solution&lt;/h4&gt;

&lt;p&gt;My C language solution is straightforward but quite a bit more verbose and detailed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read  the data file line by line.&lt;/li&gt;
&lt;li&gt;For each line, increase the lines counter.&lt;/li&gt;
&lt;li&gt;For each line, get the value of the column containing the actor 1 code.&lt;/li&gt;
&lt;li&gt;If the actor 1 code column contains what we are looking for, increase the second counter.&lt;/li&gt;
&lt;li&gt;After all lines were read, print the results to the console.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After some research, we can add some details on how to achieve our goal:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read  the data file line by line.

&lt;ul&gt;
&lt;li&gt;Open the file with the &lt;code&gt;fopen()&lt;/code&gt; function from the &lt;code&gt;stdio&lt;/code&gt; library.&lt;/li&gt;
&lt;li&gt;Read the file line by line with successive calls to &lt;code&gt;fgets()&lt;/code&gt; function, also from &lt;code&gt;sdio&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;For each line, increase the lines counter.&lt;/li&gt;
&lt;li&gt;For each line, get the value of the column containing the actor 1 code.

&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;strtok()&lt;/code&gt; function, from the &lt;code&gt;strlib&lt;/code&gt; library to tokenise on a given delimiter.&lt;/li&gt;
&lt;li&gt;The actor 1 code is in the fith column from the start of each line, where columns are seperated by the tab character.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If the actor 1 code column contains what we are looking for, increase the second counter.

&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;strcmp()&lt;/code&gt; function from &lt;code&gt;strlib&lt;/code&gt; to compare two strings for equality.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;After all lines were read, print the results to the console.

&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;printf()&lt;/code&gt; function with its very easy to use string interpolation system.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And here is the simplest first attempt C solution:&lt;/p&gt;
#include &lt;stdio.h&gt;  // fopen, fclose, fgets
#include &lt;string.h&gt;  // strtoc, strcmp

int main() 
{
   const char FILE_PATH[] = &#34;../gdelt/20190531.export.CSV&#34;;
   const char DELIMITER[] = &#34;\t&#34;;
   const char MATCH_TEXT[] = &#34;AFR&#34;;
   const int MATCH_COLUMN_POS = 5; 

   int match_count = 0;

   FILE *fp;
   const int BUFF_SIZE = 2048;
   char buff[BUFF_SIZE];

   fp = fopen(FILE_PATH, &#34;r&#34;); 
   if (fp == NULL) return -1; 
   int lines_count = 0;
   while (fgets(buff, BUFF_SIZE, (FILE*)fp)) {
       lines_count++;
       char *ptr = strtok(buff, DELIMITER);

       // find the value of the ACTOR_CODE
       for (int i=0; i&lt;MATCH_COLUMN_POS; i++)
           ptr = strtok(NULL, DELIMITER);

if (strcmp(ptr, MATCH_TEXT) == 0)
{
    match_count += 1;
}
   }
   printf(&#34;Total number of lines: %d\n&#34;, lines_count );
   printf(&#34;Number of lines matching &#39;%s&#39; in column %d: %d\n&#34;, MATCH_TEXT, 
           MATCH_COLUMN_POS,
           match_count );
   fclose(fp);
   return 0;
}

&lt;p&gt;After compiling  with gcc ./query.c -o ./query, (gcc .\query.c -o .\query.exe on Windows) and running it, the output is:&lt;/p&gt;
Total number of lines: 176780
Number of lines matching &#39;AFR&#39; in column 5: 1184

&lt;p&gt;Nice. This answer corresponds perfectly with the *nix solution,
but  what a lot of work for answering such a trivial question!
Clearly we cannot write programs each time we want to answer something like this.&lt;/p&gt;

&lt;h4 id=&#34;sql-solution:7167d0e820c7576947c6f743fb434a0c&#34;&gt;SQL Solution&lt;/h4&gt;

&lt;p&gt;As you might know, SQL is not a general purpose programming language, but rather a kind of mini language implemented in a larger database management system (or DBMS).
Because of this, you cannot directly work with flat files, but have to  first load the files into the system.
For my SQL solution I&amp;rsquo;ll use &lt;a href=&#34;sqlite&#34;&gt;SQLite&lt;/a&gt; as the SQL query engine; the DBMS.&lt;/p&gt;

&lt;p&gt;With the data and header file in the &lt;code&gt;./gdelt&lt;/code&gt; directory, we get them loaded like this:&lt;/p&gt;
&gt; sqlite3 gdelt.md
SQLite version 3.28.0 2019-04-16 19:49:53
Enter &#34;.help&#34; for usage hints.
sqlite&gt; .mode csv
sqlite&gt; .timer on
sqlite&gt; .separator &#34;\t&#34; &#34;\r\n&#34;
sqlite&gt; .import ../gdelt/CSV.header.dailyupdates.txt oneday
sqlite&gt; .import ../gdelt/20190531.export.CSV oneday

&lt;p&gt;Here is the breakdown:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I start by invoking the sqlite3 interactive CLI program with a name of the file where we want our database to live.&lt;/li&gt;
&lt;li&gt;All the commands starting with period (&amp;lsquo;.&amp;rsquo;) are system related commands, and not SQL as such.&lt;/li&gt;
&lt;li&gt;The data file has no header info, but we do have a file with header info, which we load first.&lt;/li&gt;
&lt;li&gt;Loading the header file first conveniently creates the table &lt;code&gt;oneday&lt;/code&gt;, which will hold the actual data. Later on we will show how to create tables from scrach.&lt;/li&gt;
&lt;li&gt;Loading the data file into the created table &lt;code&gt;oneday&lt;/code&gt; does not re-create the table, but appends to it.&lt;/li&gt;
&lt;li&gt;The  command &lt;code&gt;.timer on&lt;/code&gt; is not required but provides interesting timing info output.&lt;/li&gt;
&lt;li&gt;The data file is tab delimited, new line record seperated, and we let sqlite know about this with the command &lt;code&gt;.separator &amp;quot;\t&amp;quot; &amp;quot;\r\n&amp;quot;&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now we are ready to answer our question using SQL!&lt;/p&gt;
sqlite&gt; select count(1) from oneday;
176780
Run Time: real 0.163 user 0.031250 sys 0.125000
sqlite&gt; select count(1) from oneday where actor1code = &#39;AFR&#39;;
1056
Run Time: real 0.181 user 0.046875 sys 0.140625
sqlite&gt;

&lt;p&gt;Hmm&amp;hellip; We arrive at the same total row or lines count, but the count for the matching actor 1 code values is very different.
To figure out why this is the case is not trivial, but very instructive to illustrate even more the need for something like SQL and database systems, so stay with me.&lt;/p&gt;

&lt;h3 id=&#34;debugging:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Debugging&lt;/h3&gt;

&lt;p&gt;Why do the figure for the actor 1 code column  equal to &amp;lsquo;AFR&amp;rsquo; correlate for the *nix and C solutions,
yet differ for the SQL sqlite3 solution?
Since the majority rules, the SQL sqlite3 solution must be incorect. What is this SQLite3 anyways - it&amp;rsquo;s probably got a bug.
Yet,  SQLite is probably the most widely deployed database system, primarily due to it being used on mobile platforms such as Android.
Surely they would nnot use a broken thing!&lt;/p&gt;

&lt;h4 id=&#34;what-we-know:7167d0e820c7576947c6f743fb434a0c&#34;&gt;What we know&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;All solutions correlate on the row / line / record count.&lt;/li&gt;
&lt;li&gt;The *nix and C language solution correlates on actor 1 code count.&lt;/li&gt;
&lt;li&gt;The SQLite3 solution actor 1 code count is less than the other two solutions.&lt;/li&gt;
&lt;li&gt;None of the solutions produced any errors or warnings on data load or query.&lt;/li&gt;
&lt;li&gt;SQLite is unlikely to have a bug causing the issue, but we might have loaded data wrongly somehow.&lt;/li&gt;
&lt;li&gt;It is unlikely that twoout of our three solutions would produce the same wrong result.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;poking-around-in-the-data:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Poking around in the data&lt;/h3&gt;

&lt;p&gt;Since we have everything setup in SQLite already, let&amp;rsquo;s query the data a bit more, in hope we find some clues to help us solve the dilemma.&lt;/p&gt;

&lt;p&gt;How many values are there for column &lt;code&gt;actor1code&lt;/code&gt;?&lt;/p&gt;
sqlite&gt; select count(distinct actor1code) from oneday;
2148

&lt;p&gt;Is the count from before for the SQLite solution still the same?&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code like &#39;AFR&#39;;
1056

&lt;p&gt;Perhaps the comparison predicate in the &lt;code&gt;where&lt;/code&gt; clause &lt;code&gt;actor1code = &#39;AFR&#39;&lt;/code&gt; has a problem.
Let us try some &lt;code&gt;LIKE&lt;/code&gt; matching instead.&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code like &#39;%AFR&#39;;
1056
sqlite&gt; select count(1) from oneday where actor1code like &#39;%AFR%&#39;;
1241
sqlite&gt; select count(1) from oneday where actor1code like &#39;AFR%&#39;;
1139

&lt;p&gt;Nope, this is not helping much, andsomething else is going on.&lt;/p&gt;

&lt;h4 id=&#34;edge-cases:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Edge cases&lt;/h4&gt;

&lt;p&gt;It&amp;rsquo;s always a good idea to consider some edge cases in the data.
One such edge case is blank values, so let&amp;rsquo;s check for them in the actor 1 column:&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code = &#39;&#39;;
16475

&lt;p&gt;We have seen that the SQLite solution under counts, or, &lt;strong&gt;gasp&lt;/strong&gt; BOTH the AWK *nix solution AND the C language one over counts, and do it the same.
Since both the AWK and the C language solution produced the same result, let&amp;rsquo;s see if we can find the blanks count of above with AWK:&lt;/p&gt;
 $ gawk &#39;{ if ($6 == &#34;&#34;) sum += 1; } END { print sum; }&#39; ./gdelt/20190531.export.CSV


&lt;p&gt;Well, it produced no output, so gawk wasn&amp;rsquo;t able to count blanks&amp;hellip;.
I bet the C language solution will do exactly the same!&lt;/p&gt;

&lt;p&gt;Now the actor 1 code of &amp;lsquo;AFR&amp;rsquo; is quite special, but we know from the documentation of GDELT and also from the header columns, that there is also an actor 2 code column, and it follows right after all the actor 1 columns.
How many rows are there where the column actor1code is blank, and the actor2code is &amp;lsquo;AFR&amp;rsquo;:&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code = &#39;AFR&#39;;
1056

sqlite&gt; select count(1) from oneday where actor1code = &#39;&#39; and actor2code = &#39;AFR&#39;;
128

sqlite&gt; select count(1) from oneday where actor1code = &#39;&#39; and actor2code = &#39;AFR&#39; or actor1code = &#39;AFR&#39;;
1184

&lt;p&gt;Now that is interesting.  The C and AWK solutions count, in column 6 where actor 1 code should live, the sum of the columns where actor 1 code is &amp;lsquo;AFR&amp;rsquo;, plus the number of
rows where actor 1 code is blank AND actor 2 code is &amp;lsquo;AFR&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;Before I touch that C solution again, let&amp;rsquo;s do something easier, and search a bit online; my money is on the &lt;code&gt;strtok()&lt;/code&gt; function: I think it works different to what we assumed&amp;hellip;&lt;/p&gt;

&lt;p&gt;And yes! Indeed! A fellow Internet citizen &lt;a href=&#34;strtok_skipping&#34;&gt;came across this already&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We could now proceed and fix up the C solution, but by now I&amp;rsquo;ve kind of lost interest, and I&amp;rsquo;m truely thankful I do not have to code C for a living - it must be very, very hard.
None the less, perhaps for the love of Linux, let&amp;rsquo;s at least get AWK right.&lt;/p&gt;
$ gawk &#39;BEGIN {FS = &#34;\t&#34;} { if ($6 == &#34;AFR&#34;) sum += 1; } END { print sum; }&#39; gdelt/20190531.export.CSV
1056

&lt;p&gt;Yay for AWK, yay for the Unix tradition, yay for Linux!
We explicitly set the field seperator (FS), same as for the SQLite solution.&lt;/p&gt;

&lt;h2 id=&#34;ddl-and-dml:7167d0e820c7576947c6f743fb434a0c&#34;&gt;DDL and DML&lt;/h2&gt;

&lt;p&gt;You may have heard the acronym DDL and DML before.
The objectives of SQL can be categorised into two broad groups. The first group  of objectives is  related to the definition of artifacts to organise and access data.
This group is commonly referred to as the data definition language (or DDL) statements .
The second group of objectives relates to manipulation; the creation, the modification and the deletion of data.
This group is commonly refered to as the data manipulation language (or DML) statements.&lt;/p&gt;

&lt;p&gt;Outside the DDL and DML groups there usually exists additional statements concerned with particulars of the  actual SQL-based system, such as those intended for administration and maintenance.
For SQLite examples of these more admin-type keywords and functions are the ones starting with &amp;lsquo;.&amp;rsquo; as illustrated above when we loaded the CSV file.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Conclusion&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Answering even the simplest of questions about data, using only imperative programming like C is tough and error prone.&lt;/li&gt;
&lt;li&gt;The Unix tradition has many, many jewels; AWK and wc two nice examples, and knowing about these can give us an edge.&lt;/li&gt;
&lt;li&gt;For answering anything but the simplest queries it is best to rope in a proper SQL database system, and this can be light weight such as SQLite.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;What&amp;rdquo; rather than &amp;ldquo;How&amp;rdquo; idea of SQL is huge: don&amp;rsquo;t worry how to do stuff, just declare to the computer what you want, and let the system sort out how to give it to you.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s very powerful to be able to answer questions on data, simbply by writing short queries rather than whole programs.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bar-sized factorial</title>
      <link>https://blog.goodfast.info/post/big-factorial/</link>
      <pubDate>Sun, 23 Jun 2019 22:26:20 +0000</pubDate>
      
      <guid>https://blog.goodfast.info/post/big-factorial/</guid>
      <description>&lt;p&gt;What&amp;rsquo;s the first five digits of 1,000,000! or 1 million factorial?
Let&amp;rsquo;s ask Haskell:&lt;/p&gt;
Prelude&gt; take 5 $ show $ product [1..1000000] 
&#34;82639&#34; 
(1043.56 secs, 1,444,511,743,624 bytes) 

&lt;p&gt;Neat!&lt;/p&gt;

&lt;p&gt;But you might ask, how many digits is this rather large number, of 1 bar factorial? Let&amp;rsquo;s ask again:&lt;/p&gt;
Prelude&gt; length $ show $ product [1..1000000] 
5565709 
(1103.04 secs, 1,444,845,773,000 bytes) 

&lt;p&gt;Wow! 5.5 million digits! Way more than all the electrons in the universe.&lt;/p&gt;

&lt;p&gt;Those second timings is how long it took to compute, and bytes used is sort of like the amount of memory used, but not at once, more like in batches and then garbage collected.&lt;/p&gt;

&lt;p&gt;I set it off, then left it running on spare compute in the background; hardly noticed anything.&lt;/p&gt;

&lt;p&gt;What is the world coming too when you can compute stuff like that with one line of code&amp;hellip;?&lt;/p&gt;

&lt;p&gt;Haskell is something else; definitely the most interesting and powerful language I&amp;rsquo;ve come across thus far.&lt;/p&gt;

&lt;p&gt;To be perfectly honest, it is about the third or fourth time round  for me, trying to learn and actually use Haskell.
Thus far I&amp;rsquo;m rather impressed.&lt;/p&gt;

&lt;p&gt;Things are starting to click&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Permutations and combinations, and ways to calculate the number of draws in a lottery</title>
      <link>https://blog.goodfast.info/post/permutations-combinations/</link>
      <pubDate>Sun, 23 Jun 2019 08:25:20 +0000</pubDate>
      
      <guid>https://blog.goodfast.info/post/permutations-combinations/</guid>
      <description>

&lt;p&gt;With a recent, unusually high Power Ball jackpot of ZAR ~220 million, we could not help discussing one&amp;rsquo;s chances for actually hitting the jackpot as a bit of nice office banter.&lt;/p&gt;

&lt;p&gt;Despite your personal take on the Power Ball and Lotto,  we can turn it into a nice mathematical puzzle, and sanatise the gambling aspect away. Yeah, we can do that, because we are into maths, programming and data!
For the uninitiated among us, the way it works is simple really.
You choose 5 numbers, from a set of  numbers ranging from 1 to 50,
and one number from a set of numbers from 1 to 20 - the &amp;ldquo;Power Ball&amp;rdquo;.
Order does not matter.&lt;/p&gt;

&lt;p&gt;So, how do we calculate how many possible options is available to you?
I use the term &amp;ldquo;options&amp;rdquo; deliberately, since the proper maths terms is actually &amp;ldquo;combinations&amp;rdquo;.
Note that the maths terminology is contrary to commons sense: where order of selection matters, maths-speak calls it a permutation, and where order does not matter, a combination.
Confusing, since we generally do not speak of the permutation of the security alarm code!
Away with maths terminology! OK, only until the end of this post.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For picking number one, you can choose from 50 options.&lt;/li&gt;
&lt;li&gt;For number two, from 49 options.&lt;/li&gt;
&lt;li&gt;For number three, from 48 options, and so on&lt;/li&gt;
&lt;li&gt;At number five only 46 options remain to choose from.&lt;/li&gt;
&lt;li&gt;For picking the Power Ball, there are always 20 options.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we end up with this:&lt;/p&gt;
NumberOfPowerBallDraws = 50 * 49 * 48 * 47 * 46 * 20 
NumberOfPowerBallDraws  = 5085024000 

&lt;p&gt;Hmm, that is very big, too big  wouldn&amp;rsquo;t you say?
At those odds, jackpots would be very seldom.&lt;/p&gt;

&lt;p&gt;What is wrong?
Well, suppose we chose option 1 for our first choice, and then option 2 for our second, that is all fine, but what if we then chose option 2 for our first choice, and option 1 for our second?
In our equation above, both those would be valid start choices, but in actual fact, since order does not matter, those are the same choices according to the definition of the game: we are double counting.
So how badly are we double counting?
Well, as badly as you can choose to order five numbers in a row, so that order matters, and that is kind of what we figured out in our mistake above, giving:&lt;/p&gt;
WaysToOrderFiveNumbers = 1 * 2 * 3 * 4 * 5 
WaysToOrderFiveNumbers = 120 

&lt;p&gt;Finally,  to fix our error, we reduce our previous answer by this factor, to get:&lt;/p&gt;
CorrectNumberOfPowerBallDraws = NumberOfPowerBallDraws / WaysToOrderFiveNumbers 

CorrectNumberOfPowerBallDraws = 5085024000 / 120 

CorrectNumberOfPowerBallDraws = 42375200 

&lt;p&gt;Is this correct now? Yeah, go ahead and Google it!&lt;/p&gt;

&lt;p&gt;OK, back to the maths terms, because  it helps us actually.&lt;/p&gt;

&lt;p&gt;The multiplication of N numbers, from 1 to N is called factorial, and denoted $N!$, with $0!$ defined as 1.
A permutations is an ordered selection of  k items from a set of N items, given by:&lt;/p&gt;

&lt;p&gt;$$
P(N,k) = N! / (N - k)!
$$&lt;/p&gt;

&lt;p&gt;A combination is an unordered selection of k items from a set of N items, and given by:&lt;/p&gt;

&lt;p&gt;$$
C(N,k) = N! / (N - k)! * k!
$$&lt;/p&gt;

&lt;p&gt;It all boils down to: don&amp;rsquo;t play Power Ball, or any other such lotteries since the odds are firmly stacked against you.&lt;/p&gt;

&lt;h2 id=&#34;solutions-in-a-couple-of-languages:b6bf697cd8bd4495824c64b2e031761c&#34;&gt;Solutions in a couple of languages&lt;/h2&gt;

&lt;p&gt;Here are a couple of ways to calculate the figure above.&lt;/p&gt;

&lt;h3 id=&#34;python:b6bf697cd8bd4495824c64b2e031761c&#34;&gt;Python&lt;/h3&gt;

&lt;p&gt;Using the names from the analysis above, we get a rather long solution:&lt;/p&gt;
import functools as ft # for reduce

# n: the number of options, p: how many &#34;power ball&#34; options there are, k: the number of allowed numbers that can be chosen
def numberOfPowerBallDraws(n,p,k):
    return ft.reduce(lambda a,b: a*b, range(n-k+1,n+1)) * p

def waysToOrderKNumbers(k):
    return ft.reduce(lambda a,b: a*b, range(1,k+1))

def correctNumberOfPowerBallDraws(n,p,k):
    return numberOfPowerBallDraws(n,p,k) / waysToOrderKNumbers(k)

p, n, k = 20, 50, 5
print(correctNumberOfPowerBallDraws(n,p,k))
# output 42375200.0

&lt;p&gt;Some points to note:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The  function &lt;code&gt;range(a,b)&lt;/code&gt; produce a closed, open interval from a through b, so b itself is not included.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;reduce&lt;/code&gt; function needs to be imported from functools since it is not part of the base Python system in the REPL anymore.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can simplify the code into one line, which is less readable but more concise:&lt;/p&gt;
import functools as ft
p, n, k = 20, 50, 5
ft.reduce(lambda a,b: a*b, range(n-k+1,n+1)) * p / ft.reduce(lambda a,b: a*b, range(1,k+1))
# output 42375200.0

&lt;p&gt;Noticing that we multiply lists in two places, we can simplify even more by adding a &lt;code&gt;mul(r)&lt;/code&gt; product function like this.&lt;/p&gt;
import functools as ft

def mul(r): return ft.reduce(lambda a,b: a*b, r)

p, n, k = 20, 50, 5
mul(range(n-k+1,n+1)) * p / mul(range(1,k+1))
# output 42375200.0

&lt;p&gt;Much better, but still to me at least, not as clean as it could be.&lt;/p&gt;

&lt;h3 id=&#34;haskell:b6bf697cd8bd4495824c64b2e031761c&#34;&gt;Haskell&lt;/h3&gt;
Prelude&gt; let draws n p k = product [n, (n-1) .. (n-k+1)] * p / product [1 .. k]
Prelude&gt; draws 50 20 5
-- outputs 4.23752e7

&lt;p&gt;In my opinion, the simplest, most straightforward, most beautiful code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to use the transitive closure over a set of relations for fast path finding in SQL</title>
      <link>https://blog.goodfast.info/post/transitive-closures/</link>
      <pubDate>Tue, 26 Sep 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/transitive-closures/</guid>
      <description>

&lt;p&gt;In a &lt;a href=&#34;http://goodfast.info/post/speed-up-views-through-custom-materialization/&#34;&gt;previous post&lt;/a&gt;, I wrote about how we make sense of the world by modelling relationships between things as tree-like hierarchies.
This time we will add to this hierarchical data structure, a representation derived by calculating all possible paths.
This set of paths is referred to as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Transitive_closurejj&#34;&gt;transitive closure&lt;/a&gt;, and can be thought of as the set of all paths if you start at each node in the tree.&lt;/p&gt;

&lt;p&gt;I wish I could tell you that it is as simple as Mr. Eby in &lt;a href=&#34;http://dirtsimple.org/2010/11/simplest-way-to-do-tree-based-queries.html&#34;&gt;this article&lt;/a&gt; makes it out to be, but
alas, when I got right down implementing a full solution, things got quite involved.
It tends to be like that.
None the less, credit where credit is due; go read that article first I can highly recommend it, and then come back here for more!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve developed the code for SQL Server, so it is immediately T-SQL compatible, but you can surely alter it for any decent database.&lt;/p&gt;

&lt;h2 id=&#34;background:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;Trees and hierarchies in general can get quite complicated, so why would we choose to add to this complexity yet another data structure?
The reason is so that we can apply &lt;a href=&#34;https://en.wikipedia.org/wiki/Memoization&#34;&gt;memoization&lt;/a&gt;; we buy time with space.
At the cost of the time to compute the transitive closure once, and the cost of the space required to persist it, we gain the time it would have taken to calculate it each time it is needed.&lt;/p&gt;

&lt;p&gt;Previously I wrote about how one can go about to materialize an entire complex and expensive view.
The use of the transitive closure can also be thought of as a kind of materialization, but it is far smarter and promises to be more useful.&lt;/p&gt;

&lt;p&gt;The [transitive] part in the name refers to a property that a relation can exhibit.
Since you are related to your father, and your child is related to you, your child is also related to your father.
We can say that the inheritance relation is transitive.
Since 9 &amp;gt; 5, and 5 &amp;gt; 3, it is also true that 9 &amp;gt; 5 &amp;gt; 3 and 9 &amp;gt; 3;
the &amp;ldquo;greater than&amp;rdquo; relation is transitive.&lt;/p&gt;

&lt;h2 id=&#34;representing-the-hierarchy:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Representing the hierarchy&lt;/h2&gt;

&lt;p&gt;The hierarchy we will work with is a simple one:&lt;/p&gt;


&lt;img srcset=&#34;../../ct_tree.svg&#34; src=&#34;../../ct_tree.png&#34; alt=&#34;Diagram of the tree with node a at root, nodes b and c below it, below node b is node d and e, below node c is node f, below node d is node g.&#34;&gt; 



&lt;p&gt;The usual way to represent such a hierarchy in a table is through self referencing records:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;id&lt;/th&gt;
&lt;th&gt;parent_id&lt;/th&gt;
&lt;th&gt;label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;lsquo;a&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;lsquo;b&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;lsquo;c&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&amp;rsquo;d&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&amp;lsquo;e&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&amp;lsquo;f&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&amp;lsquo;g&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Such self referencing records in a single table makes manipulation of the hierarchy very simple.
For example, to move the sub tree rooted in node 2 and make it fall under node 6, we simply update the parent_id of node 2 to reference node 6.
This simplicity, however, comes at a cost.
When you want to traverse the hierarchy, you require iteration or recursion which is generally expensive.
This is especially so if all that you are after is only a portion of the tree, or worse, only the path from the root to a particular intermediate or leaf node.&lt;/p&gt;

&lt;p&gt;Suppose you want to find out the path from  node &amp;lsquo;g&amp;rsquo; to the root.
After finding the entry for node &amp;lsquo;g&amp;rsquo;, you have to repeatedly find the parent until there is no more parent.
Suppose you want to get the path from the root for each node in the tree.
The database has to do this process for every node.&lt;/p&gt;

&lt;p&gt;A transitive closure over all the relations in the base table gives you a ready-made set of paths which you can index and query, just like any other set of records.
No more need for recursive CTE&amp;rsquo;s each time you want path information, or worse still, multiple queries!&lt;/p&gt;

&lt;h2 id=&#34;setup:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Setup&lt;/h2&gt;

-- create table node to represent the relations
create table node (id int, parent_id int, label varchar(50));

-- load noad
insert into node values 
(1,0,&#39;a&#39;),
(2,1,&#39;b&#39;),
(3,1,&#39;c&#39;),
(4,2,&#39;d&#39;),
(5,2,&#39;e&#39;),
(6,3,&#39;f&#39;),
(7,4,&#39;g&#39;);

-- table to hold the transitive closure over nodes
create table closure (parent_id int, child_id int, depth int, route varchar(100));

&lt;p&gt;Table node is the standard self referencing structure, and table closure will contain the paths that node represent.
Column node.label is only the label that applies to the particular node row, but column closure.route will contain a nice chain of all the labels from closure.parent_id to closure.child_id.
In a database that supports arrays, such as PostgreSQL, we can even go so far as to store the actual id values of the whole path.
In column closure.depth we want to store how many hops it takes to go from the node at parent_id to the node at child_id.&lt;/p&gt;

&lt;p&gt;What we want to achieve is to calculate all possible paths in the tree and represent them like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;parent_id&lt;/th&gt;
&lt;th&gt;child_id&lt;/th&gt;
&lt;th&gt;depth&lt;/th&gt;
&lt;th&gt;route&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a &amp;gt; b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a &amp;gt; c&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;c &amp;gt; f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; c &amp;gt; f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b &amp;gt; d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b &amp;gt; e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;b &amp;gt; d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Notice the identity paths, or paths starting and ending in the same node, with depth of zero.
Later on it will become apparent why we require these.&lt;/p&gt;

&lt;h3 id=&#34;closure-insert:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure insert&lt;/h3&gt;

&lt;p&gt;A new entry in table node can only be one of the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;a new, additional root - there exists no parent for it&lt;/li&gt;
&lt;li&gt;a new, additional child - there exists a parent for it&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For either type of new node, table closure requires a new identity path.
Furthermore, if the new node has a parent, we also need to add entries for all paths ending in the new node.&lt;/p&gt;
create proc closure_insert @parent_id int, @child_id int, @route varchar(50) as 
begin
-- always insert identity
insert into closure (parent_id,child_id,depth,route) 
values (@child_id,@child_id,0, @route);
if  @parent_id &lt;&gt; @child_id and @parent_id is not null
insert into closure (parent_id,child_id,depth, route) 
select parent.parent_id, child.child_id,
parent.depth + child.depth + 1, 
parent.route + &#39; &gt; &#39; + child.route 
from closure as parent cross join closure as child 
where parent.child_id = @parent_id and child.parent_id = @child_id;
end;

&lt;h3 id=&#34;closure-delete:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure delete&lt;/h3&gt;

&lt;p&gt;Deleting a relation (suppose it is the node with id @child_id)from the node table requires us to delete all paths from the closure table that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;start out from @child_id,&lt;/li&gt;
&lt;li&gt;end in @child_id,&lt;/li&gt;
&lt;li&gt;runs through @child_id, so any path that starts at any of @child_id&amp;rsquo;s parents,and any path that ends in any of @child_id&amp;rsquo;s children&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
create proc closure_delete @child_id int as begin
delete from link  
from closure as parent, closure as link, closure as child, closure as to_delete 
where 
parent.parent_id = link.parent_id and child.child_id = link.child_id
and parent.child_id = to_delete.parent_id and child.parent_id = to_delete.child_id
and (to_delete.child_id = @child_id or to_delete.parent_id = @child_id) 
and to_delete.depth &lt; 2;
end;

&lt;h3 id=&#34;closure-update:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure update&lt;/h3&gt;

&lt;p&gt;Similar to the delete operation, if a node changes, all paths that start from, or end in, or go through that node needs to be updated since the label of the node might have changed and thus the routes need to be built again.
If a node moved; if it is now a sub node of another node then all paths starting from, ending in and going through the node that moved needs to be deleted, and new paths added.&lt;/p&gt;

&lt;p&gt;To simplify matters we are going to delete all such starting from, ending in, and going through paths, and re-insert the individual nodes again in the correct order.
Before we delete all the paths we first will temporarily store all the nodes we will be re-inserting afresh after the delete.
Then we will do the delete, followed by successively calling the insert proc to insert everything again.&lt;/p&gt;

&lt;p&gt;But what if we insert a node (think add all paths involving this node) before we inserted the node&amp;rsquo;s parent(s)?
If you take a look at our insert proc you&amp;rsquo;ll see very quickly that this will result in problems; we will not find the parent paths, and thus add too few actual paths.
For this reason it is essential that we first insert all nodes without parents, then followed by the rest of the set that do have a parent, which will already be inserted.
The insert proc will take care of adding all required paths, but only if we insert in the correct order.&lt;/p&gt;
create proc closure_update @child_id int as begin
-- temp storage of nodes to insert after the deletion
declare @t as table (id int primary key, parent_id int, label varchar(50));

insert into @t (id,parent_id,label) 
select link.child_id,n.parent_id,n.label 
from closure as link join node as n on n.id = link.child_id
where  link.parent_id = @child_id;

delete from link 
from closure as link join @t as t
on link.child_id = t.id or link.child_id = @child_id;

-- repeatedly call the insert proc in the correct order, 
-- which is ensured by the recursive CTE over the set of nodes to insert
declare @_p int, @_c int, @_l varchar(50);
declare cur cursor fast_forward for 
with  to_insert as (
	select parent_id, id, label from @t
),  to_insert_ordered  as (
-- the anchor for the recursion
	select ti.parent_id, ti.id, ti.label 
	from to_insert as ti
	where ti.parent_id = 0
	or ti.id = @child_id
	or ti.parent_id not in (
		select id from to_insert
	)
	union all
	select ti.parent_id, ti.id, ti.label 
	from to_insert_ordered as tio
	join to_insert as ti
	on ti.parent_id = tio.id
) select parent_id, id, label from to_insert_ordered;
open cur;
fetch next from cur into @_p, @_c, @_l;
while @@FETCH_STATUS = 0 begin
	exec closure_insert @_p,@_c,@_l;
fetch next from cur into @_p, @_c, @_l;
end close cur deallocate cur;
end;

&lt;p&gt;The insertion cursor query follows the usual pattern for a recursive CTE:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;establish the data to recurse over: to_insert&lt;/li&gt;
&lt;li&gt;establish the anchor, or starting point: to_insert_ordered&lt;/li&gt;
&lt;li&gt;union with a join onto itself and the whole list to_insert&lt;/li&gt;
&lt;li&gt;select the result&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;closure-refresh:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure refresh&lt;/h3&gt;

&lt;p&gt;To do the initial load, and in case something goes wrong, we add one more proc; a full refresh proc, which is simply successive calls to the insert proc, but in the correct order as described in the previous section.&lt;/p&gt;
create proc closure_refresh as begin
truncate table closure;

declare @p int, @c int, @l varchar(50);
declare cur cursor fast_forward for with  to_insert as (
	select parent_id, id, label from node
),  to_insert_ordered  as (
	select ti.parent_id, ti.id, ti.label 
	from to_insert as ti
	where ti.parent_id = 0 or
	ti.parent_id not in (
		select id from to_insert
	)
	union all
	select ti.parent_id, ti.id, ti.label 
	from to_insert_ordered as tio
	join to_insert as ti
	on ti.parent_id = tio.id
) select parent_id, id, label from to_insert_ordered;
open cur;
fetch next from cur into @p, @c, @l;
while @@FETCH_STATUS = 0 begin
	exec closure_insert @p,@c,@l;
fetch next from cur into @p, @c, @l;
end close cur deallocate cur;
end;

&lt;h2 id=&#34;testing:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;Here are some sample queries which you can use to test your solution. I&amp;rsquo;ve done it, not going to paste 50 tables of output here, try all sorts and see what happens:&lt;/p&gt;
-- start clean
exec closure_refresh;
-- check that everything is in order
select * from closure;
-- relabel node 2
update node set label = &#39;z&#39; where id = 2;
-- do what the after update trigger will do:
exec closure_update 2;
-- not only node 2&#39;s identity entry, 
-- but all paths involving it should now say &#39;z&#39; instead of &#39;b&#39;
select * from closure;
-- fix node 2 again:
update node set label = &#39;b&#39; where id = 2;
exec closure_update 2;
-- things should be back to what it was initially...
select * from closure;
-- move node 2, or &#39;b&#39; under node 6
update node set parent_id = 6 where id = 2;
exec closure_update 2;
-- node 2 should now be under node 6
select * from closure;

&lt;h2 id=&#34;go-forth-and-conquer:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Go forth and conquer!&lt;/h2&gt;

&lt;p&gt;Armed with the procedures we developed thus far, we can proceed to hook them up with after triggers on the node table.
When the node table change, the after triggers fire and the closure table stays up to date.&lt;/p&gt;

&lt;p&gt;When we want to answer questions such as, does there exist a path from X to Y, what is the longest path from X to Y (for obtaining the full path from the root), and many more such queries, we can simply perform fast selects against the closure table.
We can index parent_id, child_id, and (parent_id,child_id) and so on in order to speed things up.
We can create a few custom views for quickly determining all the paths from the roots to the individual intermediate and leaf nodes.&lt;/p&gt;

&lt;p&gt;The recursive CTE that is needed for the update procedure is unfortunate, but luckily it only operates on the set of nodes that need to change.
Unless you are altering root nodes all the time, this will generally be limited to a small number, and not the entire set of data as is the case in a full materialization of the whole tree.
This means that the maintenance overhead will generally be far less.&lt;/p&gt;

&lt;p&gt;This is only a proof of concept, but you can go forth from here and use it as a reference to implement more complex and advanced transitive closures.
I hope it helps you maintain performance on queries on hierarchies, and I hope you&amp;rsquo;ve learned something new along the way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wizen up a bit : solve problems bitwize</title>
      <link>https://blog.goodfast.info/post/wizen-up-a-bit/</link>
      <pubDate>Wed, 20 Sep 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/wizen-up-a-bit/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m rather obsessed with bits. All sorts of bits, at various times, but in particular, the digital bit of the Binary system. Notice the capitalization of &amp;ldquo;Binary&amp;rdquo; - it is intended.
Efficient bit representations of information is purity ; ever more compact representations elegance itself,
so for this post I invite you to come with me, way back to 2013, when a nice couple of  &lt;a href=&#34;https://en.wikipedia.org/wiki/Bitwise_operation&#34;&gt;bitwise&lt;/a&gt; operations flaunted their power and expressiveness.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s tough for me when a value that will only ever require N distinct states gets over represented by vastly more bits than is needed.
A 4-byte integer to store a human age, a UUID, with it&amp;rsquo;s crazy 16 bytes, and the list goes on and on: we can do better!
Think of the real, actual small value, arriving inside the 64-bit CPU register, and the wasteful runs of zeros.
Think of it, happening billions and billions of times a second.
We can only hope that somewhere along the way, things get packed in a bit to mitigate those wasted runs of zeros.&lt;/p&gt;

&lt;p&gt;Once I read that using a short, or 2-byte database column type or variable is actually not more performant than just using a 4-byte integer.
Due to the fact that the smallest addressable space is anyways 4 bytes large, it took exactly the same resources to process 4, 2 or even a 1 byte word.
The exact context of that claim escapes me.
I put it down to my confirmation bias, operating sub consciously, blocking it out.
I like my confirmation bias working for me like that.&lt;/p&gt;

&lt;h2 id=&#34;the-back-story:d352b201a1938ed46d030d55323cc26d&#34;&gt;The back story&lt;/h2&gt;

&lt;p&gt;It was the weekend of the Google CodeJam qualifier.
Having heard of it some weeks before, I planned to check out the rules properly, and do some practice problems. It never happened.
Arriving home in the afternoon on the Saturday, I thought I did not have much of a chance at it due to the time constraint, but I could not help myself giving it a go anyways.
So it happened. Pacing around in my flat, beer in hand, I chose &lt;a href=&#34;https://code.google.com/codejam/contest/2270488/dashboard#s=p0&#34;&gt;this little problem&lt;/a&gt; to solve.
It was a sort of tic-tac-toe variant, but only the board state identifier part.&lt;/p&gt;

&lt;p&gt;Given a 4-x-4 board, determine if either player is in the winning position, or if a tie occurred.
The &amp;rsquo;T&amp;rsquo; symbol being neutral in that it could count in either player&amp;rsquo;s favor.&lt;/p&gt;

&lt;p&gt;Yes, you can loop over the elements or throw a Linq expression at it, but hey, we can have more fun than that, with bit masks and bit shifts.
This was bit mask land, and I was in my element!
For more details about the problem, visit the &lt;a href=&#34;https://code.google.com/codejam/contest/2270488/dashboard#s=p0&#34;&gt;problem statement&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;bit-boards:d352b201a1938ed46d030d55323cc26d&#34;&gt;Bit boards&lt;/h2&gt;

&lt;p&gt;We choose an integer, 4 bytes, to store a bit mask of the player O, player X, how a column looks, how each of the two diagonals look and so on. We could have used a 2-byte unsigned  short instead of an int, sadly we didn&amp;rsquo;t. Well, got to leave something for next time.&lt;/p&gt;
        public static int column, diagonal1, diagonal2, fullboard;

&lt;p&gt;and&lt;/p&gt;
            int xboard = 0, oboard = 0, tboard = 0;

&lt;p&gt;We assign the static patterns we&amp;rsquo;ll use for column, the two diagonals and the full board:&lt;/p&gt;
            column = Convert.ToInt32(&#34;1000100010001&#34;, 2);
            diagonal1 = Convert.ToInt32(&#34;1000010000100001&#34;, 2);
            diagonal2 = Convert.ToInt32(&#34;0001001001001000&#34;, 2);
            fullboard = Convert.ToInt32(&#34;1111111111111111&#34;, 2);

&lt;p&gt;I&amp;rsquo;m not going to bother you with file IO, so we skip to where we set the X, O and T boards:&lt;/p&gt;
                        xboard = StringBoardToBitBoard(line, &#39;X&#39;);
                        oboard = StringBoardToBitBoard(line, &#39;O&#39;);
                        tboard = StringBoardToBitBoard(line, &#39;T&#39;);

&lt;p&gt;Function StringBoardToBitBoard does exactly what  you would imagine - it assigns a &amp;lsquo;1&amp;rsquo; in the bit position if the char occurs and a &amp;lsquo;0&amp;rsquo; otherwise - see below for the full program.&lt;/p&gt;

&lt;h2 id=&#34;decision-and-output:d352b201a1938ed46d030d55323cc26d&#34;&gt;Decision and output&lt;/h2&gt;
                        if (Won(xboard, tboard))
                            sw.WriteLine(&#34;Case #{0}: X won&#34;, i);
                        else if (Won(oboard, tboard))
                            sw.WriteLine(&#34;Case #{0}: O won&#34;, i);
                        else if (((xboard | oboard | tboard) &amp; fullboard) == fullboard)
                            sw.WriteLine(&#34;Case #{0}: Draw&#34;, i);
                        else
                            sw.WriteLine(&#34;Case #{0}: Game has not completed&#34;, i);

&lt;h2 id=&#34;the-real-magic-bitwise-operators-and-shifts:d352b201a1938ed46d030d55323cc26d&#34;&gt;The real magic: bitwise operators and shifts&lt;/h2&gt;

&lt;p&gt;And finally, let&amp;rsquo;s have a look at the elegant part; function Won and friends:&lt;/p&gt;
        public static bool Won(int board, int tboard)
        {
            board |= tboard;  // apply the T position
            return LinesMatch(board) || ColumnsMatch(board) || DiagonalsMatch(board);
        }

&lt;p&gt;makes sense doesn&amp;rsquo;t it; you win if you have a line, a column or a diagonal.&lt;/p&gt;
        public static bool LinesMatch(int board)
        {
            return board == (board | 15)
                || board == (board | (15 &lt;&lt; 4))
                || board == (board | (15 &lt;&lt; 8))
                || board == (board | (15 &lt;&lt; 12));
        }

&lt;p&gt;You have a lines match if your board position
is logical OR-ed (the &amp;lsquo;|&amp;rsquo; operator)  with any of the line representations, and still is equal to itself.
For the 4 line representations we could have used constants, to avoid the bit shift (&amp;rsquo;&amp;lt;&amp;lt;&amp;lsquo;) but defining them; that would be too laborious, but would probably have been better.
Also, the 15 magic number in there, for the first lines representation; should really live in a constant.&lt;/p&gt;

&lt;p&gt;In the same way, but slightly different, the columns and diagonals are checked:&lt;/p&gt;
        public static bool ColumnsMatch(int board)
        {
            return
                board == (board | column)
                || board == (board | (column &lt;&lt; 1))
                || board == (board | (column &lt;&lt; 2))
                || board == (board | (column &lt;&lt; 3));
        }
        public static bool DiagonalsMatch(int board)
        {
            return board == (board | diagonal1)
                || board == (board | diagonal2);
        }

&lt;p&gt;The diagonals checking could have happened all at once, why is this possible?&lt;/p&gt;

&lt;h2 id=&#34;conclusion:d352b201a1938ed46d030d55323cc26d&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Well, contrary to the confusion to what a casual glance of the code might  evoke in you, it&amp;rsquo;s really not terribly convoluted, but possibly slightly so.
The heart of the program is only one loop over the input set, while the win checking is a small constant set of finite bit operations, and bit operations can definitely be implemented as single machine instructions.
It surely ran fast on the small and large inputs, and so this question I got right.&lt;/p&gt;

&lt;p&gt;Not realizing that one only needed to get some of the problems correct in order to qualify for the actual contest, I happily shut down my computer and went to sleep.
Yay, I managed to do one of the CodeJam qualifier challenges, but there were no time to do all of them!
The next week when I realized my mistake, I was upset. I just had to get one or two more right and I could possibly have qualified!&lt;/p&gt;

&lt;p&gt;Less is more.  The saxophonist  &lt;a href=&#34;https://en.wikipedia.org/wiki/Jan_Garbarek&#34;&gt;Jan Garbarek&lt;/a&gt; has been credited with his &amp;ldquo;generous use of silence&amp;rdquo;.
I like that. May it be that, when people survey our code and data representations, may it be that they would reflect upon how few bits was used, or how few lines of code were written&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;full-program:d352b201a1938ed46d030d55323cc26d&#34;&gt;Full program&lt;/h2&gt;
using System;
using System.IO;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ConsoleApplication1
{
    class Program
    {
        public static int column, diagonal1, diagonal2, fullboard;

        static void Main(string[] args)
        {
            column = Convert.ToInt32(&#34;1000100010001&#34;, 2);
            diagonal1 = Convert.ToInt32(&#34;1000010000100001&#34;, 2);
            diagonal2 = Convert.ToInt32(&#34;0001001001001000&#34;, 2);
            fullboard = Convert.ToInt32(&#34;1111111111111111&#34;, 2);

            int xboard = 0, oboard = 0, tboard = 0;

            try
            {
                string infile = &#34;A-large.in&#34;, outfile = &#34;A-large.out&#34;, line = &#34;&#34;;
                using (StreamReader sr = new StreamReader(infile))
                {
                    using (StreamWriter sw = new StreamWriter(outfile))
                    {
                    int numberOfProblems = Int32.Parse(sr.ReadLine());
                    for (int i = 1; i &lt;= numberOfProblems; i++)
                    {
                        line = sr.ReadLine();
                        line += sr.ReadLine();
                        line += sr.ReadLine();
                        line += sr.ReadLine();

                        xboard = StringBoardToBitBoard(line, &#39;X&#39;);
                        oboard = StringBoardToBitBoard(line, &#39;O&#39;);
                        tboard = StringBoardToBitBoard(line, &#39;T&#39;);
                        if (Won(xboard, tboard))
                            sw.WriteLine(&#34;Case #{0}: X won&#34;, i);
                        else if (Won(oboard, tboard))
                            sw.WriteLine(&#34;Case #{0}: O won&#34;, i);
                        else if (((xboard | oboard | tboard) &amp; fullboard) == fullboard)
                            sw.WriteLine(&#34;Case #{0}: Draw&#34;, i);
                        else
                            sw.WriteLine(&#34;Case #{0}: Game has not completed&#34;, i);

                        line = sr.ReadLine();
                    }
                    }
                    //Console.WriteLine(line);
                }
            }
            catch (Exception e)
            {

            }
        }

        public static bool Won(int board, int tboard)
        {
            board |= tboard;  // apply the T position
            return LinesMatch(board) || ColumnsMatch(board) || DiagonalsMatch(board);
        }
        public static bool LinesMatch(int board)
        {
            return board == (board | 15)
                || board == (board | (15 &lt;&lt; 4))
                || board == (board | (15 &lt;&lt; 8))
                || board == (board | (15 &lt;&lt; 12));
        }
        public static bool ColumnsMatch(int board)
        {
            return
                board == (board | column)
                || board == (board | (column &lt;&lt; 1))
                || board == (board | (column &lt;&lt; 2))
                || board == (board | (column &lt;&lt; 3));
        }
        public static bool DiagonalsMatch(int board)
        {
            return board == (board | diagonal1)
                || board == (board | diagonal2);
        }

        public static int StringBoardToBitBoard(string stringBoard, char oneChar)
        {
            string bitBoard = new String(stringBoard.Select(
                x =&gt; x == oneChar ? &#39;1&#39; : &#39;0&#39;)
                .ToArray());
            return Convert.ToInt32(bitBoard, 2);
        }
    }
}

</description>
    </item>
    
    <item>
      <title>10 Ways to outsmart cyber criminals</title>
      <link>https://blog.goodfast.info/post/outsmart-cyber-crims/</link>
      <pubDate>Tue, 27 Jun 2017 14:14:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/outsmart-cyber-crims/</guid>
      <description>

&lt;p&gt;Big day; our first guest contribution! &lt;a href=&#34;http://www.proz.com/profile/1571336&#34;&gt;Johann Bergh&lt;/a&gt;, a professional translator and recovering software developer put together this list of 10 ways how you can be proactive and outsmart cyber criminals. Enjoy, and take action!&lt;/p&gt;

&lt;p&gt;Information is key to the success of cyber criminals. It is the driver that enables them to destroy, steal and extort. Cyber criminals are great detectives. They unite scraps of information from various sources into a nefarious plan.
&amp;ldquo;Whats the big deal?&amp;rdquo;, you may ask. You are the big deal, because you could be their next target. Your online presence puts you at risk. Reducing your personal online content is an important weapon in the fight against cyber criminals.&lt;/p&gt;

&lt;h3 id=&#34;1-social-media-you-don-t-have-to-be-famous:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;1.  Social media  you dont have to be famous&lt;/h3&gt;

&lt;p&gt;We all love social media. Posting cool pictures of yourself in weird and wonderful places, complemented by exotic people in the same photo frame. And its so much fun browsing the photos (and the latest gossip news) of family and friends.
Beware! The information that you post online is a treasure trove for foreign governments and cyber criminals alike. Be selective with what you share. If you took 100 photos of a weekend outing, select 3 or 4 appropriate pictures and be cognisant of the comments that you add. Too much information gives would-be identity thieves a foot hole &lt;a href=&#34;http://www.bbc.co.uk/webwise/0/21259413&#34;&gt;1&lt;/a&gt;.
Most social media platforms have privacy and security settings. Be sure to tighten them. You dont want everything to be available to the whole world!&lt;/p&gt;

&lt;h3 id=&#34;2-minimize-online-accounts:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;2.  Minimize online accounts&lt;/h3&gt;

&lt;p&gt;Lets be honest. The average person has accounts on more websites than they care to remember. Unfortunately, many sites only give access to their resources after you have created an account, resulting in an explosion of your digital footprint. In the world of cyber security, more information is bad news &lt;a href=&#34;http://www.pcworld.com/article/2143846/leave-no-trace-tips-to-cover-your-digital-footprint-and-reclaim-your-privacy.html&#34;&gt;5&lt;/a&gt;.
How can you reign in your online omniscience? Make an inventory of all the online accounts that you have. Your email inbox might be able to give you some hints if you have forgotten. Log into the accounts and delete them if they have fallen into disuse. If you are unable to delete the account, remove as much sensitive information as you can.&lt;/p&gt;

&lt;h3 id=&#34;3-clean-up-your-pc:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;3.  Clean up your PC&lt;/h3&gt;

&lt;p&gt;Did you know that your PCs hard drive contains documents with very personal and sensitive information? When is the last time that you cleaned up your files? Cyber criminals love information. Financial and personal information are very valuable to them.&lt;/p&gt;

&lt;p&gt;How much is the information on your PC worth to you? Intruders can install ransomware &lt;a href=&#34;http://www.bbc.com/news/uk-northern-ireland-37660657&#34;&gt;2&lt;/a&gt; on your PC. The result is that all your files are encrypted, and you wont be able to access it again unless you pay a ransom to the intruder. Delete unimportant files and move unused files with valuable data to an external hard drive (or to the cloud). Ransomware? No problem, reformat and start afresh.&lt;/p&gt;

&lt;h3 id=&#34;4-where-are-all-my-passwords:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;4.  Where are all my passwords?&lt;/h3&gt;

&lt;p&gt;There are so many websites that you sign up for and you dont want to create a unique password for each of them.  So you just reuse the same password across all of the sites. No problem, right? Wrong! Big problem &lt;a href=&#34;http://dl.acm.org/citation.cfm?id=1143127&#34;&gt;4&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Consider the following scenario. John creates an account on website X but decides to use the same password as his email website because its easier to remember.  Unfortunately, website X doesnt have the same security measures as Johns email provider. Cyber criminals can easily get Johns email address and the password that he used to sign into website X. The cyber criminals go to Johns email website and try to log in with the password John used on website X. And low and behold; they have access to Johns email inbox!&lt;/p&gt;

&lt;p&gt;Luckily, there are many free password managers that you can use to generate random passwords for different sites. Download a password manager and make use of it.&lt;/p&gt;

&lt;h3 id=&#34;5-don-t-be-the-weakest-link-keep-yourself-updated:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;5.  Dont be the weakest link, keep yourself updated&lt;/h3&gt;

&lt;p&gt;Cyber criminals, like pick-pockets, tend to prey on easy targets. Do you have virus protection and is it enabled? Is all your software up to date? Outdated software poses a significant risk &lt;a href=&#34;https://community.norton.com/en/blogs/norton-protection-blog/importance-general-software-updates-and-patches&#34;&gt;6&lt;/a&gt;. It is not uncommon for software packages to release patches on a weekly basis, and there is reason why they call some of these patches &amp;ldquo;security patches.&amp;rdquo; Enable automatic updates for all your installed software. These basic steps are easy to follow. Dont become the weakest link!&lt;/p&gt;

&lt;h3 id=&#34;6-avoid-the-seedy-places-on-the-web:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;6.  Avoid the seedy places on the web&lt;/h3&gt;

&lt;p&gt;Watching the latest series? You didnt get it from a torrent site. Or did you? Files downloaded from torrent sites often include a nasty surprise: malware &lt;a href=&#34;https://pdfs.semanticscholar.org/899b/9b82abe737118efe131b899bc1bdc3cc8938.pdf&#34;&gt;7&lt;/a&gt;. Software, with malicious intent, that will be installed on your PC without your consent. You are potentially giving cyber criminals access to your files and computing resources. Its a blank check. Theyre having a field day.
Torrent sites arent the only dubious, dark corners of the web. Porn sites and online gambling platforms are notorious contenders, inter alia.&lt;/p&gt;

&lt;h3 id=&#34;7-they-are-watching-you-don-t-get-tracked:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;7.  They are watching you  dont get tracked&lt;/h3&gt;

&lt;p&gt;Tracking, tracking, tracking. Online advertising is big business, and you might not be aware that everything you do on the web is tracked and analyzed.
Most websites that you visit store a small file on your computers hard drive called a &amp;ldquo;cookie&amp;rdquo;. These are often used to track your browsing activities for online advertising purposes. More information is available about you. It could give cyber criminals some valuable insights. Browsers have privacy and security settings that you could tighten to minimize tracking &lt;a href=&#34;http://www.zonealarm.com/blog/2014/07/online-privacy-how-to-minimize-your-digital-footprint/&#34;&gt;3&lt;/a&gt;. Browsers also have several plugins on offer that can help thwart tracking efforts by third parties. Eat the cookie before it eats you!&lt;/p&gt;

&lt;h3 id=&#34;8-choose-your-friends-carefully:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;8.  Choose your friends carefully&lt;/h3&gt;

&lt;p&gt;The internet is a great place to meet new people. Online friendships and romances are commonplace in the modern era of the internet.&lt;/p&gt;

&lt;p&gt;However, not all people using the internet are well-intentioned &lt;a href=&#34;http://edition.cnn.com/2011/11/11/tech/gaming-gadgets/kids-online-safety-steinberg/index.html&#34;&gt;9&lt;/a&gt;. The good news is that there are a few basic considerations that will shield you from internet fraudsters. Dont reply to dubious Emails. Messages dont go to your spam folder without reason. Maybe you are love sick. Do your research and join a reputable online dating site and you will meet plenty of like-minded individuals.&lt;/p&gt;

&lt;h3 id=&#34;9-don-t-leave-a-paper-trail:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;9.  Dont leave a paper trail&lt;/h3&gt;

&lt;p&gt;Despite all the propaganda about saving trees, hard copies of documents are more pervasive than ever. The information on physical documents is just as valuable as the information on digital documents.&lt;/p&gt;

&lt;p&gt;Dumpster divers are not only fictional characters that you see in the movies. They exist in the real world &lt;a href=&#34;http://verb.lib.lehigh.edu/index.php/verb/article/viewArticle/19/18&#34;&gt;8&lt;/a&gt;. How many hard copies of documents with sensitive information do you have? Maybe its time for a spring-clean. Consider investing in a paper shredder, if you like working with hard copies.&lt;/p&gt;

&lt;h3 id=&#34;10-physical-security:9a68116aa77ac38be69d0ef6e7b8e503&#34;&gt;10. Physical security&lt;/h3&gt;

&lt;p&gt;Physical security? What does that have to do with keeping cyber criminals at bay? The answer is simple: Grabbing a laptop is the easiest way to make a quick buck and get free access to juicy information &lt;a href=&#34;http://www.macleans.ca/education/uniandcollege/the-danger-of-laptop-theft/&#34;&gt;10&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It goes without saying that you shouldnt leave your electronic devices unattended in public areas. Laptop bags also tend to attract maliciously inclined individuals. A nifty trick is to get a backpack that has a laptop compartment. It will keep the criminals guessing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speed up slow views through custom materialization</title>
      <link>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</link>
      <pubDate>Thu, 08 Jun 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</guid>
      <description>

&lt;p&gt;SQL views are aluring as a means of abstraction; a &amp;ldquo;building block&amp;rdquo; to hide away commonly used complexity.
It is no wonder then that us developers will try them out, and before you know it, your clever recursive CTE view on that hierarchy is used everywhere, by everyone, but how is it affecting overall database performance&amp;hellip;&lt;/p&gt;

&lt;p&gt;They look like tables, can be joined on, selected from, and in some cases even updated just like tables, yet the reality is that they are not like tables.
So, you cannot consider a view to be a type of stored procedure, and you can also not consider a view to be a type of table or index; it is something in between.&lt;/p&gt;

&lt;p&gt;It is possible for the query planner to &amp;ldquo;reach into&amp;rdquo; a view, and discover which indexes to use in order to access information in the best way, but this quikcly breaks down once you perform any kind of complicated thing, such as a CTE, UNION statement, or anything else that breaks up the link from the source tables to the result set of the view.
When exactly you break this ability of the query planner to use appropriate indexes is a great idea for a future post - I have not found anything that directly states this as of yet.
Intuitivly  it makes sense that some kinds of data mangling will just make it impossible for the query planner to find indexes to use.&lt;/p&gt;
Note that it&#39;s of course always best to first inspect the query plan before concluding that a fiew is or is not making use of a particular index. I have made the mistake before of making grand statements on how poor the query planner is at choosing an index when dealing with a view, only to be shown that it in fact can do a bit more than what you might expect!

&lt;p&gt;My goal in this post is simply to make you aware of the possability that complex views might be causing your database to perform sub optimally, and then to offer an in place, zero downtime solution to the problem.&lt;/p&gt;

&lt;h3 id=&#34;when-to-use-views:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;When to use views&lt;/h3&gt;

&lt;p&gt;The way I currently understand it,  you should use views when you want different &lt;em&gt;views&lt;/em&gt; on the same table, or simple connected set of tables; i.e. you want to include/exclude certain columns/rows, so in other words, as a means of information hiding, a means of performing restricted access to the information in the underlying tables; a different take on the same data.
It is debatable how many new systems are developed, that would choose to deligate security, access restriction type of functionality to the database, but there is a fair chance that it is happening out in the wild, since a recent SQL Server feature is row-level access, and data masking.&lt;/p&gt;

&lt;h3 id=&#34;discovering-the-problem:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Discovering the problem&lt;/h3&gt;

&lt;p&gt;It was while I was performance tuning a very busy Azure Database, that I discovered a collection of particularly slow executing queries, spending most of their time in CPU.
The data volume involved could not account for the poor performance, being in the mere tens of thousands of small rows.
As far as I could determine, most of the appropriate indexes existed that would normally make things perform acceptably.
Something else was up&amp;hellip;&lt;/p&gt;

&lt;p&gt;Turning to the query plans, a pattern started emerging; slow, very slow views were joined on.
The views themselves were not very complex, but they did something interesting: they were recursive CTEs designed to traverse
a hierarchy, essentially a tree structure, and produce a full fan out of the entire tree.&lt;/p&gt;

&lt;h3 id=&#34;solution:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Solution&lt;/h3&gt;

&lt;p&gt;My first inclination was to have SQL Server materialize these views for me. Materialized (or indexed) views is an &lt;a href=&#34;http://sqlmag.com/database-performance-tuning/introducing-indexed-views&#34;&gt;old feature of the server&lt;/a&gt;, dating back to SQL Server 2000 if I&amp;rsquo;m not mistaking, so surely in 2017 this should be completely possible.
Well, it turns out that in order for a view to be materialized, &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/relational-databases/views/create-indexed-views&#34;&gt;a whole list of requirements&lt;/a&gt; need to be satisfied. For example, something as innocently looking as a LEFT JOIN in the view query would put a quick end to this solution path.&lt;br /&gt;
Researching it a bit further shed some light on why all these restrictions apply, but although it does make you be a bit more understanding, it still feels like this is something that should be possible, no matter how complex the view is.&lt;/p&gt;

&lt;p&gt;Completely redesigning the underlying hierarchical representation, with something like transative closures was not really an option, so the next best idea was to custom materialize these views.
The data access characteristics of the hierarchy and supporting tables was that they did not change all that often, yet they were queried all the time.
This was great news, since it meant that even if the materialization process took a bit of time, this would quickly be compensated for by the much, much faster query times.
Having the previously computed data now reside in a proper table also meant that it could be appropriately indexed, clustered, and even partitioned (although the volume was far too low for this need).&lt;/p&gt;

&lt;h4 id=&#34;the-procedure:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;The procedure&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Create a table (we&amp;rsquo;ll call it the Working) that structurally mirrors the result of querying the view (the View, later to be renamed to the Origin).&lt;/li&gt;
&lt;li&gt;Create a stored procedure (RefreshWorking) that will make use of Origin to refresh Working.&lt;/li&gt;
&lt;li&gt;Create AFTER triggers for all tables referenced by Origin, that will call RefreshWorking.&lt;/li&gt;
&lt;li&gt;Make the triggers intelligent in that they will only call RefreshWorking when the DML operation of the source table would actually affect the outcome of the Origin view.&lt;/li&gt;
&lt;li&gt;Optionally pass the source table name and the key values, through a table valued parameter to RefreshWorking, so that the procedure can more intelligently pick out which parts of Working will need refreshing.&lt;/li&gt;
&lt;li&gt;Create a view, CheckWorkingAndOrigin, that FULL JOIN view Origin and table Working, to ensure that they are identical.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once all of this is done, it is time to test.
For your testing I can highly recommend &lt;a href=&#34;http://tsqlt.org/&#34;&gt;tSQLt&lt;/a&gt;; a completely T-SQL based unit testing system.
When you have assured yourself that RefreshWorking properly updates table Working, it is time for the deployment.&lt;/p&gt;

&lt;p&gt;In one transaction, rename  the original, slow view to Origin, create a synonym with the same name as the original slow view, and point the synonym at the Working table.
As the last step of the transaction, run RefreshWorking procedure so that the Working table will get properly updated and be primed for showtime.&lt;/p&gt;

&lt;h3 id=&#34;reward:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Reward&lt;/h3&gt;

&lt;p&gt;After we implemented this procedure on a heavily queried complex view, we saw a query plan simplification going from
over 70 steps, to only 3 steps. More impressive is that the plan now took 481 times less CPU time!
The RefreshWorking procedure still called the original, slow, complex view, but it did this only when the source tables changed and in particular ways.
The procedure also minimized writes to the Working table, to prevent table locking for the heavy reading on it.&lt;/p&gt;

&lt;h3 id=&#34;summary:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Summary&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;One way we make sense of the world is by modelling relationships between things as a tree-like hierarchy.&lt;/li&gt;
&lt;li&gt;Most database systems, however, are essentially flat when it comes to the most basic collection of storage; the table.&lt;/li&gt;
&lt;li&gt;We overcome the flatness in one particular way, the simpler  of possible ways, by representing the unbounded nested characteristic of hierarchies by self referencing records in a table.&lt;/li&gt;
&lt;li&gt;Self referencing records in a single table makes manipulation of the hierarchy a very simple operation, but this simplicity comes at a cost when you want to traverse the hierarchy.&lt;/li&gt;
&lt;li&gt;For traversing arbitrarily  deep hierarchies represented by self referencing records, you inevitably require recursion.&lt;/li&gt;
&lt;li&gt;Recursive CTEs break the essential link between the source tables and the view result set, making it impossible for the query planner to do anything but perform the entire process of the view&amp;rsquo;s query, even when you only desire a small subset.&lt;/li&gt;
&lt;li&gt;In the scenario where the source tables for the complex view are written to less than they are read from, you can optimize the complex view by materializing it into a concrete table.&lt;/li&gt;
&lt;li&gt;The materialized table can then be properly indexed for maximum query performance, at a relatively small index maintenance cost at write time.&lt;/li&gt;
&lt;li&gt;With this approach you are trading computation time for storage space.&lt;/li&gt;
&lt;li&gt;The writing of the materialized table happens on DML AFTER triggers, so that you first have the change written to the source tables before the materialized table is updated.&lt;/li&gt;
&lt;li&gt;Updating of the materialized table need not be a complete rewrite; the AFTER triggers can be programmed so that they only fire when columns that partake in the SELECT list for the origin view query change.&lt;/li&gt;
&lt;li&gt;A further optimization can be made where by the refresh stored procedure recieves a list of keys of rows that changed, and can then use this info to only update the materialized table where it actually needs to change.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope that you will try out this procedure on slow views on your databases; it has really helped us a lot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Knapsack Bitwise</title>
      <link>https://blog.goodfast.info/post/knapsack-bitwise/</link>
      <pubDate>Wed, 12 Oct 2016 22:01:20 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/knapsack-bitwise/</guid>
      <description>

&lt;p&gt;An interesting bit of computer science, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Knapsack_problem&#34;&gt;knapsack problem&lt;/a&gt; has been studied for over a century, and according to Wikipedia, seems to be quite  popular - as these sort of things go.
For the first post in this series I&amp;rsquo;ll present a solution to the &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;0&lt;/sub&gt;, or binary version of this famous problem
I designed in 2015.&lt;/p&gt;

&lt;p&gt;I was immediately intrigued by it when I first read the problem statement.
It&amp;rsquo;s application to anything requireing optimal resource allocation was
very clear, and my mind started obsessively thinking of how to solve this efficiently.
Now with a heavily studied problem like this, there are of course already many algorithms
developed, but since this problem was a test, I did not look anything up, and just started coding a solution as soon as I had one.&lt;/p&gt;

&lt;p&gt;The full solution is &lt;a href=&#34;https://github.com/Kerneels/knapsack&#34;&gt;available here on GitHub&lt;/a&gt;, but you are encouraged to copy and paste from this article, into your own project, to understand the whole thing bit by bit.&lt;/p&gt;

&lt;h2 id=&#34;what-is-it:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;What is it?&lt;/h2&gt;

&lt;p&gt;The 0/1 or binary variant is very simple. Given a set of items, where each item has a weight and value, determine the optimal
selection of items such that the sum of the weight of all the items do not exceed some limit,
while the sum of the value of all of the items is maximised.&lt;/p&gt;

&lt;p&gt;The 0/1 or binary part of the name comes from the restriction that only one of each item may be chosen.
The &amp;ldquo;knapsack&amp;rdquo; in the name refers to a ficticious rugsack or bag that can only contain a given weight.
The complexity of the problem lies in the exponential explosion of all the possible selection of items.&lt;/p&gt;

&lt;h2 id=&#34;a-concrete-example:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;A concrete example&lt;/h2&gt;

&lt;p&gt;Suppose we have the following set of 15 items:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Number&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Weight Grams&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;map&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;compass&lt;/td&gt;
&lt;td&gt;130&lt;/td&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;water&lt;/td&gt;
&lt;td&gt;1530&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Gold bar&lt;/td&gt;
&lt;td&gt;3000&lt;/td&gt;
&lt;td&gt;130&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;sandwich&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;glucose&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;tin&lt;/td&gt;
&lt;td&gt;680&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;banana&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;apple&lt;/td&gt;
&lt;td&gt;390&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;cheese&lt;/td&gt;
&lt;td&gt;230&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;beer&lt;/td&gt;
&lt;td&gt;620&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;suntan cream&lt;/td&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;camera&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;T-shirt&lt;/td&gt;
&lt;td&gt;240&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;trousers&lt;/td&gt;
&lt;td&gt;480&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Our knapsack can only hold a maximum of 4 Kg or 4000 grams, but we want to choose  a selection of items (or inventory) with the highest possible value.&lt;/p&gt;

&lt;h2 id=&#34;how-many-possible-inventories-exist:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;How many possible inventories exist?&lt;/h2&gt;

&lt;p&gt;Given we have &lt;code&gt;n&lt;/code&gt; items to choose from, each with a weight &lt;code&gt;w&lt;/code&gt; and value &lt;code&gt;v&lt;/code&gt;, we observe that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We will never be choosing zero items.&lt;/li&gt;
&lt;li&gt;We can either choose to include an item or not.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We take a stab at a formula for calculating the number of choices: &lt;code&gt;c = 2^n - 1&lt;/code&gt;. The &lt;code&gt;2^n&lt;/code&gt; is because each item can either be chosen or not chosen, and the &lt;code&gt;- 1&lt;/code&gt; is to eliminate the selection of not choosing anything.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how it holds for one item: &lt;code&gt;c = 2^1 - 1 = 2 - 1 = 1&lt;/code&gt;, obvious, since with one item you only have one choice - this can be our base case, so for &lt;code&gt;n = 1&lt;/code&gt; the formula holds.
Now we consider &lt;code&gt;n + 1&lt;/code&gt;, so for 2: &lt;code&gt;c = 2^2 - 1 = 4 - 1 = 3&lt;/code&gt;, and this makes sense since you can either choose one of the items or both, so for &lt;code&gt;n + 1&lt;/code&gt; the formula also holds.
So, by mathematical induction, our formula is proven, although we knew it was going to be right intuetively.&lt;/p&gt;

&lt;p&gt;Using our formula, we conclude that there are &lt;code&gt;c = 2^15 - 1 = 32768 - 1 = 32767&lt;/code&gt; possible inventories.&lt;br /&gt;
This number is neglegeable in computer terms, yet already far too big for by hand calculation.&lt;/p&gt;

&lt;h2 id=&#34;brute-force-solution:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Brute force solution&lt;/h2&gt;

&lt;p&gt;Since our particular senario is so small we will simply do a brute force search to find the best inventories (if any) that satisfy the 4000 grams weight limit.&lt;/p&gt;

&lt;p&gt;Any inventory can only have a maximum of 15 items, so we will represent an arbitrary inventory with a
bit mask, where each bit will represent a particular item, with 0 meaning the item is not chosen, and 1 meaning the item was chosen.&lt;/p&gt;

&lt;p&gt;The bit mask will drive a calculation function to determine the total weight and value of a given inventory.
Considering all possible inventories is now reduced to iterating over the number 1 through 32767 possible options,
calculating the sum of the weights and values each time, and retaining those inventories where the weight limit is satisfied.&lt;/p&gt;

&lt;h2 id=&#34;complexity:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Complexity&lt;/h2&gt;

&lt;p&gt;Using our formula, we can say that our solution will have &lt;code&gt;O(2^n - 1)&lt;/code&gt; time complexity, and similarly, worst case &lt;code&gt;O(2^n - 1)&lt;/code&gt;
space complexity.&lt;/p&gt;

&lt;h2 id=&#34;implementation:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Implementation&lt;/h2&gt;

&lt;h3 id=&#34;belonging-and-inventory-class:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Belonging and Inventory class&lt;/h3&gt;

&lt;p&gt;Looking at the table of belongings, we realise we would need a class to
represent individual belongings (we&amp;rsquo;ll call it &lt;code&gt;class Belonging&lt;/code&gt;), as well as the set of all   of them (&lt;code&gt;class Inventory&lt;/code&gt;).&lt;/p&gt;
namespace Knapsack
{
public class Belonging
	{
		public byte Number { get; set; }
		public string Name { get; set; }
		public int GramsWeight { get; set; }
		public int Value { get; set; }
	}

	public class Inventory
	{
		public static List&lt;Belonging&gt; AllGear { get; set; }

		public uint Gear { get; set; }
	}
}

&lt;h3 id=&#34;functions-for-belonging-class:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Functions for Belonging class&lt;/h3&gt;

&lt;p&gt;Now that we&amp;rsquo;ve dealt with the data needs, let&amp;rsquo;s add some functionality to each of the classes.
In true &lt;a href=&#34;http://agiledata.org/essays/tdd.html&#34;&gt;test driven development&lt;/a&gt; style (also see &lt;a href=&#34;https://en.wikipedia.org/wiki/Test-driven_development&#34;&gt;Wikipedia on TDD&lt;/a&gt;), let&amp;rsquo;s first create unit tests for each function, followed by an implementation.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll wrap all the test classes in a general &lt;code&gt;TestKnapsack&lt;/code&gt; class which we can use to do setup and teardown actions if required.&lt;/p&gt;
namespace Knapsack
{
	[TestClass]
	public class TestKnapsack
	{
		[TestClass]
		public class TestBelonging : TestKnapsack
		{
			[TestMethod]
			public void ShouldAddRemoveAndConfirmItIsInGear()
			{
				uint gear = 0; // empty set of gear
				var testBelonging = new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 };

				Assert.IsFalse(testBelonging.IsInGear(gear));
				uint gearAfterAdd = testBelonging.AddToGear(gear);
				Assert.IsFalse(testBelonging.IsInGear(gear));
				Assert.IsTrue(testBelonging.IsInGear(gearAfterAdd));
				Assert.AreNotEqual(gearAfterAdd, gear);

				uint gearAfterRemove= testBelonging.RemoveFromGear(gearAfterAdd);
				Assert.AreEqual(gearAfterRemove, gear);
				Assert.IsFalse(testBelonging.IsInGear(gearAfterRemove));
			}
		}
	}
}

&lt;p&gt;Returning to our &lt;code&gt;Belonging&lt;/code&gt; class,  let&amp;rsquo;s implement the methods we described in our unit test class.&lt;/p&gt;

&lt;p&gt;We will make heavy use of the bitwise operators:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; : bit shifts, which moves all the bits in a number left or right by the given count&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; : bitwise AND and OR which combines the two numbers bit by bit and returns the resulting number&lt;/li&gt;
&lt;li&gt;&lt;code&gt;~&lt;/code&gt; : the compliment or negation unary operator that inverts all bits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Make sure you are completely familiar with all the &lt;a href=&#34;https://www.tutorialspoint.com/csharp/csharp_bitwise_operators.htm&#34;&gt;C# bit wise operators&lt;/a&gt; before proceeding.&lt;/p&gt;

&lt;p&gt;Here are the functions we add to &lt;code&gt;Belonging&lt;/code&gt;:&lt;/p&gt;
public uint AddToGear(uint gear)
		{
			return gear | (uint)(1 &lt;&lt; this.Number);
		}
		public uint RemoveFromGear(uint gear)
		{
			return gear &amp; ~(uint)(1 &lt;&lt; this.Number);
		}
		public bool IsInGear(uint gear)
		{
			return (gear &amp; (1 &lt;&lt; this.Number)) == (1 &lt;&lt; this.Number);
		}
		public int GramsWeightInGear(uint gear)
		{
			return this.IsInGear(gear) ? this.GramsWeight : 0;
		}
		public int ValueInGear (uint gear)
		{
			return this.IsInGear(gear) ? this.Value : 0;
		}

&lt;p&gt;Success! Our tests all pass, and we can proceed to testing and developing &lt;code&gt;Inventory&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;functions-for-inventory:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Functions for Inventory&lt;/h3&gt;

&lt;p&gt;We start by testing the very simple summation functions.
Our test creates a sample set of belongings (&lt;code&gt;allKit&lt;/code&gt;) with 3 items in it.
We then add the first and last item from &lt;code&gt;allKit&lt;/code&gt; and ensure we get the correct totals when calling the summation functions on
&lt;code&gt;Inventory&lt;/code&gt;:&lt;/p&gt;
		[TestClass]
		public class TestInventory : TestKnapsack
		{
			[TestMethod]
			public void ShouldSumProperly()
			{
				var allKit = new List&lt;Belonging&gt;();
				allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
				allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
				allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });

				Inventory.AllGear = allKit;
				var testInventory = new Inventory { Gear = 0 };
				testInventory.Gear = allKit[0].AddToGear(
					testInventory.Gear);
				testInventory.Gear = allKit[2].AddToGear(
					testInventory.Gear);

				Assert.AreEqual(allKit[0].GramsWeight + allKit[2].GramsWeight,
					testInventory.TotalGramsWeight);
				Assert.AreEqual(allKit[0].Value + allKit[2].Value,
					testInventory.TotalValue);
			}
		}

&lt;p&gt;Here are the summation functions to add to &lt;code&gt;Inventory&lt;/code&gt;:&lt;/p&gt;

		public int TotalGramsWeight { get { return Inventory.TotalGramsWeightForGear(this.Gear); } }
		public int TotalValue { get { return Inventory.TotalValueForGear(this.Gear); } }

		public static int TotalGramsWeightForGear (uint gear)
		{
				return AllGear.Select(o =&gt; 
					o.GramsWeightInGear(gear))
					.Sum();
			}

		public static int TotalValueForGear(uint gear)
		{
				return AllGear.Select(o =&gt; 
					o.ValueInGear(gear))
					.Sum();
			}

&lt;p&gt;Nothing too complicated, and the tests all still pass. Next we move on to the actual search&lt;br /&gt;
for the valid inventories:&lt;/p&gt;

&lt;h3 id=&#34;search-functions-for-inventory:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Search functions for Inventory&lt;/h3&gt;

			[TestMethod]
			public void TestFirstBestInventory()
			{
				var allKit = new List&lt;Belonging&gt;();
				allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
				allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
				allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });
				Inventory.AllGear = allKit;

				Assert.IsTrue(
					allKit[1].IsInGear(
					Inventory.FirstBestInventory(220).Gear));

				Assert.IsFalse(
					allKit[2].IsInGear(
					Inventory.FirstBestInventory(220).Gear));
			}

			[TestMethod]
			public void TestValidInventories()
			{
				var allKit = new List&lt;Belonging&gt;();
				allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
				allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
				allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });
				Inventory.AllGear = allKit;

				// nothing except the empty inventory - so only one possible empty inventory
				Assert.AreEqual(1, Inventory.ValidInventories(5).Count());

				// since the upper weight limit is so large we end up with all possible inventories over 3 belongings, which is 8 including the empty inventory
				Assert.AreEqual(8, Inventory.ValidInventories(5000).Count());
			}

&lt;p&gt;We implement the new functions on &lt;code&gt;Invetory&lt;/code&gt; as follows:&lt;/p&gt;
		public static uint NumberOfCombinations
		{
			get
			{
				return (~(uint)0 % (uint)(1 &lt;&lt; AllGear.Count()));
			}
		}

		public static Inventory FirstBestInventory(int maxGramsWeight)
		{
			var numberOfCombinations = Inventory.NumberOfCombinations;
			int currentMaxValue = 0, overallBestValue = 0;
			var bestInventory = new Inventory();

			for (uint g = 0; g &lt;= numberOfCombinations; g++)
				if (Inventory.TotalGramsWeightForGear(g) &lt;= maxGramsWeight &amp;&amp;
					(currentMaxValue = Inventory.TotalValueForGear(g)) &gt; overallBestValue)
				{
					bestInventory = new Inventory
					{
						Gear = g
					};
					overallBestValue = currentMaxValue;
				}

			return bestInventory;
		}

		public static IQueryable&lt;Inventory&gt; ValidInventories(int maxGramsWeight)
		{
			var numberOfCombinations = Inventory.NumberOfCombinations;
			var validInventories = new List&lt;Inventory&gt;();

			for (uint g = 0; g &lt;= numberOfCombinations; g++)
				if (Inventory.TotalGramsWeightForGear(g) &lt;= maxGramsWeight)
					validInventories.Add(new Inventory
					{
						Gear = g
					}
					);

			return validInventories.AsQueryable();
		}

&lt;h3 id=&#34;final-wrapup:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Final wrapup&lt;/h3&gt;

&lt;p&gt;After testing everything again and getting all tests to still pass, we are very nearly done.
Our final work will be to create nice printout &lt;code&gt;ToString()&lt;/code&gt; methods, and putting it all together in a &lt;code&gt;main&lt;/code&gt; method which will be the entry point for the console application.&lt;/p&gt;

&lt;p&gt;We add the following to &lt;code&gt;Belonging&lt;/code&gt;:&lt;/p&gt;

		public override string ToString()
		{
			return string.Format(&#34;{0}, {1} g, valued at {2}&#34;,
				this.Name,
				this.GramsWeight,
				this.Value);
		}

&lt;p&gt;We want nice printout for &lt;code&gt;Inventory&lt;/code&gt; also, so we add the following to it:&lt;/p&gt;
public override string ToString()
		{
			StringBuilder sb = new StringBuilder();
			sb.AppendLine(&#34;---- Inventory Start:&#34;);
			AllGear.Where(o =&gt; o.IsInGear(this.Gear))
				.ToList()
				.ForEach(o =&gt; sb.AppendLine(o.ToString()));
			sb.AppendFormat(&#34;---- Inventory End: Total Weight: {0} g, Total Value: {1}&#34;,
				this.TotalGramsWeight,
				this.TotalValue);
			return sb.ToString();
		}

&lt;p&gt;Finally, we add the &lt;code&gt;main&lt;/code&gt; method, along with a little helper method for setting up our set of all kit:&lt;/p&gt;
class Program
	{
		static void Main(string[] args)
		{
			var allKit = LoadData();

			Inventory.AllGear = allKit;

			var bestInventories = Inventory
				.ValidInventories(maxGramsWeight: 4000)
				.OrderByDescending(o =&gt; o.TotalValue)
				.Take(5)
				.ToList();

			if (bestInventories.Count() == 0)
				Console.WriteLine(&#34;No inventory match the requirements.&#34;);
			else
			Console.WriteLine(string.Format(&#34;Best {0} inventories in descending order of value are:&#34;,bestInventories.Count()));
			bestInventories.ForEach(o =&gt;
				Console.WriteLine(o)
				);

			var firstBestInventory = Inventory.FirstBestInventory( maxGramsWeight: 4000 );
			if (firstBestInventory == null)
				Console.WriteLine(&#34;No first best inventory found.&#34;);
			else
			{
				Console.WriteLine(&#34;Best inventory found: &#34;);
				Console.WriteLine(firstBestInventory);
			}
		}

		private static List&lt;Belonging&gt; LoadData()
		{
			var allKit = new List&lt;Belonging&gt;();
			allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
			allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
			allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });
			allKit.Add(new Belonging { Number = 3, Name = &#34;Gold bar&#34;, GramsWeight = 3000, Value = 130 });
			allKit.Add(new Belonging { Number = 4, Name = &#34;sandwich&#34;, GramsWeight = 500, Value = 160 });
			allKit.Add(new Belonging { Number = 5, Name = &#34;glucose&#34;, GramsWeight = 150, Value = 60 });
			allKit.Add(new Belonging { Number = 6, Name = &#34;tin&#34;, GramsWeight = 680, Value = 45 });
			allKit.Add(new Belonging { Number = 7, Name = &#34;banana&#34;, GramsWeight = 270, Value = 60 });
			allKit.Add(new Belonging { Number = 8, Name = &#34;apple&#34;, GramsWeight = 390, Value = 40 });
			allKit.Add(new Belonging { Number = 9, Name = &#34;cheese&#34;, GramsWeight = 230, Value = 30 });
			allKit.Add(new Belonging { Number = 10, Name = &#34;beer&#34;, GramsWeight = 620, Value = 10 });
			allKit.Add(new Belonging { Number = 11, Name = &#34;suntan cream&#34;, GramsWeight = 110, Value = 70 });
			allKit.Add(new Belonging { Number = 12, Name = &#34;camera&#34;, GramsWeight = 320, Value = 30 });
			allKit.Add(new Belonging { Number = 13, Name = &#34;T-shirt&#34;, GramsWeight = 240, Value = 15 });
			allKit.Add(new Belonging { Number = 14, Name = &#34;trousers&#34;, GramsWeight = 480, Value = 10 });

			return allKit;
		}
	}

&lt;h2 id=&#34;finished-let-s-give-it-a-spin:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Finished! Let&amp;rsquo;s give it a spin&amp;hellip;&lt;/h2&gt;

&lt;p&gt;We are done! Let&amp;rsquo;s see what we get if we run it all:&lt;/p&gt;
Best 5 inventories in descending order of value are:
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
camera, 320 g, valued at 30
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3960 g, Total Value: 950
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
camera, 320 g, valued at 30
---- Inventory End: Total Weight: 3720 g, Total Value: 935
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
tin, 680 g, valued at 45
banana, 270 g, valued at 60
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3930 g, Total Value: 925
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
tin, 680 g, valued at 45
banana, 270 g, valued at 60
apple, 390 g, valued at 40
suntan cream, 110 g, valued at 70
---- Inventory End: Total Weight: 3850 g, Total Value: 920
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3640 g, Total Value: 920
Best inventory found: 
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
camera, 320 g, valued at 30
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3960 g, Total Value: 950


&lt;h2 id=&#34;conclusion:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have solved the 0/1  version of the knapsack problem, and it has been fun!
Not only is our solution well tested, but it runs fast, for small numbers of items.&lt;/p&gt;

&lt;p&gt;In subsequent articles in this series we will expand on our solution, test it for larger number of items, and hopefully tackle the
other, harder versions of this interesting computer science problem.&lt;/p&gt;

&lt;p&gt;Who knows, perhaps we&amp;rsquo;ll even take a stab at a quantum algorithm!&lt;/p&gt;

&lt;p&gt;If only we could do this type of programming all day long&amp;hellip;&lt;/p&gt;

&lt;p&gt;The full solution is &lt;a href=&#34;https://github.com/Kerneels/knapsack&#34;&gt;available here, on GitHub&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CTE : simplify those nested sub queries</title>
      <link>https://blog.goodfast.info/post/cte-sub-queries-simplified/</link>
      <pubDate>Sun, 09 Oct 2016 21:26:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/cte-sub-queries-simplified/</guid>
      <description>

&lt;p&gt;This article examines how sub queries can be substituted for the far more readable common table expressions, or CTEs available in many RDBMS systems.
I was motivated to write this article when a friend who is fairly new to SQL expressed difficulty in grasping queries containing nested sub queries.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;ve never heard of CTEs before and you want to get the most out of this article, I recommend you get &lt;a href=&#34;https://msftdbprodsamples.codeplex.com/releases/view/125550&#34;&gt;AdventureWorks2014&lt;/a&gt; sample database and experiment a little with the queries below.
Adventureworks is a nice sample database designed to demonstrate SQL Server features, and many examples online makes use of it.
If on the other hand CTEs are old hat to you a quick skim reading of the below will suffice, but pay attention since there is something
interesting below which you might never have known, or maybe forgotten.&lt;/p&gt;

&lt;h2 id=&#34;is-it-just-me-or-is-this-strange:0dc054d968169eec76f7161b9d98121b&#34;&gt;Is it just me, or is this strange?&lt;/h2&gt;

&lt;p&gt;No-no, this thing came from long, long ago, was what I was thinking
the first time I was confronted with SQL SELECT syntax.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s all the wrong way round.
Take the most basic form of  a SELECT statement for example&amp;hellip;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You start out with what you want in the column list while starting the SELECT statement.&lt;/li&gt;
&lt;li&gt;Next you proceed to say from where you want the data  to come from, in the FROM clause.&lt;/li&gt;
&lt;li&gt;Then you stipulate  how  you want to filter what is returned when you write the WHERE clause.&lt;/li&gt;
&lt;li&gt;Finally  you end off with how stuff should be ordered in the ORDER BY clause.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, I do realise that there are (were) probably very good reasons why this particular order was chosen, but
would it not make more sense to rather start out with saying where you want to draw info from (FROM clause), proceed to filter that with the WHERE clause, followed by your preferred ordering in the ORDER BY, and finishing up with the list of which columns you are interested in, e.g:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;FROM&lt;/li&gt;
&lt;li&gt;WHERE&lt;/li&gt;
&lt;li&gt;ORDER BY&lt;/li&gt;
&lt;li&gt;SELECT&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Well, it is really of no consequence now - we have come too far along this road, however, note that the fluent way of constructing queries
with .NET C# Linq offer exactly this; a more logical flow of steps.&lt;/p&gt;

&lt;p&gt;This post is about simplifying subqueries, so getting back on track, it turns out there is a powerful feature of SQL99 called
Common Table Expressions, or CTEs which offers a great help when you are dealing with a monster query with little minions of subqueries, nested deeper in there than  your TV remote that sunk into the couch folds.&lt;/p&gt;

&lt;p&gt;Whereas sub query syntax quickly gets confusing and convoluted, using the CTE approach breaks down complexity and deals with each step of the workload in separate parts before moving on to
constructing a more intertwined picture.&lt;/p&gt;

&lt;p&gt;To me, it somewhat moves towards my &amp;ldquo;more ideal&amp;rdquo; ordering of statements, all be it no real alteration to the standard SELECT syntax.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at an example.&lt;/p&gt;

&lt;h2 id=&#34;adventureworks-customers-and-their-addresses:0dc054d968169eec76f7161b9d98121b&#34;&gt;Adventureworks customers and their addresses&lt;/h2&gt;

&lt;p&gt;In Adventureworks, customers are modeled in the person.BusinessEntity table, and their addresses in the person.Address table.
The relation between customers and their addresses is maintained in the person.BusinessEntityAddress table, which adds a type
for each address (Billing, Home, Main Office etc).&lt;/p&gt;

&lt;h2 id=&#34;we-want-to-know-how-many-customers-have-one-two-and-three-or-more-addresses:0dc054d968169eec76f7161b9d98121b&#34;&gt;We want to know how many customers have one, two and three or more addresses.&lt;/h2&gt;
Since the purpose of this article is to discuss subquery vs CTEs, excuse the somewhat contrived query.
At the end of this article I&#39;ll show  a simpler way to accomplish the tast at hand.

&lt;h2 id=&#34;sub-query-building-block:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query building block&lt;/h2&gt;

&lt;p&gt;We can establish that no BusinessEntity has more than two addresses by the empty result set returned by:&lt;/p&gt;
select ba.BusinessEntityId, count(ba.AddressId)
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2;

&lt;p&gt;We&amp;rsquo;ll assume for the remainder of this article that  this query is our best attempt, and that
it is a &amp;ldquo;black box&amp;rdquo; with which we cannot tamper, save for adjusting the count in the HAVING clause.&lt;/p&gt;

&lt;p&gt;Note that technically we do not need the additional join:&lt;/p&gt;
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId

&lt;p&gt;however this ensures we do not consider addresses in isolation - we ensure all addresses actually have an existing business tied to them.&lt;/p&gt;

&lt;h2 id=&#34;sub-query-with-count-of-zero-instead-of-nothing:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query with count of zero instead of nothing&lt;/h2&gt;

&lt;p&gt;Now we would like to improve this result a bit, by rather returning a count of zero instead of nothing.
We do this by wrapping the above query in a super query:&lt;/p&gt;
select 
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses;

&lt;p&gt;Not too bad in terms of readability, and since we alias the sub query we always know where the result for the outer select column list comes from.
But what if we would like to augment this query to also return the number of businesses with only one, and two addresses respectively?
Or to cover all possabilities, the number of businesses with one, two and three or more addresses respectively.
Let&amp;rsquo;s try with the sub query approach:&lt;/p&gt;

&lt;h2 id=&#34;combined-sub-query-approach:0dc054d968169eec76f7161b9d98121b&#34;&gt;Combined sub query approach&lt;/h2&gt;
select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress,
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses,
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses;

&lt;p&gt;Hmm&amp;hellip; We get zero for everything. Something has gone wrong.
Running each super/sub query pair separately we obtain a number for the one and two address scenario, but nothing for the three or more one.
Somehow, the zero result from the three or more address scenario is forcing the other two to become zero as well.
Well, where else would two hole numbers and zero uniformly combined yeald zero?
Multiplication of course, and observing how we unwittingly join the three subqueries with a comma (&amp;ldquo;,&amp;rdquo;) which is nothing but a
cartesian or cross join, everything start to make sense!
The cross join is causing our result set to first be the number of rows returned by the first subquery,  but then these rows are &amp;ldquo;multiplied&amp;rdquo; in the first cross join with the two address subquery, at which point we are already off track, and finally, to make matters even
worse, the last subquery returns zero rows, and anything multiplied by zero is of course again zero.&lt;/p&gt;

&lt;h2 id=&#34;fixup-by-throwing-more-sub-queries-at-the-problem:0dc054d968169eec76f7161b9d98121b&#34;&gt;Fixup by throwing more sub queries at the problem&lt;/h2&gt;

&lt;p&gt;Now, let&amp;rsquo;s first remedy this situation by cross joining on only one row each time.
Hold on, it is going to get a bit messy since in order to do this, we have to introduce yet another subquery.&lt;/p&gt;
select 
A.NumberOfBusinessesWithOneAddress,
B.NumberOfBusinessesWithTwoAddresses,
C.NumberOfBusinessesWithMoreThanTwoAddresses
from
(select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress) as A,
(select
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses) as B,
(select
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses) as C;

&lt;p&gt;Success, but at quite a complexity and readability cost.&lt;/p&gt;

&lt;h2 id=&#34;first-cte-attempt:0dc054d968169eec76f7161b9d98121b&#34;&gt;First CTE attempt&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s see if we can simplify this into something more readable by using a few CTEs instead of so many subqueries.&lt;/p&gt;

&lt;p&gt;Our query will consist of 3 parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First, the three innermost sub queries are stated as CTEs named businessEntitiesWithOneAddress, businessEntitiesWithTwoAddresses and businessEntitiesWithMoreThanTwoAddresses.&lt;/li&gt;
&lt;li&gt;Next, the encapsulating super queries that reduce the sub queries to  single values are also stated as CTEs named A, B and C.&lt;/li&gt;
&lt;li&gt;Finally we simply select from the aggregation CTEs A, B and C.&lt;/li&gt;
&lt;/ol&gt;
-- First, the three innermost sub queries are stated as CTEs named businessEntitiesWithOneAddress, businessEntitiesWithTwoAddresses and businessEntitiesWithMoreThanTwoAddresses.
with  businessEntitiesWithOneAddress as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
), businessEntitiesWithTwoAddresses as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
), businessEntitiesWithMoreThanTwoAddresses  as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
), 

-- Next, the encapsulating super queries that reduce the sub queries to  single values are also stated as CTEs named A, B and C.
A as
(select count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress
from businessEntitiesWithOneAddress
), B as
(select count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses
from businessEntitiesWithTwoAddresses
), C as
(select count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from businessEntitiesWithMoreThanTwoAddresses
)

-- Finally we simply select from the aggregation CTEs A, B and C.
select
A.NumberOfBusinessesWithOneAddress,
B.NumberOfBusinessesWithTwoAddresses,
C.NumberOfBusinessesWithMoreThanTwoAddresses
from A, B, C;

&lt;h2 id=&#34;is-this-not-so-much-more-readable:0dc054d968169eec76f7161b9d98121b&#34;&gt;Is this not so much more readable?&lt;/h2&gt;

&lt;p&gt;This last query (with CTEs only) is functionally equivalent   to the full query with only sub queries, yet, personally I find the CTE-based one
significnatly more readable and understandable.
For me there are many reasons for this, but I list a few:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instead of a nested style, the CTEs flow in a top to bottom manner.&lt;/li&gt;
&lt;li&gt;Each similar set of queries are grouped together.&lt;/li&gt;
&lt;li&gt;The CTEs are named upfront, so you have an idea of what is being done right from the start.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;cte-support:0dc054d968169eec76f7161b9d98121b&#34;&gt;CTE support&lt;/h2&gt;

&lt;p&gt;CTE is well supported among database systems, and I was surprised to learn that even SqLite supports it!&lt;/p&gt;

&lt;p&gt;Wikipedia says:&lt;/p&gt;

&lt;p&gt;Recursive CTEs are also supported by&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Microsoft SQL Server,&lt;/li&gt;
&lt;li&gt;Firebird 2.1,&lt;/li&gt;
&lt;li&gt;PostgreSQL 8.4+,&lt;/li&gt;
&lt;li&gt;SQLite 3.8.3+,&lt;/li&gt;
&lt;li&gt;Oracle 11g Release 2,&lt;/li&gt;
&lt;li&gt;IBM Informix version 11.50+ and&lt;/li&gt;
&lt;li&gt;CUBRID.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CTE support has been lacking from MySQL for some time now, however, derived tables can be used successfully to achieve standard or non recursive CTE functionality.
The good news is that &lt;a href=&#34;http://mysqlserverteam.com/mysql-8-0-labs-recursive-common-table-expressions-in-mysql-ctes/&#34;&gt;full featured CTE support, including recursiveness&lt;/a&gt; is not far off for MySQL.&lt;/p&gt;

&lt;h2 id=&#34;but-can-we-not-just-join-better:0dc054d968169eec76f7161b9d98121b&#34;&gt;But, can we not just join better?&lt;/h2&gt;

&lt;p&gt;Now that I&amp;rsquo;ve shown how the use of CTEs can eliminate sub queries, lets briefly return to the problem of the cartesian join.
I really did not like the additional sub query, and it&amp;rsquo;s additional CTE counterpart, for reducing the result sets into something that could
cross join without problems (those A, B and C).
Is there not perhaps a way to join without throwing anything away?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider the &lt;a href=&#34;http://sqlmag.com/t-sql/t-sql-join-types&#34;&gt;possible join types&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;CROSS JOIN :  We already saw this does not work.&lt;/li&gt;
&lt;li&gt;INNER JOIN : This will also not work since per definition our three sets are disjoint / contain no similar items.&lt;/li&gt;
&lt;li&gt;LEFT, RIGHT JOIN : We have the same problem; our sets are disjoint.&lt;/li&gt;
&lt;li&gt;FULL JOIN : Bingo! Full joins attempt to join on the join condition, but thereafter behaves like a LEFT and RIGHT JOIN combined; nothing is lost, and unmatched rows are still returned.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Did you know about the full join? I will admit that although I knew there had to be something like it, I did not know / forgotten all about it.&lt;/p&gt;

&lt;h2 id=&#34;sub-query-approach-simplified-with-a-full-join:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query approach simplified with a FULL JOIN&lt;/h2&gt;
select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress 
full join
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses 
on
businessEntitiesWithOneAddress.BusinessEntityId = 
businessEntitiesWithTwoAddresses.BusinessEntityId  
full join
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses
on
businessEntitiesWithTwoAddresses.BusinessEntityId   = 
businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId;

&lt;p&gt;Yes, we do away with one level of sub queries, but it&amp;rsquo;s still not great.&lt;/p&gt;

&lt;h2 id=&#34;cte-approach-simplified-with-full-join:0dc054d968169eec76f7161b9d98121b&#34;&gt;CTE approach simplified with FULL JOIN&lt;/h2&gt;
with  businessEntitiesWithOneAddress as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
), businessEntitiesWithTwoAddresses as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
), businessEntitiesWithMoreThanTwoAddresses  as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
)
select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from businessEntitiesWithOneAddress
full join businessEntitiesWithTwoAddresses
on businessEntitiesWithOneAddress.BusinessEntityId = businessEntitiesWithTwoAddresses.BusinessEntityId
full join businessEntitiesWithMoreThanTwoAddresses
on businessEntitiesWithTwoAddresses.BusinessEntityId = businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId

&lt;h2 id=&#34;simplest-solution:0dc054d968169eec76f7161b9d98121b&#34;&gt;Simplest solution&lt;/h2&gt;

&lt;p&gt;Finally, if we relax the requirement (which was self imposed in the first place) to have the results in one row,
as well as not worrying about returning the count for the three or more scenario,
we can simplify everything very nice like this:&lt;/p&gt;
with  businessEntitiesAddressCount as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
)
select 
count(BusinessEntityId) as NumberOfBusinesses,
NumberOfAddresses 
from businessEntitiesAddressCount 
group by NumberOfAddresses;

&lt;p&gt;Best of all; we still get to use a wonderful CTE!&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts:0dc054d968169eec76f7161b9d98121b&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;The first time I realised that it is possible to nest queries arbitrarily it seemed like the sky would be the limit to what wonderfully complex and deep stuff one could do,
all be it with a heavy performance penalty if you were not careful.
As time went on and I wrote larger and larger queries, often times with many levels of nested sub queries, it became
apparent that the readability of the code drastically reduced.
In general purpose programming languages one could avoid deep nested structures quite successfully, but in SQL there seemed to be no way around it
save for making use of temporary tables, or breaking the work up into smaller parts.&lt;/p&gt;

&lt;p&gt;When I found out about CTEs I was very impressed - what an elegant solution!
Yes, a carefully thought out join can often times remove the need for a sub query or CTE, but in cases where sub query is required I now opt for the CTE approach instead.&lt;/p&gt;

&lt;p&gt;Some prominant SQL experts even &lt;a href=&#34;http://modern-sql.com/use-case/literate-sql&#34;&gt;equate the use of CTEs / the WITH clause to literate programming&lt;/a&gt;, and I concur as far as readability and logical flow is concerned.
There is much more to CTEs, especially when considering the recursive variaty, but that is a story for another time.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s been a bit of a ramble, but if you made it this far, you&amp;rsquo;ll do well to give CTEs a try,
and maybe even find it as useful as I do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient git command line &amp; reasonable workflo</title>
      <link>https://blog.goodfast.info/post/reasonable-git-workflow/</link>
      <pubDate>Tue, 19 Jan 2016 21:26:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/reasonable-git-workflow/</guid>
      <description>

&lt;p&gt;Owning a software development shop, or being the prolific master coder that you are, what would you say is your most valuable assit? Is it your carefully acquired intellectual capital in the form of your people / your awesome self? Is it your revolutionary, novel  ideas?  Perhaps it&amp;rsquo;s both of those, in some sense, but how about something more mundane, like the great code written in the last hour, or day or months?
People might leave, ideas might deliver less than what they originally promised, but your code lives forever - should you not be pedantic to take excellent care of it, to track it&amp;rsquo;s growth and progress through every feature added to your products?&lt;/p&gt;

&lt;p&gt;When thinking about preserving even the seemingly insignificant little pieces, the old story of how IBM managed to boost sales of their casch register machines during the American great depression years always springs to mind.
The sales line reportedly went something like this. Since times are so tough you cannot afford to loose one penny, so upgrade your casch register to prevent this.&lt;/p&gt;

&lt;p&gt;In our coder context, enter git: the defacto distributed version control system, developed by none other than the linux kernel overlord, Mr. Linus Torvalds himself.
In this article I&amp;rsquo;ll put forward a painless and efficient git workflow that has been battle tested by our team over two years.
There are excellent git resources online, but I feel it still worth wile to write this  in order to cement these ideas for myself, and hopefully help others that have possibly been burnt by the very versatile git system.&lt;/p&gt;

&lt;p&gt;Since git developed as a command line tool for linux kernel hackers, I believe that if you want the best out of it you need to use the command line version, which  is git + bash.
The Git Extensions UI tools seems promising and useful at times, but I believe that with a little bit of setup and thought, the command line can fasilitate a much more efficient workflow.
The secret to efficient git command line usage I believe lies in making use of bash aliases and other standard bash features.
Most of the commands and aliases presented here came from my buddy Jacques over at Subverting Complexity.
He started me on a nice set of aliases and I&amp;rsquo;ve been tweaking them ever since.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-most-out-of-this-article:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Getting the most out of this article&lt;/h2&gt;

&lt;p&gt;If you want to gain the most out of this article I recommend you install git bash and open up (or create) the file  ~/.bash_aliases which will become your secret weapon towards git guru status.
In your ~/.bash_aliases file you will define short commands for  lengthy git command sequences.
To ensure that your ~/.bash_aliases file is sourced on bash startup, put this in your ~/.bashrc file:&lt;/p&gt;
if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

&lt;h2 id=&#34;learn-how-to-jump-to-git-bash-quickly-you-will-need-to-do-this-often:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Learn how to jump to git bash quickly: you will need to do this often&lt;/h2&gt;

&lt;p&gt;After installing git for Windows you get shell integration, which means that you can right click a folder and open up a git bash session for that folder as the PWD (present working directory).
Besides some general bash and git setup that would live in your user home, everything about the repo (repository) is self contained - you can copy it somewhere else and git will function exactly as before.
This is wonderful when you are working on a large repo and need to take it with you, to work on it from somewhere where you won&amp;rsquo;t have good Internet connectivity.&lt;/p&gt;

&lt;p&gt;A nice trick to be able to open git with one keystroke, no matter what you are currently busy with, is to use the Windows quick launch list. Items that are pinned down in the quick  launch list can be activated by the Windows Key + Number Key corresponding to the position the icon appears in the list.
For example, on my system git is seventh in the list, so Windows Key + 7 will always open it immediately, and Shift + Windows Key + 7 would open a new instance instead of jumping to the currently running one.
This quick launch behaviour is very useful for other applications too of course.
I&amp;rsquo;ve found that the main part of my work day is spent only between 5 or 6 at most applications, and being able to jump between them instantly is really lovely.&lt;/p&gt;

&lt;h2 id=&#34;what-we-will-be-looking-at:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;What we will be looking at&amp;hellip;&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Always branch off into a feature branch named after the ticket / issue number, prefixed with the parent branch name - so you and everyone else know why the branch exists.&lt;/li&gt;
&lt;li&gt;Commit regularly, rebasing on master each time - so you always have everyone&amp;rsquo;s latest and greatest to work with.&lt;/li&gt;
&lt;li&gt;Since you rebase down, always merge up.&lt;/li&gt;
&lt;li&gt;Before travelling commit and push remotely - so the thug / hardware failure / bus crush / bath tub / lightning don&amp;rsquo;t cost you a days work in addition to everything else.&lt;/li&gt;
&lt;li&gt;Provide short meaningful commit messages that include the issue number - so others know at a glance how your commit change the codebase.&lt;/li&gt;
&lt;li&gt;Try always to have a short lived feature branch - so you can avoid wandering too far from the parent branch.&lt;/li&gt;
&lt;li&gt;Schedule a time to nuke  your spent  feature branches locally and remotely if applicable.&lt;/li&gt;
&lt;li&gt;Proactive mutual understanding always trumps resolving merge conflicts.&lt;/li&gt;
&lt;li&gt;No need to memorise and type  lengthy git commands - use the power of bash aliases and functions.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;always-branch-off-into-a-feature-branch-named-after-the-ticket-issue-number-prefixed-with-the-parent-branch-name-so-you-and-everyone-else-know-why-the-branch-exists:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Always branch off into a feature branch named after the ticket / issue number, prefixed with the parent branch name - so you and everyone else know why the branch exists.&lt;/h3&gt;

&lt;p&gt;Cryptic and creative nameing of feature branches, vairiables or anything for that matter have never worked out for me.  Why bother if one can simply use the proper key, the feature or issue number generated by your issue tracking system?
A proper   issue tracking system like the excellent Atlassian Jira names issues with the project short code, followed by a dash and a number: i.e. &amp;ldquo;BANK-123&amp;rdquo;.
When you suspect that confusion might arise over which branch your feature branch stemmed off from you can include that name as well: &amp;ldquo;master-BANK-123&amp;rdquo;.
If you can see this feature branch being worked on by others also, and you want to make everyone know you are the original owner / creator you can prepend your initials: &amp;ldquo;kr/BANK-123&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Here is your first three commands and aliases to make working with short lived feature branches a breeze:&lt;/p&gt;
alias +=&#39;git checkout -b&#39;
alias D=&#39;git branch -D&#39;
alias C=&#39;git checkout&#39;

&lt;p&gt;With those you can quickly:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;create a new branch off the current: # + BANK-123&lt;/li&gt;
&lt;li&gt;change branches: $ C master&lt;/li&gt;
&lt;li&gt;nuke spent branches: $ D BANK-123&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;commit-regularly-rebasing-on-master-each-time-so-you-always-have-everyone-s-latest-and-greatest-to-work-with:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Commit regularly, rebasing on master each time - so you always have everyone&amp;rsquo;s latest and greatest to work with.&lt;/h3&gt;

&lt;p&gt;I had to learn the hard way, getting into the zone, working for an hour (or a shocking two hours) between commits, only to realise that in some way I&amp;rsquo;ve broken everything during the last 5 minutes of that two hours.
&amp;ldquo;Is it nice to bang your head against the wall?&amp;rdquo; a colleague remarked.
Now everything up to 5 minutes earlier was great work, yet it seems impossible to undo the tragedy that has been the last 5 minutes.&lt;/p&gt;

&lt;p&gt;Had I commited even half an hour ago I could have spared myself tremendous frustration.
We&amp;rsquo;ve all experienced this. It use to be a big deal in computer gaming, but I believe that these days the games saves for you.&lt;/p&gt;

&lt;p&gt;Here is your commands and aliases  so you have no more excuses - they make committing regularly totally painless:&lt;/p&gt;
# git add &amp; commit
	# git add
	alias ga=&#39;git add -A&#39;

	#git commit (usage: gc &#34;&lt;message&gt;&#34;)
	alias gc=&#39;git commit -m&#39;
	
	#git commit (usage: gac &#34;&lt;message&gt;&#34;
	alias gac=&#39;git add -A &amp;&amp; git commit&#39;

&lt;p&gt;Of these I tend to only use gac, and should probably change it to c only, but hey who&amp;rsquo;s perfect now?&lt;/p&gt;

&lt;p&gt;Here are some aliases for rebasing on the remote branch, or local parent branch:&lt;/p&gt;
	# rebase this branch of its tracked remote branch
	alias gpr=&#39;git pull --rebase&#39;

&lt;p&gt;However, if your local feature branch  (call it BANK-123) came from a local branch (call it master) which in turn is tracking the remote branch origin/master, then the following command will:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;pull the remotely tracked master right from within your feature branch&lt;/li&gt;
&lt;li&gt;rebase your local feature branch on the remote master&lt;/li&gt;
&lt;/ol&gt;
	# rebase this branch off its original parent tracked remote branch named master
	alias gprm=&#39;git pull --rebase origin master&#39;

&lt;p&gt;You can achieve the same effect with more steps:&lt;/p&gt;
	$ C master &amp;&amp; gpr &amp;&amp; C BANK-123 &amp;&amp; gpr master

&lt;p&gt;Notice the &amp;amp;&amp;amp; logical combinators. It&amp;rsquo;s a bash feature that will shortcut as expected - i.e. if you have unstaged changes in your current branch, the &amp;lsquo;C master&amp;rsquo; will fail and hence the whole command line.
This combining commands on the fly is very useful if you want to fire and forget and only come back later to check if everything went according to plan.
Notice that the above will also bring your local master up to date, while the &amp;lsquo;gprm&amp;rsquo; will not.&lt;/p&gt;

&lt;h3 id=&#34;since-you-rebase-down-always-merge-up:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Since you rebase down, always merge up.&lt;/h3&gt;

&lt;p&gt;When you are sure you want to add your changes to your parent branch, we call it master for argument&amp;rsquo;s sake, you need to merge.
Note that this is after you have successfully rebased on master, ran all relevant testing, pushed to your remote tracked feature branch (if applicable).
You need one more alias:&lt;/p&gt;
	alias gm=&#39;git merge&#39;

&lt;p&gt;You will now want to change to master, get the latest, merge in your feature branch and push to master.
You can do this with the following:&lt;/p&gt;
  $ C master &amp;&amp; gpr &amp;&amp; gm BANK-123 &amp;&amp; gh

&lt;p&gt;where BANK-123 is the name of your feature branch.
Of course you can make an alias out of this too, but personally I like to retain a little bit of control / do some things explicitly :-) else it gets a little bit scary ($ rm -R / # anyone perhaps?).&lt;/p&gt;

&lt;h3 id=&#34;before-travelling-commit-and-push-remotely-so-the-thug-hardware-failure-bus-crush-bath-tub-lightning-don-t-cost-you-a-days-work-in-addition-to-everything-else:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Before travelling commit and push remotely - so the thug / hardware failure / bus crush / bath tub / lightning don&amp;rsquo;t cost you a days work in addition to everything else.&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s a good idea to always have code that can go directly into production without breaking anything.
If you apply TDD properly, and hide more involved not yet completed features behind toggles this is very possible, however there might be times when you are not confident your code is ready for production yet you need to commit and push remotely.&lt;/p&gt;

&lt;p&gt;Here is an alias for setting a remote branch upstream to track your local branch and push there in one go:&lt;/p&gt;
	alias ghr=&#39;git push --set-upstream origin &#39;

&lt;p&gt;After issuing this command you can use the &amp;lsquo;gh&amp;rsquo; alias for subsequent pushes.&lt;/p&gt;

&lt;h3 id=&#34;provide-short-meaningful-commit-messages-that-include-the-issue-number-so-others-know-at-a-glance-how-your-commit-change-the-codebase:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Provide short meaningful commit messages that include the issue number - so others know at a glance how your commit change the codebase.&lt;/h3&gt;

&lt;p&gt;A format that works well is to write your commit message as if you are completing the sentance: &amp;ldquo;When this commit is applied it will &amp;hellip;&amp;rdquo;.
Example: &amp;ldquo;When this commit is applied it will proove P = NP and solve everything at once&amp;rdquo;.
Example: &amp;ldquo;When this commit is applied it will add feature X which was not in the spec, yet the client wants it&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;In addition, it is helpful to prepend the issue number to the message.
If your branch name is the same as your issue number you can use the following command to do this automatically for you:&lt;/p&gt;
	alias gab=&#39;git add -A &amp;&amp; git commit -m &#34;`git rev-parse --abbrev-ref HEAD` - &#39;

&lt;p&gt;So if you are on branch BANK-123 you can issue:&lt;/p&gt;
	$ gab proove P = NP and solve everything&#34;

&lt;p&gt;and the commit message would read: &amp;ldquo;BANK-123 - proove P = NP and solve everything&amp;rdquo;&lt;/p&gt;

&lt;h3 id=&#34;try-always-to-have-a-short-lived-feature-branch-so-you-can-avoid-wandering-too-far-from-the-parent-branch:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Try always to have a short lived feature branch - so you can avoid wandering too far from the parent branch.&lt;/h3&gt;

&lt;p&gt;As stated earlier, this is very possible when you apply reasonable TDD and hide larger incomplete functionality behind configurable toggles.
This is just a piece of wisdom. I&amp;rsquo;ll give you one more, for free: &lt;a href=&#34;http://en.wikipedia.org/wiki/Release_early,_release_often&#34;&gt;release early, release often (RERO)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;proactive-mutual-understanding-always-trumps-resolving-merge-conflicts:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;Proactive mutual understanding always trumps resolving merge conflicts.&lt;/h3&gt;

&lt;p&gt;I find manually merging files very difficult, and I&amp;rsquo;m sure no one really enjoys it. I&amp;rsquo;ve also found that a little bit of mutual understanding in terms of who is working on what part of the system goes a long, long way towards avoiding merge conflicts.
That, together with the other good principals presented here, such as rebasing often, and short lived feature branches also can do a lot to prevent merge conflicts.&lt;/p&gt;

&lt;h2 id=&#34;no-need-to-memorise-and-type-lengthy-git-commands-use-the-power-of-bash-aliases-and-functions:43ce4f2c703a95a3e073d309189c3a90&#34;&gt;No need to memorise and type  lengthy git commands - use the power of bash aliases and functions.&lt;/h2&gt;

&lt;p&gt;If the only thing you take with you after reading this article is that you can use bash features to make lengthy and hard to remember commands simpler then you did well and got the main idea.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>now make it fast</title>
      <link>https://blog.goodfast.info/post/now-make-it-fast/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://blog.goodfast.info/post/now-make-it-fast/</guid>
      <description>

&lt;p&gt;&amp;ldquo;He began to copy one of our tables, which contained partial user information, including email IDs, hashed passwords, and last tested URL. His copy operation
    locked the database table, which raised alerts on our monitoring system. On receiving the alerts, we checked the logs, saw an unrecognized IP, and blocked
    it right away. In that time, the hacker had been able to retrieve only a
    portion of the data.&amp;rdquo;
    &amp;ndash; From the postmortem of the
    &lt;a href=&#34;http://www.browserstack.com/attack-and-downtime-on-9-November&#34;&gt;Browser Stack hack of 9th November, 2014 at 23:30 GMT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since relational database management systems (RDBMS) have been
&lt;a href=&#34;http://en.wikipedia.org/wiki/Micro_DBMS&#34;&gt;used in production environments since 1970 (Micro DBMS)&lt;/a&gt;, and the theory on which they run was
developed in the preceding decade,and perfected in the three remaining decades
of the previous century (long  ago), it was not surprising  that, as  a subject,
it received very little attention
in our curriculum - at least where I studied.&lt;/p&gt;

&lt;p&gt;To further degrade the already &lt;em&gt;apparently&lt;/em&gt; low relevance  of the poor subject, we
had to cope with a very thick and overly theoretical textbook we could not yet appreciate,
and subscribe to, if you wanted to be cool and smart, to the snooty perception
held by many peers that the lowly database was useful, but entirely boring.&lt;/p&gt;

&lt;p&gt;A database was considered the type of thing  you rigged up for a family member in Access in a
few hours. Any larger system was the domain of &amp;lsquo;informatics&amp;rsquo; - the
less inspired, more practical brother field of study to the sexy &amp;lsquo;computer science&amp;rsquo; with its focus on AI, advanced programming in C++, the  new Java and the emerging world of Linux and Open Source.&lt;/p&gt;

&lt;p&gt;Stepping outside into the real world, and the rest was history - databases
everywhere, for everything!
In my working career since 2001, every single business system I worked on had
a RDBMS - primarily MS SQL Server, and sometimes MySQL or Oracle as the persistence store.
Through the years I accumulated skill in modelling domains and
manipulating the information through SQL, but optimisation of the storage
structures for efficiency was a task I chose to ignore. I reasoned along the
lines of it is the DBA&amp;rsquo;s / RDBMS&amp;rsquo;s work, or
the mostly false assumption that computers are fast and how much data will
the system  realistically have anyways?&lt;/p&gt;

&lt;p&gt;Really? Is that professional, to say this far will I go and no further,
while you as the programmer is directly responsible for those
horribly slow queries?
Yes, the system works, some things are slow, but hey, they have a lot of data
you say, it&amp;rsquo;s going to be slow at times!
Now you are suppressing   that little voice inside of you, quietly telling you it&amp;rsquo;s wrong
aren&amp;rsquo;t you - it &lt;em&gt;can&lt;/em&gt; be faster&amp;hellip;
Think of the waiting user, the wasted time, times hundreds for internal
systems, times thousands for customers, times  hundreds of thousands or even
millions in the case of the web&amp;hellip;
All that wasted time, all that wasted energy, all the trees&amp;hellip;
Don&amp;rsquo;t think about it too much.
You feel pretty bad by now don&amp;rsquo;t you&amp;hellip;&lt;/p&gt;

&lt;p&gt;Well, I have more bad news for you. At the end of this article you are going to feel even worse, because you
are going to see how simple it is to start to turn things around.&lt;/p&gt;

&lt;p&gt;Dramatics aside, we will have a quick look at the  choices  for table
physical layout and ponder the implications on performance.&lt;/p&gt;

&lt;p&gt;Be warned that we will of course only scratch the surface of SQL performance  optimisation
on SQL Server, and that this article is aimed at software developers
with limited skill in this area, so I&amp;rsquo;m going to explain a bit
- i.e. sit down, this might
take a while.&lt;/p&gt;

&lt;p&gt;Many folks have written fine articles on this subject, and I&amp;rsquo;ll refer to
some of those as we go along.&lt;/p&gt;

&lt;p&gt;My primary justification for writing this article is that I&amp;rsquo;d like to cement
this stuff in my mind, and the best way for me is to write it all out. I hope
you can gain something from this also, and please comment if you disagree or
whatever.&lt;/p&gt;

&lt;h2 id=&#34;it-s-out-there-somewhere-but-has-it-order:810306c57b772cfaef22459bbb55c3d7&#34;&gt;It&amp;rsquo;s out there&amp;hellip; Somewhere&amp;hellip; But has it order?&lt;/h2&gt;

&lt;p&gt;Contrary to popular current belief, the data isn&amp;rsquo;t somewhere in a [cloud]
but it actually resides on one or more  physical storage media (think disk
drives), and
importantly, it&amp;rsquo;s laid down either sorted or just as it was received - for all
practical purposes, unsorted.&lt;/p&gt;

&lt;p&gt;You probably knew this already, but I needed that sentence
because it contained the word &lt;em&gt;cloud&lt;/em&gt; so that I had an excuse to quote
Stallman on cloud computing from 2008:&lt;/p&gt;
&#34;It&#39;s stupidity. It&#39;s worse than stupidity: it&#39;s a marketing hype campaign,&#34;

&lt;p&gt;Back to tables&amp;hellip; It is probably because SQL Server&amp;rsquo;s default is to choose the
physically sorted way of laying down the data, when you define a primary key on
a new table, that this table organisation
prevails and is common in most systems.
There is nothing like one size fits all and although the
ordered layout of records is a great fit most of the time, it is not the best
layout in all cases (more on that later).
However, keep in mind there is wisdom in the choice of this default none
the less.&lt;/p&gt;

&lt;h2 id=&#34;some-terminology:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Some terminology&lt;/h2&gt;

&lt;p&gt;For the purpose  of the discussion we&amp;rsquo;ll stick to the most widely used
terminology in the SQL Server world and call the ordered layout of records a
&lt;em&gt;clustered index&lt;/em&gt;, and the
layout without any physical ordering a &lt;em&gt;heap table&lt;/em&gt;, or simply a heap.
The structure  existing purely to speed up locating records  we will refer to
as a &lt;em&gt;nonclustered index&lt;/em&gt;, and how the &lt;em&gt;clustered index&lt;/em&gt;
should be ordered we will call the &lt;em&gt;clustering key&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;clustered-index-the-sorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustered index - the sorted one&lt;/h3&gt;

&lt;p&gt;The term clustered index is unfortunate, and probably the reason a
lot of people develop a blurred notion of cluster vs. non clustered, and table
vs. index.
You could think of a &lt;em&gt;clustered index&lt;/em&gt; as a
database table for maintaining data in an ordered fashion (ordered by one or more
columns) thereby giving efficient access to all the data for one record if the
values of some of the sorting columns are known.  Alternatively you could think of a &lt;em&gt;clustered index&lt;/em&gt; as an
database index to
efficiently gain access to more data,
organised by a subset of  all the columns, yet it has all that added
data in itself.
Either way, Viewed as an ordered tabular representation of data, or an index existing for
efficient access to itself - the important point is the physical sorted
layout, and that the most efficient way to retrieve records from it is via the
index that is itself given the primary key.&lt;/p&gt;

&lt;h3 id=&#34;heap-the-unsorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Heap - the unsorted one&lt;/h3&gt;

&lt;p&gt;A heap is simpler to understand.  Think of a file that grows by repeatedly appending new
lines to it without trying to maintain any kind of order.
Obviously, for efficient access to the data in a heap we need one or more
nonclustered indexes targeting the heap&amp;rsquo;s data, and built up from one or more components
of that data.&lt;/p&gt;

&lt;p&gt;When comparing clustered indexes with heaps we will assume that at least for
the primary key, a nonclustered index is defined on the heap that would make the clustered
index and heap nearly equally  efficient when retrieving a record, given a single primary key value.&lt;/p&gt;

&lt;h3 id=&#34;nonclustered-index-the-real-index:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Nonclustered index - the real index&lt;/h3&gt;

&lt;p&gt;The term &lt;em&gt;nonclustered index&lt;/em&gt; we will use to refer to the structure
that exists primarily to provide   indexed lookup to clustered indexes and heaps
alike.  In SQL Server a heap or clustered index can have many nonclustered
indexes targeting it.&lt;/p&gt;

&lt;h3 id=&#34;clustering-key-how-the-clustered-index-is-clustered-or-sorted:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustering key - how the clustered index is clustered, or sorted&lt;/h3&gt;

&lt;p&gt;Apart from the notion of a primary key, for a clustered index, the &lt;em&gt;clustering
key&lt;/em&gt; defines the subset of columns that instruct the system on how to cluster
or sort the records.
A clustering key should be chosen to be as unique  as possible, but uniqueness
is not required (unlike for the primary key),since the system will add four bytes  called the uniquifier to
the clustering key if it is not indicated as being unique already.&lt;/p&gt;

&lt;h2 id=&#34;implications-of-heap-vs-clustered-index-for-dml:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Implications of heap vs. clustered index for DML&lt;/h2&gt;

&lt;p&gt;To compare the efficiency implications of DML statements on these two broad ways of physical data layout we have to look closer
at the nature of the operations we would like to perform.&lt;/p&gt;

&lt;p&gt;It is fairly intuitive  to reason about  this if we keep in mind:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The physical layout of a heap vs. a
clustered index.&lt;/li&gt;
&lt;li&gt;Indexes should stay up to date after the operation completes.&lt;/li&gt;
&lt;li&gt;Is the operation on one or on multiple records.&lt;/li&gt;
&lt;li&gt;What piece of data is required for the fastest retrieval of records.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;the-single-index-primary-key-as-clustering-key-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;The single index, primary key as clustering key scenario.&lt;/h3&gt;

&lt;p&gt;It turns out that it is generally the best choice to use a clustered index
instead of a heap, for the scenario where you need only one index on
one or more indexing columns. For this scenario, a clustered index takes up
less space, performs better overall, and releases space better when records
get deleted.&lt;/p&gt;

&lt;p&gt;The short answer to why this is the case is that for a clustered index, the
data is the index, so lookups on the clustering key finds the relevant records
directly (after climbing the index B-tree), and alterations affecting the clustering key requires alterations to
one structure - the clustered index. For a heap plus one nonclustered index
however, lookups given the index key is a two-step process. First the nonclustered
index is queried to find the RID, the uniquely identifying key for each heap
row corresponding to the clustering key, and
then the RID is used to fetch the record from the heap.
In addition to this, although alterations involving the index key might not require much work on the heap,
the nonclustered index needs to be updated - again a two-step process.&lt;/p&gt;

&lt;p&gt;Nevertheless, there is a great case to be made for choosing a heap over a
clustered index, which we&amp;rsquo;ll get to shortly, but for the scenario as explained
above (one index on the clustering key), a clustered index is the better choice.&lt;/p&gt;

&lt;p&gt;See this &lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc917672.aspx&#34;&gt;Microsoft best practices&lt;/a&gt; white paper, but be
warned that it is proving the superiority of choosing a clustered index in
favor of a heap in a scenario
where a clustered index is the  best choice.
Do not be fooled by this article into thinking heaps are overall inferior to
clustered indexes, as a casual reading might lead you into believing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at the various DML operations quickly on clusterd index and heap&amp;hellip;&lt;/p&gt;

&lt;h4 id=&#34;insert:810306c57b772cfaef22459bbb55c3d7&#34;&gt;INSERT&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;INSERT&lt;/em&gt; into a heap is simply a case of appending where there is space
available in a data page (or creating a new page at the end). After this, a
second write operation is required - insertion of the index key into the
B-tree  of the nonclustered index.&lt;/p&gt;

&lt;p&gt;Now for a clustered index, the new record
must be inserted in the correct location. If this correct location is on a
data page that is full, that page needs to be split in two .
This might be more time-consuming than the simple insert on the heap, but for
a clustered index there is no second write operation to maintain the index -
the data and index are one. Note that this might even turn out to make a
clustered index perform better overall since it only requires one write
operation.&lt;/p&gt;

&lt;p&gt;Any additional nonclustered indexes on a clustered index or heap would take
roughly the same time to update or keep in sync with the actual data, so we
can ignore their maintenance penalty when comparing.&lt;/p&gt;

&lt;p&gt;But surely we should be able to gain performance with inserts by choosing
either of the two table layouts,   given that heaps
and clustered indexes differ so fundamentally?
In this particular scenario they are  equally good because both
options still require that indexes be kept up to date. If we were to choose a
heap and define absolutely no nonclustered index on it we will gain the fastest
insert performance since inserting would simply be adding data on anywhere
where there is space.&lt;/p&gt;

&lt;p&gt;There is actually a great use case for this: logs and other archival type of
storage that do not require immediate querying.&lt;/p&gt;

&lt;p&gt;But optimisation is a tricky problem, since even for this use case, if many
concurrent inserts are expected, it might   very well be better to choose a
clustered index instead, on some non unique but range-like clustering key that
will jump around a bit, to
achieve an effect of inserting in different locations to prevent everything
from trying to insert in the same data page all the time (as would be the
case for a heap).&lt;/p&gt;

&lt;h4 id=&#34;select-update-and-delete:810306c57b772cfaef22459bbb55c3d7&#34;&gt;SELECT, UPDATE and DELETE&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;SELECT&lt;/em&gt;, &lt;em&gt;UPDATE&lt;/em&gt; and &lt;em&gt;DELETE&lt;/em&gt; first requires locating where the records to
operate on need to be physically
found, followed by the actual action on the data.&lt;/p&gt;

&lt;p&gt;Assuming that the operation simply applies to data given a single primary key
value that is the clustering key, the finding
or first part of this operation
is slightly  less efficient for a heap compared to  a clustered index. For
a clustered index, after the tree is climbed the information is there and
ready to be retrieved, while for a heap, after climbing the tree of the
nonclustered index, you only get the RID, and then require a second
operation to (all be it directly) get at the data in the heap - one additional
level of indirection.&lt;/p&gt;

&lt;p&gt;There is an option to &lt;em&gt;include&lt;/em&gt; columns of the heap or clustered index in the
nonclustered indexes. The effect of this is that, after climbing the tree of
the nonclustered index, those &lt;em&gt;included&lt;/em&gt; column&amp;rsquo;s data is immediately
available - a mini clustered index in the form of a nonclustered index with
included columns. All very straight forward and unambiguous  wouldn&amp;rsquo;t you say?&lt;/p&gt;

&lt;p&gt;Except for &lt;em&gt;SELECT&lt;/em&gt;, the efficiency of the second part, the action part of the operation
can vary much more between heap and clustered index.
For &lt;em&gt;UPDATE&lt;/em&gt;, if the column being updated happens to be one or more of the
columns comprising the clustering key, and the table is a clustered index, then
in order to keep the data  sorted, the system might have to
do page splits.
This in turn mean that potentially, large amounts of data need to be copied around.
For a heap this is never the case, and the columns  can simply be updated in
place - order is of no importance.&lt;/p&gt;

&lt;p&gt;For both heap and clustered index, if any of the columns are part of any
defined nonclustered indexes then altering them might have nonclustered index
maintenance time as a further performance penalty.&lt;/p&gt;

&lt;p&gt;Fortunately, for both heap and clustered index, if the columns being updated are not part
of the clustering key the efficiency of the action part of the operation is nearly similar.&lt;/p&gt;

&lt;p&gt;Performing a &lt;em&gt;DELETE&lt;/em&gt; on a heap or clustered index should be simply a case of marking that record
as deleted and making the space available for potential future inserts. For a
clustered index, no index maintenance is yet again needed while for the heap,
the nonclustered index needs updating.&lt;/p&gt;

&lt;h2 id=&#34;welcome-to-the-real-world-where-tree-climbing-is-to-be-avoided-the-multiple-indexes-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Welcome to the real world, where tree climbing is to be avoided - the multiple indexes scenario&lt;/h2&gt;

&lt;p&gt;The single index, primary key as clustering key and lookup scenario described earlier
might appear  early on, and a lot in most models, but very soon you
will also want to efficiently query on other columns on wider  (more columns)
and deeper (more rows) tables.&lt;/p&gt;

&lt;p&gt;To prevent full table scans, you start adding nonclustered indexes,
and this is where heaps start to become the more attractive alternative.&lt;/p&gt;

&lt;p&gt;Suppose for a moment that your  table (let&amp;rsquo;s call it table &lt;em&gt;T&lt;/em&gt;) that became wide and deep overnight is a
clustered index, and the primary key (&lt;em&gt;K&lt;/em&gt;) is also the clustering key.&lt;/p&gt;

&lt;p&gt;Each additional nonclustered index (&lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip;) on &lt;em&gt;T&lt;/em&gt; will store in its leaf nodes the
values of &lt;em&gt;K&lt;/em&gt;.
This means that a query on &lt;em&gt;T&lt;/em&gt; utilising  some nonclustered index &lt;em&gt;N&lt;/em&gt;  results
in a tree climb of &lt;em&gt;N&lt;/em&gt; that yields some value of &lt;em&gt;K&lt;/em&gt;. Following this we
require another tree climb of the clustered index that
is &lt;em&gt;T&lt;/em&gt;, given a value for &lt;em&gt;K&lt;/em&gt;, and only then is the actual data reached.&lt;/p&gt;

&lt;p&gt;On the other hand, suppose  now that your wide and deep table  &lt;em&gt;H&lt;/em&gt; is a heap
instead, with one or more nonclustered indexes &lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip; and so on.
This time, each nonclustered index &lt;em&gt;N&lt;/em&gt; will store at the leaf node the RID
of the relevant row, and not simply  yet another key into a further index.
This means that if some query on &lt;em&gt;H&lt;/em&gt; utilise one of the nonclustered indexes,
only one tree climb of that nonclustered index is required, after which the
RID is obtained, and unlike a clustering key, a RID represents the
physical position of the record in the heap, and thus can be directly accessed
- no further tree climbing required.&lt;/p&gt;

&lt;p&gt;For a more in-depth look at this, and some hard numbers comprising a
compelling case,  do yourself a favor and read Markus
Wienand&amp;rsquo;s fine article,
&lt;a href=&#34;http://use-the-index-luke.com/blog/2014-01/unreasonable-defaults-primary-key-clustering-key&#34;&gt;Unreasonable Defaults: Primary Key as Clustering
Key&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary-and-closing-thoughts-optimising-performance-is-an-interesting-and-very-relevant-problem:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Summary and closing thoughts - Optimising performance is an  interesting and very relevant problem&lt;/h2&gt;
&#34;Premature optimisation is the root of all evil&#34;
 -- Donald E. Knuth

&lt;p&gt;As much as I concur with that statement, especially how it applies to code, I do think that a good understanding
of the options available to you when turning a data model into an actual
database schema can proactively prevent vicious      cycles of poor performing
monster database servers.
Yes, there is a lot of things one can do, and the precise case where one
technique or option would be the better option is hard to identify, but the
better your understanding of the internals, the more likely you are to get it
right first time,
and the more it will start to happen that you are writing a query and you
suddenly realise that a specific index would benefit that query tremendously.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve rambled a bit, but to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are two main choices for the physical layout of data for tables.&lt;/li&gt;
&lt;li&gt;If only one index on the primary key is required it&amp;rsquo;s probably the best to choose a clustered index.&lt;/li&gt;
&lt;li&gt;For a many index scenario choose a heap.&lt;/li&gt;
&lt;li&gt;For best insert performance on high  loads choose a heap with no nonclustered indexes.&lt;/li&gt;
&lt;li&gt;Use the include columns feature of nonclustered indexes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Today there are exciting new alternative data storage technologies like fully
in memory databases, distributed systems such  as Hadoop, Google&amp;rsquo;s BigTable
approach, and document oriented noSQL
options such as ElasticSearch to name only a very few.
These alternative solutions to the problem of working with large data sets
have and will continue to be applied more and more, but if the last decade is
anything to go by,  the relational database is going to stick around for quite
some time still, so investing time into learning how to optimise it is
time well spent.&lt;/p&gt;

&lt;p&gt;The reason for SQL systems remaining central to all serious data storage
applications  is not by accident. There is a theoretical reason, routed in the
so-called CAP theorem.
For an overview of how the CAP theorem restricted the growth and adoption of
noSQL systems, have a look at
&lt;a href=&#34;http://use-the-index-luke.com/blog/2013-04/whats-left-of-nosql&#34;&gt;Whats left of NoSQL?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An interesting response to this is FoundationDB (see
&lt;a href=&#34;http://www.theregister.co.uk/Print/2012/11/22/foundationdb_fear_of_cap_theorem/&#34;&gt;NoSQL&amp;rsquo;s CAP theorem busters: We don&amp;rsquo;t drop ACID&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;An exciting emerging trend is to harness the strengths of both the traditional
RDBMS and the more recent big data distributed, more normalised data storage
technologies. For an interesting application of this, see
&lt;a href=&#34;http://msdn.microsoft.com/en-gb/magazine/dn802606.aspx&#34;&gt;Use Updatable Tables for Responsive Real-Time Reporting&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I hope this short info burst tickled your interest enough that you will go
ahead and look into all of it a bit more.
Personally I have been pleasantly surprised by the depth of this subject area.&lt;/p&gt;

&lt;p&gt;I can highly recommend the book
&lt;a href=&#34;http://sql-performance-explained.com/?utm_source=UTIL&amp;amp;utm_medium=main&amp;amp;utm_campaign=second&#34;&gt;SQL Performance Optimisation&lt;/a&gt; for an in-depth look at this
subject, and the
&lt;a href=&#34;http://use-the-index-luke.com/&#34;&gt;Use The Index Luke&lt;/a&gt; site.&lt;/p&gt;

&lt;p&gt;This article also appears on &lt;a href=&#34;http://www.inivit.com/blog/&#34;&gt;Inivit&amp;rsquo;s blog&lt;/a&gt; along with some other fine posts from former colleagues.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>