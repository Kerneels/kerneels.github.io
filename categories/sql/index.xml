<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sql on Good Fast</title>
    <link>https://blog.goodfast.info/categories/sql/</link>
    <description>Recent content in Sql on Good Fast</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jul 2019 21:45:07 +0200</lastBuildDate>
    <atom:link href="https://blog.goodfast.info/categories/sql/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Two big ideas of SQL: The What, not How, and querying with ease</title>
      <link>https://blog.goodfast.info/post/what-not-how/</link>
      <pubDate>Wed, 24 Jul 2019 21:45:07 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/what-not-how/</guid>
      <description>

&lt;p&gt;There are many factors that have contributed to the slow but steady rise of the popularity and ubiquity of SQL as a way to work with data.
Among all these factors, the most significant one in my opinion is the concept of &amp;ldquo;what&amp;rdquo; rather than &amp;ldquo;how&amp;rdquo;.
The second factor of greatness of SQL for me is the ability to quickly and easily answer questions about data.&lt;/p&gt;

&lt;p&gt;I choose to start out with these two &lt;em&gt;big ideas&lt;/em&gt; that to me, is fundamental to the language.&lt;/p&gt;

&lt;h2 id=&#34;the-what-rather-than-the-how:7167d0e820c7576947c6f743fb434a0c&#34;&gt;The &amp;ldquo;what&amp;rdquo; rather than the &amp;ldquo;how&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;The &amp;ldquo;what&amp;rdquo; rather than &amp;ldquo;how&amp;rdquo; idea seems simple, but it contains within it far reaching implications.
It is the idea of expressing &amp;ldquo;what&amp;rdquo; you want to achieve, rather than stipulating exactly &amp;ldquo;how&amp;rdquo; this should be achieved, leaving the &amp;ldquo;how&amp;rdquo; up to the system to decide.&lt;/p&gt;

&lt;p&gt;If you have had some experience solving problems by programming in a general purpose language such as C, C#, Java or Python, you should be familiar with the necessity to explain to the computer exactly how to do what you want it to do.
Failing to correctly specify every step the computer should take can lead to the inability to compile your program, or the wrong output being produced, or no output at all, or at best the correct output but too late.
This need for verbosity seems like a reasonable requirement, after all, the computer cannot guess what needs to happen, and you should find it natural that you have to tell it exactly what to do as well as how to do it.&lt;/p&gt;

&lt;p&gt;The need for total control and complete verbosity is perhaps most evident in  the domain of real time systems, where it is vital for the software not only to produce the correct result, but this result also needs to be produced at the exact correct time.
In these real time systems, it can be that the right result at the wrong time would be as disastrous as the wrong result or total failure.
A common programming language for building real time applications is C, because C allows the developer to specify the what and how very precisely, making it possible to calculate the exact time each step would take to execute.
So I got it into my head to try to do some things that are easy with SQL, in none other than C language. It turned out much harder than I thought, painful really, so read on for the gory details.&lt;/p&gt;

&lt;h2 id=&#34;the-ability-to-ask-easily:7167d0e820c7576947c6f743fb434a0c&#34;&gt;The ability to ask easily&lt;/h2&gt;

&lt;p&gt;The next great idea of SQL is the ability to quickly and easily craft questions, or queries on data.
I understand that SQL might look foreign and strange to someone who has never worked with it.
I do invite that group of people to consider how much stranger, harder and more foreign other ways of answering
this kind of questions can be if you have to use general purpose programming languages;
this post being  a case in point.&lt;/p&gt;

&lt;h3 id=&#34;data-example-from-the-gdelt-data-set:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Data :   Example from the GDELT data set&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;gdelt&#34;&gt;GDELT data set&lt;/a&gt; is a very high resolution global collection of events, gathered from numerous online news agencies and publications.
For &lt;a href=&#34;gdelt1oneday&#34;&gt;one days worth of data&lt;/a&gt; from the GDELT project, we would simply like to know:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How many data points, or events, is contained in the sample file.&lt;/li&gt;
&lt;li&gt;How many events had the actor 1 code of &amp;lsquo;AFR&amp;rsquo;, the code for Africa.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;a href=&#34;gdelt1header&#34;&gt;header columns&lt;/a&gt; is not part of the data file.
We place the data and header file in a &lt;code&gt;./gdelt&lt;/code&gt; directory after download.&lt;/p&gt;

&lt;h4 id=&#34;nix-solution:7167d0e820c7576947c6f743fb434a0c&#34;&gt;*Nix Solution&lt;/h4&gt;

&lt;p&gt;While already well away cooking  up a C language solution, I remembered AWK, and the wc program.&lt;/p&gt;
 $ wc -l gdelt/20190531.export.CSV
 176780

  $ gawk &#39;{ if ($6 == &#34;AFR&#34;) sum += 1; } END { print sum; }&#39; gdelt/20190531.export.CSV
 1184

&lt;p&gt;Here I used the &lt;code&gt;wc&lt;/code&gt; or word count program, with the &lt;code&gt;-l&lt;/code&gt; flag to count lines.
For the count of &amp;lsquo;AFR&amp;rsquo; valued actor codes I used the GNU version of &lt;code&gt;awk&lt;/code&gt;, together with a tiny awk program for a conditional incrementing of a counter.
We might want to stop here and conclude that we need go no further since we&amp;rsquo;ve gotten our answer,
but the truth is that we were quite fortunate in that there even exists a tool like &lt;code&gt;wc&lt;/code&gt; and that
conditionally counting the occurances of a particular value in a particular column is fairly straightforward with &lt;code&gt;awk&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Raising the bar ever so slightly by, for example wanting to know all the actor codes that appear in the file, along with the counts for each becomes tricky using awk.
Furthermore, we need to  verify our answer above,  by computing  the same in a few more ways.&lt;/p&gt;

&lt;h4 id=&#34;c-solution:7167d0e820c7576947c6f743fb434a0c&#34;&gt;C Solution&lt;/h4&gt;

&lt;p&gt;My C language solution is straightforward but quite a bit more verbose and detailed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read  the data file line by line.&lt;/li&gt;
&lt;li&gt;For each line, increase the lines counter.&lt;/li&gt;
&lt;li&gt;For each line, get the value of the column containing the actor 1 code.&lt;/li&gt;
&lt;li&gt;If the actor 1 code column contains what we are looking for, increase the second counter.&lt;/li&gt;
&lt;li&gt;After all lines were read, print the results to the console.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After some research, we can add some details on how to achieve our goal:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read  the data file line by line.

&lt;ul&gt;
&lt;li&gt;Open the file with the &lt;code&gt;fopen()&lt;/code&gt; function from the &lt;code&gt;stdio&lt;/code&gt; library.&lt;/li&gt;
&lt;li&gt;Read the file line by line with successive calls to &lt;code&gt;fgets()&lt;/code&gt; function, also from &lt;code&gt;sdio&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;For each line, increase the lines counter.&lt;/li&gt;
&lt;li&gt;For each line, get the value of the column containing the actor 1 code.

&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;strtok()&lt;/code&gt; function, from the &lt;code&gt;strlib&lt;/code&gt; library to tokenise on a given delimiter.&lt;/li&gt;
&lt;li&gt;The actor 1 code is in the fith column from the start of each line, where columns are seperated by the tab character.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If the actor 1 code column contains what we are looking for, increase the second counter.

&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;strcmp()&lt;/code&gt; function from &lt;code&gt;strlib&lt;/code&gt; to compare two strings for equality.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;After all lines were read, print the results to the console.

&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;printf()&lt;/code&gt; function with its very easy to use string interpolation system.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And here is the simplest first attempt C solution:&lt;/p&gt;
#include &lt;stdio.h&gt;  // fopen, fclose, fgets
#include &lt;string.h&gt;  // strtoc, strcmp

int main() 
{
   const char FILE_PATH[] = &#34;../gdelt/20190531.export.CSV&#34;;
   const char DELIMITER[] = &#34;\t&#34;;
   const char MATCH_TEXT[] = &#34;AFR&#34;;
   const int MATCH_COLUMN_POS = 5; 

   int match_count = 0;

   FILE *fp;
   const int BUFF_SIZE = 2048;
   char buff[BUFF_SIZE];

   fp = fopen(FILE_PATH, &#34;r&#34;); 
   if (fp == NULL) return -1; 
   int lines_count = 0;
   while (fgets(buff, BUFF_SIZE, (FILE*)fp)) {
       lines_count++;
       char *ptr = strtok(buff, DELIMITER);

       // find the value of the ACTOR_CODE
       for (int i=0; i&lt;MATCH_COLUMN_POS; i++)
           ptr = strtok(NULL, DELIMITER);

if (strcmp(ptr, MATCH_TEXT) == 0)
{
    match_count += 1;
}
   }
   printf(&#34;Total number of lines: %d\n&#34;, lines_count );
   printf(&#34;Number of lines matching &#39;%s&#39; in column %d: %d\n&#34;, MATCH_TEXT, 
           MATCH_COLUMN_POS,
           match_count );
   fclose(fp);
   return 0;
}

&lt;p&gt;After compiling  with gcc ./query.c -o ./query, (gcc .\query.c -o .\query.exe on Windows) and running it, the output is:&lt;/p&gt;
Total number of lines: 176780
Number of lines matching &#39;AFR&#39; in column 5: 1184

&lt;p&gt;Nice. This answer corresponds perfectly with the *nix solution,
but  what a lot of work for answering such a trivial question!
Clearly we cannot write programs each time we want to answer something like this.&lt;/p&gt;

&lt;h4 id=&#34;sql-solution:7167d0e820c7576947c6f743fb434a0c&#34;&gt;SQL Solution&lt;/h4&gt;

&lt;p&gt;As you might know, SQL is not a general purpose programming language, but rather a kind of mini language implemented in a larger database management system (or DBMS).
Because of this, you cannot directly work with flat files, but have to  first load the files into the system.
For my SQL solution I&amp;rsquo;ll use &lt;a href=&#34;sqlite&#34;&gt;SQLite&lt;/a&gt; as the SQL query engine; the DBMS.&lt;/p&gt;

&lt;p&gt;With the data and header file in the &lt;code&gt;./gdelt&lt;/code&gt; directory, we get them loaded like this:&lt;/p&gt;
&gt; sqlite3 gdelt.md
SQLite version 3.28.0 2019-04-16 19:49:53
Enter &#34;.help&#34; for usage hints.
sqlite&gt; .mode csv
sqlite&gt; .timer on
sqlite&gt; .separator &#34;\t&#34; &#34;\r\n&#34;
sqlite&gt; .import ../gdelt/CSV.header.dailyupdates.txt oneday
sqlite&gt; .import ../gdelt/20190531.export.CSV oneday

&lt;p&gt;Here is the breakdown:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I start by invoking the sqlite3 interactive CLI program with a name of the file where we want our database to live.&lt;/li&gt;
&lt;li&gt;All the commands starting with period (&amp;lsquo;.&amp;rsquo;) are system related commands, and not SQL as such.&lt;/li&gt;
&lt;li&gt;The data file has no header info, but we do have a file with header info, which we load first.&lt;/li&gt;
&lt;li&gt;Loading the header file first conveniently creates the table &lt;code&gt;oneday&lt;/code&gt;, which will hold the actual data. Later on we will show how to create tables from scrach.&lt;/li&gt;
&lt;li&gt;Loading the data file into the created table &lt;code&gt;oneday&lt;/code&gt; does not re-create the table, but appends to it.&lt;/li&gt;
&lt;li&gt;The  command &lt;code&gt;.timer on&lt;/code&gt; is not required but provides interesting timing info output.&lt;/li&gt;
&lt;li&gt;The data file is tab delimited, new line record seperated, and we let sqlite know about this with the command &lt;code&gt;.separator &amp;quot;\t&amp;quot; &amp;quot;\r\n&amp;quot;&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now we are ready to answer our question using SQL!&lt;/p&gt;
sqlite&gt; select count(1) from oneday;
176780
Run Time: real 0.163 user 0.031250 sys 0.125000
sqlite&gt; select count(1) from oneday where actor1code = &#39;AFR&#39;;
1056
Run Time: real 0.181 user 0.046875 sys 0.140625
sqlite&gt;

&lt;p&gt;Hmm&amp;hellip; We arrive at the same total row or lines count, but the count for the matching actor 1 code values is very different.
To figure out why this is the case is not trivial, but very instructive to illustrate even more the need for something like SQL and database systems, so stay with me.&lt;/p&gt;

&lt;h3 id=&#34;debugging:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Debugging&lt;/h3&gt;

&lt;p&gt;Why do the figure for the actor 1 code column  equal to &amp;lsquo;AFR&amp;rsquo; correlate for the *nix and C solutions,
yet differ for the SQL sqlite3 solution?
Since the majority rules, the SQL sqlite3 solution must be incorect. What is this SQLite3 anyways - it&amp;rsquo;s probably got a bug.
Yet,  SQLite is probably the most widely deployed database system, primarily due to it being used on mobile platforms such as Android.
Surely they would nnot use a broken thing!&lt;/p&gt;

&lt;h4 id=&#34;what-we-know:7167d0e820c7576947c6f743fb434a0c&#34;&gt;What we know&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;All solutions correlate on the row / line / record count.&lt;/li&gt;
&lt;li&gt;The *nix and C language solution correlates on actor 1 code count.&lt;/li&gt;
&lt;li&gt;The SQLite3 solution actor 1 code count is less than the other two solutions.&lt;/li&gt;
&lt;li&gt;None of the solutions produced any errors or warnings on data load or query.&lt;/li&gt;
&lt;li&gt;SQLite is unlikely to have a bug causing the issue, but we might have loaded data wrongly somehow.&lt;/li&gt;
&lt;li&gt;It is unlikely that twoout of our three solutions would produce the same wrong result.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;poking-around-in-the-data:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Poking around in the data&lt;/h3&gt;

&lt;p&gt;Since we have everything setup in SQLite already, let&amp;rsquo;s query the data a bit more, in hope we find some clues to help us solve the dilemma.&lt;/p&gt;

&lt;p&gt;How many values are there for column &lt;code&gt;actor1code&lt;/code&gt;?&lt;/p&gt;
sqlite&gt; select count(distinct actor1code) from oneday;
2148

&lt;p&gt;Is the count from before for the SQLite solution still the same?&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code like &#39;AFR&#39;;
1056

&lt;p&gt;Perhaps the comparison predicate in the &lt;code&gt;where&lt;/code&gt; clause &lt;code&gt;actor1code = &#39;AFR&#39;&lt;/code&gt; has a problem.
Let us try some &lt;code&gt;LIKE&lt;/code&gt; matching instead.&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code like &#39;%AFR&#39;;
1056
sqlite&gt; select count(1) from oneday where actor1code like &#39;%AFR%&#39;;
1241
sqlite&gt; select count(1) from oneday where actor1code like &#39;AFR%&#39;;
1139

&lt;p&gt;Nope, this is not helping much, andsomething else is going on.&lt;/p&gt;

&lt;h4 id=&#34;edge-cases:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Edge cases&lt;/h4&gt;

&lt;p&gt;It&amp;rsquo;s always a good idea to consider some edge cases in the data.
One such edge case is blank values, so let&amp;rsquo;s check for them in the actor 1 column:&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code = &#39;&#39;;
16475

&lt;p&gt;We have seen that the SQLite solution under counts, or, &lt;strong&gt;gasp&lt;/strong&gt; BOTH the AWK *nix solution AND the C language one over counts, and do it the same.
Since both the AWK and the C language solution produced the same result, let&amp;rsquo;s see if we can find the blanks count of above with AWK:&lt;/p&gt;
 $ gawk &#39;{ if ($6 == &#34;&#34;) sum += 1; } END { print sum; }&#39; ./gdelt/20190531.export.CSV


&lt;p&gt;Well, it produced no output, so gawk wasn&amp;rsquo;t able to count blanks&amp;hellip;.
I bet the C language solution will do exactly the same!&lt;/p&gt;

&lt;p&gt;Now the actor 1 code of &amp;lsquo;AFR&amp;rsquo; is quite special, but we know from the documentation of GDELT and also from the header columns, that there is also an actor 2 code column, and it follows right after all the actor 1 columns.
How many rows are there where the column actor1code is blank, and the actor2code is &amp;lsquo;AFR&amp;rsquo;:&lt;/p&gt;
sqlite&gt; select count(1) from oneday where actor1code = &#39;AFR&#39;;
1056

sqlite&gt; select count(1) from oneday where actor1code = &#39;&#39; and actor2code = &#39;AFR&#39;;
128

sqlite&gt; select count(1) from oneday where actor1code = &#39;&#39; and actor2code = &#39;AFR&#39; or actor1code = &#39;AFR&#39;;
1184

&lt;p&gt;Now that is interesting.  The C and AWK solutions count, in column 6 where actor 1 code should live, the sum of the columns where actor 1 code is &amp;lsquo;AFR&amp;rsquo;, plus the number of
rows where actor 1 code is blank AND actor 2 code is &amp;lsquo;AFR&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;Before I touch that C solution again, let&amp;rsquo;s do something easier, and search a bit online; my money is on the &lt;code&gt;strtok()&lt;/code&gt; function: I think it works different to what we assumed&amp;hellip;&lt;/p&gt;

&lt;p&gt;And yes! Indeed! A fellow Internet citizen &lt;a href=&#34;strtok_skipping&#34;&gt;came across this already&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We could now proceed and fix up the C solution, but by now I&amp;rsquo;ve kind of lost interest, and I&amp;rsquo;m truely thankful I do not have to code C for a living - it must be very, very hard.
None the less, perhaps for the love of Linux, let&amp;rsquo;s at least get AWK right.&lt;/p&gt;
$ gawk &#39;BEGIN {FS = &#34;\t&#34;} { if ($6 == &#34;AFR&#34;) sum += 1; } END { print sum; }&#39; gdelt/20190531.export.CSV
1056

&lt;p&gt;Yay for AWK, yay for the Unix tradition, yay for Linux!
We explicitly set the field seperator (FS), same as for the SQLite solution.&lt;/p&gt;

&lt;h2 id=&#34;ddl-and-dml:7167d0e820c7576947c6f743fb434a0c&#34;&gt;DDL and DML&lt;/h2&gt;

&lt;p&gt;You may have heard the acronym DDL and DML before.
The objectives of SQL can be categorised into two broad groups. The first group  of objectives is  related to the definition of artifacts to organise and access data.
This group is commonly referred to as the data definition language (or DDL) statements .
The second group of objectives relates to manipulation; the creation, the modification and the deletion of data.
This group is commonly refered to as the data manipulation language (or DML) statements.&lt;/p&gt;

&lt;p&gt;Outside the DDL and DML groups there usually exists additional statements concerned with particulars of the  actual SQL-based system, such as those intended for administration and maintenance.
For SQLite examples of these more admin-type keywords and functions are the ones starting with &amp;lsquo;.&amp;rsquo; as illustrated above when we loaded the CSV file.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:7167d0e820c7576947c6f743fb434a0c&#34;&gt;Conclusion&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Answering even the simplest of questions about data, using only imperative programming like C is tough and error prone.&lt;/li&gt;
&lt;li&gt;The Unix tradition has many, many jewels; AWK and wc two nice examples, and knowing about these can give us an edge.&lt;/li&gt;
&lt;li&gt;For answering anything but the simplest queries it is best to rope in a proper SQL database system, and this can be light weight such as SQLite.&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;What&amp;rdquo; rather than &amp;ldquo;How&amp;rdquo; idea of SQL is huge: don&amp;rsquo;t worry how to do stuff, just declare to the computer what you want, and let the system sort out how to give it to you.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s very powerful to be able to answer questions on data, simbply by writing short queries rather than whole programs.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Speed up slow views through custom materialization</title>
      <link>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</link>
      <pubDate>Thu, 08 Jun 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</guid>
      <description>

&lt;p&gt;SQL views are aluring as a means of abstraction; a &amp;ldquo;building block&amp;rdquo; to hide away commonly used complexity.
It is no wonder then that us developers will try them out, and before you know it, your clever recursive CTE view on that hierarchy is used everywhere, by everyone, but how is it affecting overall database performance&amp;hellip;&lt;/p&gt;

&lt;p&gt;They look like tables, can be joined on, selected from, and in some cases even updated just like tables, yet the reality is that they are not like tables.
So, you cannot consider a view to be a type of stored procedure, and you can also not consider a view to be a type of table or index; it is something in between.&lt;/p&gt;

&lt;p&gt;It is possible for the query planner to &amp;ldquo;reach into&amp;rdquo; a view, and discover which indexes to use in order to access information in the best way, but this quikcly breaks down once you perform any kind of complicated thing, such as a CTE, UNION statement, or anything else that breaks up the link from the source tables to the result set of the view.
When exactly you break this ability of the query planner to use appropriate indexes is a great idea for a future post - I have not found anything that directly states this as of yet.
Intuitivly  it makes sense that some kinds of data mangling will just make it impossible for the query planner to find indexes to use.&lt;/p&gt;
Note that it&#39;s of course always best to first inspect the query plan before concluding that a fiew is or is not making use of a particular index. I have made the mistake before of making grand statements on how poor the query planner is at choosing an index when dealing with a view, only to be shown that it in fact can do a bit more than what you might expect!

&lt;p&gt;My goal in this post is simply to make you aware of the possability that complex views might be causing your database to perform sub optimally, and then to offer an in place, zero downtime solution to the problem.&lt;/p&gt;

&lt;h3 id=&#34;when-to-use-views:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;When to use views&lt;/h3&gt;

&lt;p&gt;The way I currently understand it,  you should use views when you want different &lt;em&gt;views&lt;/em&gt; on the same table, or simple connected set of tables; i.e. you want to include/exclude certain columns/rows, so in other words, as a means of information hiding, a means of performing restricted access to the information in the underlying tables; a different take on the same data.
It is debatable how many new systems are developed, that would choose to deligate security, access restriction type of functionality to the database, but there is a fair chance that it is happening out in the wild, since a recent SQL Server feature is row-level access, and data masking.&lt;/p&gt;

&lt;h3 id=&#34;discovering-the-problem:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Discovering the problem&lt;/h3&gt;

&lt;p&gt;It was while I was performance tuning a very busy Azure Database, that I discovered a collection of particularly slow executing queries, spending most of their time in CPU.
The data volume involved could not account for the poor performance, being in the mere tens of thousands of small rows.
As far as I could determine, most of the appropriate indexes existed that would normally make things perform acceptably.
Something else was up&amp;hellip;&lt;/p&gt;

&lt;p&gt;Turning to the query plans, a pattern started emerging; slow, very slow views were joined on.
The views themselves were not very complex, but they did something interesting: they were recursive CTEs designed to traverse
a hierarchy, essentially a tree structure, and produce a full fan out of the entire tree.&lt;/p&gt;

&lt;h3 id=&#34;solution:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Solution&lt;/h3&gt;

&lt;p&gt;My first inclination was to have SQL Server materialize these views for me. Materialized (or indexed) views is an &lt;a href=&#34;http://sqlmag.com/database-performance-tuning/introducing-indexed-views&#34;&gt;old feature of the server&lt;/a&gt;, dating back to SQL Server 2000 if I&amp;rsquo;m not mistaking, so surely in 2017 this should be completely possible.
Well, it turns out that in order for a view to be materialized, &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/relational-databases/views/create-indexed-views&#34;&gt;a whole list of requirements&lt;/a&gt; need to be satisfied. For example, something as innocently looking as a LEFT JOIN in the view query would put a quick end to this solution path.&lt;br /&gt;
Researching it a bit further shed some light on why all these restrictions apply, but although it does make you be a bit more understanding, it still feels like this is something that should be possible, no matter how complex the view is.&lt;/p&gt;

&lt;p&gt;Completely redesigning the underlying hierarchical representation, with something like transative closures was not really an option, so the next best idea was to custom materialize these views.
The data access characteristics of the hierarchy and supporting tables was that they did not change all that often, yet they were queried all the time.
This was great news, since it meant that even if the materialization process took a bit of time, this would quickly be compensated for by the much, much faster query times.
Having the previously computed data now reside in a proper table also meant that it could be appropriately indexed, clustered, and even partitioned (although the volume was far too low for this need).&lt;/p&gt;

&lt;h4 id=&#34;the-procedure:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;The procedure&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Create a table (we&amp;rsquo;ll call it the Working) that structurally mirrors the result of querying the view (the View, later to be renamed to the Origin).&lt;/li&gt;
&lt;li&gt;Create a stored procedure (RefreshWorking) that will make use of Origin to refresh Working.&lt;/li&gt;
&lt;li&gt;Create AFTER triggers for all tables referenced by Origin, that will call RefreshWorking.&lt;/li&gt;
&lt;li&gt;Make the triggers intelligent in that they will only call RefreshWorking when the DML operation of the source table would actually affect the outcome of the Origin view.&lt;/li&gt;
&lt;li&gt;Optionally pass the source table name and the key values, through a table valued parameter to RefreshWorking, so that the procedure can more intelligently pick out which parts of Working will need refreshing.&lt;/li&gt;
&lt;li&gt;Create a view, CheckWorkingAndOrigin, that FULL JOIN view Origin and table Working, to ensure that they are identical.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once all of this is done, it is time to test.
For your testing I can highly recommend &lt;a href=&#34;http://tsqlt.org/&#34;&gt;tSQLt&lt;/a&gt;; a completely T-SQL based unit testing system.
When you have assured yourself that RefreshWorking properly updates table Working, it is time for the deployment.&lt;/p&gt;

&lt;p&gt;In one transaction, rename  the original, slow view to Origin, create a synonym with the same name as the original slow view, and point the synonym at the Working table.
As the last step of the transaction, run RefreshWorking procedure so that the Working table will get properly updated and be primed for showtime.&lt;/p&gt;

&lt;h3 id=&#34;reward:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Reward&lt;/h3&gt;

&lt;p&gt;After we implemented this procedure on a heavily queried complex view, we saw a query plan simplification going from
over 70 steps, to only 3 steps. More impressive is that the plan now took 481 times less CPU time!
The RefreshWorking procedure still called the original, slow, complex view, but it did this only when the source tables changed and in particular ways.
The procedure also minimized writes to the Working table, to prevent table locking for the heavy reading on it.&lt;/p&gt;

&lt;h3 id=&#34;summary:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Summary&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;One way we make sense of the world is by modelling relationships between things as a tree-like hierarchy.&lt;/li&gt;
&lt;li&gt;Most database systems, however, are essentially flat when it comes to the most basic collection of storage; the table.&lt;/li&gt;
&lt;li&gt;We overcome the flatness in one particular way, the simpler  of possible ways, by representing the unbounded nested characteristic of hierarchies by self referencing records in a table.&lt;/li&gt;
&lt;li&gt;Self referencing records in a single table makes manipulation of the hierarchy a very simple operation, but this simplicity comes at a cost when you want to traverse the hierarchy.&lt;/li&gt;
&lt;li&gt;For traversing arbitrarily  deep hierarchies represented by self referencing records, you inevitably require recursion.&lt;/li&gt;
&lt;li&gt;Recursive CTEs break the essential link between the source tables and the view result set, making it impossible for the query planner to do anything but perform the entire process of the view&amp;rsquo;s query, even when you only desire a small subset.&lt;/li&gt;
&lt;li&gt;In the scenario where the source tables for the complex view are written to less than they are read from, you can optimize the complex view by materializing it into a concrete table.&lt;/li&gt;
&lt;li&gt;The materialized table can then be properly indexed for maximum query performance, at a relatively small index maintenance cost at write time.&lt;/li&gt;
&lt;li&gt;With this approach you are trading computation time for storage space.&lt;/li&gt;
&lt;li&gt;The writing of the materialized table happens on DML AFTER triggers, so that you first have the change written to the source tables before the materialized table is updated.&lt;/li&gt;
&lt;li&gt;Updating of the materialized table need not be a complete rewrite; the AFTER triggers can be programmed so that they only fire when columns that partake in the SELECT list for the origin view query change.&lt;/li&gt;
&lt;li&gt;A further optimization can be made where by the refresh stored procedure recieves a list of keys of rows that changed, and can then use this info to only update the materialized table where it actually needs to change.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope that you will try out this procedure on slow views on your databases; it has really helped us a lot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CTE : simplify those nested sub queries</title>
      <link>https://blog.goodfast.info/post/cte-sub-queries-simplified/</link>
      <pubDate>Sun, 09 Oct 2016 21:26:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/cte-sub-queries-simplified/</guid>
      <description>

&lt;p&gt;This article examines how sub queries can be substituted for the far more readable common table expressions, or CTEs available in many RDBMS systems.
I was motivated to write this article when a friend who is fairly new to SQL expressed difficulty in grasping queries containing nested sub queries.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;ve never heard of CTEs before and you want to get the most out of this article, I recommend you get &lt;a href=&#34;https://msftdbprodsamples.codeplex.com/releases/view/125550&#34;&gt;AdventureWorks2014&lt;/a&gt; sample database and experiment a little with the queries below.
Adventureworks is a nice sample database designed to demonstrate SQL Server features, and many examples online makes use of it.
If on the other hand CTEs are old hat to you a quick skim reading of the below will suffice, but pay attention since there is something
interesting below which you might never have known, or maybe forgotten.&lt;/p&gt;

&lt;h2 id=&#34;is-it-just-me-or-is-this-strange:0dc054d968169eec76f7161b9d98121b&#34;&gt;Is it just me, or is this strange?&lt;/h2&gt;

&lt;p&gt;No-no, this thing came from long, long ago, was what I was thinking
the first time I was confronted with SQL SELECT syntax.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s all the wrong way round.
Take the most basic form of  a SELECT statement for example&amp;hellip;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You start out with what you want in the column list while starting the SELECT statement.&lt;/li&gt;
&lt;li&gt;Next you proceed to say from where you want the data  to come from, in the FROM clause.&lt;/li&gt;
&lt;li&gt;Then you stipulate  how  you want to filter what is returned when you write the WHERE clause.&lt;/li&gt;
&lt;li&gt;Finally  you end off with how stuff should be ordered in the ORDER BY clause.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, I do realise that there are (were) probably very good reasons why this particular order was chosen, but
would it not make more sense to rather start out with saying where you want to draw info from (FROM clause), proceed to filter that with the WHERE clause, followed by your preferred ordering in the ORDER BY, and finishing up with the list of which columns you are interested in, e.g:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;FROM&lt;/li&gt;
&lt;li&gt;WHERE&lt;/li&gt;
&lt;li&gt;ORDER BY&lt;/li&gt;
&lt;li&gt;SELECT&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Well, it is really of no consequence now - we have come too far along this road, however, note that the fluent way of constructing queries
with .NET C# Linq offer exactly this; a more logical flow of steps.&lt;/p&gt;

&lt;p&gt;This post is about simplifying subqueries, so getting back on track, it turns out there is a powerful feature of SQL99 called
Common Table Expressions, or CTEs which offers a great help when you are dealing with a monster query with little minions of subqueries, nested deeper in there than  your TV remote that sunk into the couch folds.&lt;/p&gt;

&lt;p&gt;Whereas sub query syntax quickly gets confusing and convoluted, using the CTE approach breaks down complexity and deals with each step of the workload in separate parts before moving on to
constructing a more intertwined picture.&lt;/p&gt;

&lt;p&gt;To me, it somewhat moves towards my &amp;ldquo;more ideal&amp;rdquo; ordering of statements, all be it no real alteration to the standard SELECT syntax.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at an example.&lt;/p&gt;

&lt;h2 id=&#34;adventureworks-customers-and-their-addresses:0dc054d968169eec76f7161b9d98121b&#34;&gt;Adventureworks customers and their addresses&lt;/h2&gt;

&lt;p&gt;In Adventureworks, customers are modeled in the person.BusinessEntity table, and their addresses in the person.Address table.
The relation between customers and their addresses is maintained in the person.BusinessEntityAddress table, which adds a type
for each address (Billing, Home, Main Office etc).&lt;/p&gt;

&lt;h2 id=&#34;we-want-to-know-how-many-customers-have-one-two-and-three-or-more-addresses:0dc054d968169eec76f7161b9d98121b&#34;&gt;We want to know how many customers have one, two and three or more addresses.&lt;/h2&gt;
Since the purpose of this article is to discuss subquery vs CTEs, excuse the somewhat contrived query.
At the end of this article I&#39;ll show  a simpler way to accomplish the tast at hand.

&lt;h2 id=&#34;sub-query-building-block:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query building block&lt;/h2&gt;

&lt;p&gt;We can establish that no BusinessEntity has more than two addresses by the empty result set returned by:&lt;/p&gt;
select ba.BusinessEntityId, count(ba.AddressId)
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2;

&lt;p&gt;We&amp;rsquo;ll assume for the remainder of this article that  this query is our best attempt, and that
it is a &amp;ldquo;black box&amp;rdquo; with which we cannot tamper, save for adjusting the count in the HAVING clause.&lt;/p&gt;

&lt;p&gt;Note that technically we do not need the additional join:&lt;/p&gt;
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId

&lt;p&gt;however this ensures we do not consider addresses in isolation - we ensure all addresses actually have an existing business tied to them.&lt;/p&gt;

&lt;h2 id=&#34;sub-query-with-count-of-zero-instead-of-nothing:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query with count of zero instead of nothing&lt;/h2&gt;

&lt;p&gt;Now we would like to improve this result a bit, by rather returning a count of zero instead of nothing.
We do this by wrapping the above query in a super query:&lt;/p&gt;
select 
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses;

&lt;p&gt;Not too bad in terms of readability, and since we alias the sub query we always know where the result for the outer select column list comes from.
But what if we would like to augment this query to also return the number of businesses with only one, and two addresses respectively?
Or to cover all possabilities, the number of businesses with one, two and three or more addresses respectively.
Let&amp;rsquo;s try with the sub query approach:&lt;/p&gt;

&lt;h2 id=&#34;combined-sub-query-approach:0dc054d968169eec76f7161b9d98121b&#34;&gt;Combined sub query approach&lt;/h2&gt;
select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress,
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses,
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses;

&lt;p&gt;Hmm&amp;hellip; We get zero for everything. Something has gone wrong.
Running each super/sub query pair separately we obtain a number for the one and two address scenario, but nothing for the three or more one.
Somehow, the zero result from the three or more address scenario is forcing the other two to become zero as well.
Well, where else would two hole numbers and zero uniformly combined yeald zero?
Multiplication of course, and observing how we unwittingly join the three subqueries with a comma (&amp;ldquo;,&amp;rdquo;) which is nothing but a
cartesian or cross join, everything start to make sense!
The cross join is causing our result set to first be the number of rows returned by the first subquery,  but then these rows are &amp;ldquo;multiplied&amp;rdquo; in the first cross join with the two address subquery, at which point we are already off track, and finally, to make matters even
worse, the last subquery returns zero rows, and anything multiplied by zero is of course again zero.&lt;/p&gt;

&lt;h2 id=&#34;fixup-by-throwing-more-sub-queries-at-the-problem:0dc054d968169eec76f7161b9d98121b&#34;&gt;Fixup by throwing more sub queries at the problem&lt;/h2&gt;

&lt;p&gt;Now, let&amp;rsquo;s first remedy this situation by cross joining on only one row each time.
Hold on, it is going to get a bit messy since in order to do this, we have to introduce yet another subquery.&lt;/p&gt;
select 
A.NumberOfBusinessesWithOneAddress,
B.NumberOfBusinessesWithTwoAddresses,
C.NumberOfBusinessesWithMoreThanTwoAddresses
from
(select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress) as A,
(select
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses) as B,
(select
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses) as C;

&lt;p&gt;Success, but at quite a complexity and readability cost.&lt;/p&gt;

&lt;h2 id=&#34;first-cte-attempt:0dc054d968169eec76f7161b9d98121b&#34;&gt;First CTE attempt&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s see if we can simplify this into something more readable by using a few CTEs instead of so many subqueries.&lt;/p&gt;

&lt;p&gt;Our query will consist of 3 parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First, the three innermost sub queries are stated as CTEs named businessEntitiesWithOneAddress, businessEntitiesWithTwoAddresses and businessEntitiesWithMoreThanTwoAddresses.&lt;/li&gt;
&lt;li&gt;Next, the encapsulating super queries that reduce the sub queries to  single values are also stated as CTEs named A, B and C.&lt;/li&gt;
&lt;li&gt;Finally we simply select from the aggregation CTEs A, B and C.&lt;/li&gt;
&lt;/ol&gt;
-- First, the three innermost sub queries are stated as CTEs named businessEntitiesWithOneAddress, businessEntitiesWithTwoAddresses and businessEntitiesWithMoreThanTwoAddresses.
with  businessEntitiesWithOneAddress as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
), businessEntitiesWithTwoAddresses as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
), businessEntitiesWithMoreThanTwoAddresses  as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
), 

-- Next, the encapsulating super queries that reduce the sub queries to  single values are also stated as CTEs named A, B and C.
A as
(select count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress
from businessEntitiesWithOneAddress
), B as
(select count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses
from businessEntitiesWithTwoAddresses
), C as
(select count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from businessEntitiesWithMoreThanTwoAddresses
)

-- Finally we simply select from the aggregation CTEs A, B and C.
select
A.NumberOfBusinessesWithOneAddress,
B.NumberOfBusinessesWithTwoAddresses,
C.NumberOfBusinessesWithMoreThanTwoAddresses
from A, B, C;

&lt;h2 id=&#34;is-this-not-so-much-more-readable:0dc054d968169eec76f7161b9d98121b&#34;&gt;Is this not so much more readable?&lt;/h2&gt;

&lt;p&gt;This last query (with CTEs only) is functionally equivalent   to the full query with only sub queries, yet, personally I find the CTE-based one
significnatly more readable and understandable.
For me there are many reasons for this, but I list a few:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instead of a nested style, the CTEs flow in a top to bottom manner.&lt;/li&gt;
&lt;li&gt;Each similar set of queries are grouped together.&lt;/li&gt;
&lt;li&gt;The CTEs are named upfront, so you have an idea of what is being done right from the start.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;cte-support:0dc054d968169eec76f7161b9d98121b&#34;&gt;CTE support&lt;/h2&gt;

&lt;p&gt;CTE is well supported among database systems, and I was surprised to learn that even SqLite supports it!&lt;/p&gt;

&lt;p&gt;Wikipedia says:&lt;/p&gt;

&lt;p&gt;Recursive CTEs are also supported by&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Microsoft SQL Server,&lt;/li&gt;
&lt;li&gt;Firebird 2.1,&lt;/li&gt;
&lt;li&gt;PostgreSQL 8.4+,&lt;/li&gt;
&lt;li&gt;SQLite 3.8.3+,&lt;/li&gt;
&lt;li&gt;Oracle 11g Release 2,&lt;/li&gt;
&lt;li&gt;IBM Informix version 11.50+ and&lt;/li&gt;
&lt;li&gt;CUBRID.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CTE support has been lacking from MySQL for some time now, however, derived tables can be used successfully to achieve standard or non recursive CTE functionality.
The good news is that &lt;a href=&#34;http://mysqlserverteam.com/mysql-8-0-labs-recursive-common-table-expressions-in-mysql-ctes/&#34;&gt;full featured CTE support, including recursiveness&lt;/a&gt; is not far off for MySQL.&lt;/p&gt;

&lt;h2 id=&#34;but-can-we-not-just-join-better:0dc054d968169eec76f7161b9d98121b&#34;&gt;But, can we not just join better?&lt;/h2&gt;

&lt;p&gt;Now that I&amp;rsquo;ve shown how the use of CTEs can eliminate sub queries, lets briefly return to the problem of the cartesian join.
I really did not like the additional sub query, and it&amp;rsquo;s additional CTE counterpart, for reducing the result sets into something that could
cross join without problems (those A, B and C).
Is there not perhaps a way to join without throwing anything away?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider the &lt;a href=&#34;http://sqlmag.com/t-sql/t-sql-join-types&#34;&gt;possible join types&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;CROSS JOIN :  We already saw this does not work.&lt;/li&gt;
&lt;li&gt;INNER JOIN : This will also not work since per definition our three sets are disjoint / contain no similar items.&lt;/li&gt;
&lt;li&gt;LEFT, RIGHT JOIN : We have the same problem; our sets are disjoint.&lt;/li&gt;
&lt;li&gt;FULL JOIN : Bingo! Full joins attempt to join on the join condition, but thereafter behaves like a LEFT and RIGHT JOIN combined; nothing is lost, and unmatched rows are still returned.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Did you know about the full join? I will admit that although I knew there had to be something like it, I did not know / forgotten all about it.&lt;/p&gt;

&lt;h2 id=&#34;sub-query-approach-simplified-with-a-full-join:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query approach simplified with a FULL JOIN&lt;/h2&gt;
select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress 
full join
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses 
on
businessEntitiesWithOneAddress.BusinessEntityId = 
businessEntitiesWithTwoAddresses.BusinessEntityId  
full join
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
) as businessEntitiesWithMoreThanTwoAddresses
on
businessEntitiesWithTwoAddresses.BusinessEntityId   = 
businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId;

&lt;p&gt;Yes, we do away with one level of sub queries, but it&amp;rsquo;s still not great.&lt;/p&gt;

&lt;h2 id=&#34;cte-approach-simplified-with-full-join:0dc054d968169eec76f7161b9d98121b&#34;&gt;CTE approach simplified with FULL JOIN&lt;/h2&gt;
with  businessEntitiesWithOneAddress as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
), businessEntitiesWithTwoAddresses as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
), businessEntitiesWithMoreThanTwoAddresses  as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &gt; 2
)
select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from businessEntitiesWithOneAddress
full join businessEntitiesWithTwoAddresses
on businessEntitiesWithOneAddress.BusinessEntityId = businessEntitiesWithTwoAddresses.BusinessEntityId
full join businessEntitiesWithMoreThanTwoAddresses
on businessEntitiesWithTwoAddresses.BusinessEntityId = businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId

&lt;h2 id=&#34;simplest-solution:0dc054d968169eec76f7161b9d98121b&#34;&gt;Simplest solution&lt;/h2&gt;

&lt;p&gt;Finally, if we relax the requirement (which was self imposed in the first place) to have the results in one row,
as well as not worrying about returning the count for the three or more scenario,
we can simplify everything very nice like this:&lt;/p&gt;
with  businessEntitiesAddressCount as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
)
select 
count(BusinessEntityId) as NumberOfBusinesses,
NumberOfAddresses 
from businessEntitiesAddressCount 
group by NumberOfAddresses;

&lt;p&gt;Best of all; we still get to use a wonderful CTE!&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts:0dc054d968169eec76f7161b9d98121b&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;The first time I realised that it is possible to nest queries arbitrarily it seemed like the sky would be the limit to what wonderfully complex and deep stuff one could do,
all be it with a heavy performance penalty if you were not careful.
As time went on and I wrote larger and larger queries, often times with many levels of nested sub queries, it became
apparent that the readability of the code drastically reduced.
In general purpose programming languages one could avoid deep nested structures quite successfully, but in SQL there seemed to be no way around it
save for making use of temporary tables, or breaking the work up into smaller parts.&lt;/p&gt;

&lt;p&gt;When I found out about CTEs I was very impressed - what an elegant solution!
Yes, a carefully thought out join can often times remove the need for a sub query or CTE, but in cases where sub query is required I now opt for the CTE approach instead.&lt;/p&gt;

&lt;p&gt;Some prominant SQL experts even &lt;a href=&#34;http://modern-sql.com/use-case/literate-sql&#34;&gt;equate the use of CTEs / the WITH clause to literate programming&lt;/a&gt;, and I concur as far as readability and logical flow is concerned.
There is much more to CTEs, especially when considering the recursive variaty, but that is a story for another time.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s been a bit of a ramble, but if you made it this far, you&amp;rsquo;ll do well to give CTEs a try,
and maybe even find it as useful as I do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>now make it fast</title>
      <link>https://blog.goodfast.info/post/now-make-it-fast/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://blog.goodfast.info/post/now-make-it-fast/</guid>
      <description>

&lt;p&gt;&amp;ldquo;He began to copy one of our tables, which contained partial user information, including email IDs, hashed passwords, and last tested URL. His copy operation
    locked the database table, which raised alerts on our monitoring system. On receiving the alerts, we checked the logs, saw an unrecognized IP, and blocked
    it right away. In that time, the hacker had been able to retrieve only a
    portion of the data.&amp;rdquo;
    &amp;ndash; From the postmortem of the
    &lt;a href=&#34;http://www.browserstack.com/attack-and-downtime-on-9-November&#34;&gt;Browser Stack hack of 9th November, 2014 at 23:30 GMT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since relational database management systems (RDBMS) have been
&lt;a href=&#34;http://en.wikipedia.org/wiki/Micro_DBMS&#34;&gt;used in production environments since 1970 (Micro DBMS)&lt;/a&gt;, and the theory on which they run was
developed in the preceding decade,and perfected in the three remaining decades
of the previous century (long  ago), it was not surprising  that, as  a subject,
it received very little attention
in our curriculum - at least where I studied.&lt;/p&gt;

&lt;p&gt;To further degrade the already &lt;em&gt;apparently&lt;/em&gt; low relevance  of the poor subject, we
had to cope with a very thick and overly theoretical textbook we could not yet appreciate,
and subscribe to, if you wanted to be cool and smart, to the snooty perception
held by many peers that the lowly database was useful, but entirely boring.&lt;/p&gt;

&lt;p&gt;A database was considered the type of thing  you rigged up for a family member in Access in a
few hours. Any larger system was the domain of &amp;lsquo;informatics&amp;rsquo; - the
less inspired, more practical brother field of study to the sexy &amp;lsquo;computer science&amp;rsquo; with its focus on AI, advanced programming in C++, the  new Java and the emerging world of Linux and Open Source.&lt;/p&gt;

&lt;p&gt;Stepping outside into the real world, and the rest was history - databases
everywhere, for everything!
In my working career since 2001, every single business system I worked on had
a RDBMS - primarily MS SQL Server, and sometimes MySQL or Oracle as the persistence store.
Through the years I accumulated skill in modelling domains and
manipulating the information through SQL, but optimisation of the storage
structures for efficiency was a task I chose to ignore. I reasoned along the
lines of it is the DBA&amp;rsquo;s / RDBMS&amp;rsquo;s work, or
the mostly false assumption that computers are fast and how much data will
the system  realistically have anyways?&lt;/p&gt;

&lt;p&gt;Really? Is that professional, to say this far will I go and no further,
while you as the programmer is directly responsible for those
horribly slow queries?
Yes, the system works, some things are slow, but hey, they have a lot of data
you say, it&amp;rsquo;s going to be slow at times!
Now you are suppressing   that little voice inside of you, quietly telling you it&amp;rsquo;s wrong
aren&amp;rsquo;t you - it &lt;em&gt;can&lt;/em&gt; be faster&amp;hellip;
Think of the waiting user, the wasted time, times hundreds for internal
systems, times thousands for customers, times  hundreds of thousands or even
millions in the case of the web&amp;hellip;
All that wasted time, all that wasted energy, all the trees&amp;hellip;
Don&amp;rsquo;t think about it too much.
You feel pretty bad by now don&amp;rsquo;t you&amp;hellip;&lt;/p&gt;

&lt;p&gt;Well, I have more bad news for you. At the end of this article you are going to feel even worse, because you
are going to see how simple it is to start to turn things around.&lt;/p&gt;

&lt;p&gt;Dramatics aside, we will have a quick look at the  choices  for table
physical layout and ponder the implications on performance.&lt;/p&gt;

&lt;p&gt;Be warned that we will of course only scratch the surface of SQL performance  optimisation
on SQL Server, and that this article is aimed at software developers
with limited skill in this area, so I&amp;rsquo;m going to explain a bit
- i.e. sit down, this might
take a while.&lt;/p&gt;

&lt;p&gt;Many folks have written fine articles on this subject, and I&amp;rsquo;ll refer to
some of those as we go along.&lt;/p&gt;

&lt;p&gt;My primary justification for writing this article is that I&amp;rsquo;d like to cement
this stuff in my mind, and the best way for me is to write it all out. I hope
you can gain something from this also, and please comment if you disagree or
whatever.&lt;/p&gt;

&lt;h2 id=&#34;it-s-out-there-somewhere-but-has-it-order:810306c57b772cfaef22459bbb55c3d7&#34;&gt;It&amp;rsquo;s out there&amp;hellip; Somewhere&amp;hellip; But has it order?&lt;/h2&gt;

&lt;p&gt;Contrary to popular current belief, the data isn&amp;rsquo;t somewhere in a [cloud]
but it actually resides on one or more  physical storage media (think disk
drives), and
importantly, it&amp;rsquo;s laid down either sorted or just as it was received - for all
practical purposes, unsorted.&lt;/p&gt;

&lt;p&gt;You probably knew this already, but I needed that sentence
because it contained the word &lt;em&gt;cloud&lt;/em&gt; so that I had an excuse to quote
Stallman on cloud computing from 2008:&lt;/p&gt;
&#34;It&#39;s stupidity. It&#39;s worse than stupidity: it&#39;s a marketing hype campaign,&#34;

&lt;p&gt;Back to tables&amp;hellip; It is probably because SQL Server&amp;rsquo;s default is to choose the
physically sorted way of laying down the data, when you define a primary key on
a new table, that this table organisation
prevails and is common in most systems.
There is nothing like one size fits all and although the
ordered layout of records is a great fit most of the time, it is not the best
layout in all cases (more on that later).
However, keep in mind there is wisdom in the choice of this default none
the less.&lt;/p&gt;

&lt;h2 id=&#34;some-terminology:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Some terminology&lt;/h2&gt;

&lt;p&gt;For the purpose  of the discussion we&amp;rsquo;ll stick to the most widely used
terminology in the SQL Server world and call the ordered layout of records a
&lt;em&gt;clustered index&lt;/em&gt;, and the
layout without any physical ordering a &lt;em&gt;heap table&lt;/em&gt;, or simply a heap.
The structure  existing purely to speed up locating records  we will refer to
as a &lt;em&gt;nonclustered index&lt;/em&gt;, and how the &lt;em&gt;clustered index&lt;/em&gt;
should be ordered we will call the &lt;em&gt;clustering key&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;clustered-index-the-sorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustered index - the sorted one&lt;/h3&gt;

&lt;p&gt;The term clustered index is unfortunate, and probably the reason a
lot of people develop a blurred notion of cluster vs. non clustered, and table
vs. index.
You could think of a &lt;em&gt;clustered index&lt;/em&gt; as a
database table for maintaining data in an ordered fashion (ordered by one or more
columns) thereby giving efficient access to all the data for one record if the
values of some of the sorting columns are known.  Alternatively you could think of a &lt;em&gt;clustered index&lt;/em&gt; as an
database index to
efficiently gain access to more data,
organised by a subset of  all the columns, yet it has all that added
data in itself.
Either way, Viewed as an ordered tabular representation of data, or an index existing for
efficient access to itself - the important point is the physical sorted
layout, and that the most efficient way to retrieve records from it is via the
index that is itself given the primary key.&lt;/p&gt;

&lt;h3 id=&#34;heap-the-unsorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Heap - the unsorted one&lt;/h3&gt;

&lt;p&gt;A heap is simpler to understand.  Think of a file that grows by repeatedly appending new
lines to it without trying to maintain any kind of order.
Obviously, for efficient access to the data in a heap we need one or more
nonclustered indexes targeting the heap&amp;rsquo;s data, and built up from one or more components
of that data.&lt;/p&gt;

&lt;p&gt;When comparing clustered indexes with heaps we will assume that at least for
the primary key, a nonclustered index is defined on the heap that would make the clustered
index and heap nearly equally  efficient when retrieving a record, given a single primary key value.&lt;/p&gt;

&lt;h3 id=&#34;nonclustered-index-the-real-index:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Nonclustered index - the real index&lt;/h3&gt;

&lt;p&gt;The term &lt;em&gt;nonclustered index&lt;/em&gt; we will use to refer to the structure
that exists primarily to provide   indexed lookup to clustered indexes and heaps
alike.  In SQL Server a heap or clustered index can have many nonclustered
indexes targeting it.&lt;/p&gt;

&lt;h3 id=&#34;clustering-key-how-the-clustered-index-is-clustered-or-sorted:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustering key - how the clustered index is clustered, or sorted&lt;/h3&gt;

&lt;p&gt;Apart from the notion of a primary key, for a clustered index, the &lt;em&gt;clustering
key&lt;/em&gt; defines the subset of columns that instruct the system on how to cluster
or sort the records.
A clustering key should be chosen to be as unique  as possible, but uniqueness
is not required (unlike for the primary key),since the system will add four bytes  called the uniquifier to
the clustering key if it is not indicated as being unique already.&lt;/p&gt;

&lt;h2 id=&#34;implications-of-heap-vs-clustered-index-for-dml:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Implications of heap vs. clustered index for DML&lt;/h2&gt;

&lt;p&gt;To compare the efficiency implications of DML statements on these two broad ways of physical data layout we have to look closer
at the nature of the operations we would like to perform.&lt;/p&gt;

&lt;p&gt;It is fairly intuitive  to reason about  this if we keep in mind:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The physical layout of a heap vs. a
clustered index.&lt;/li&gt;
&lt;li&gt;Indexes should stay up to date after the operation completes.&lt;/li&gt;
&lt;li&gt;Is the operation on one or on multiple records.&lt;/li&gt;
&lt;li&gt;What piece of data is required for the fastest retrieval of records.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;the-single-index-primary-key-as-clustering-key-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;The single index, primary key as clustering key scenario.&lt;/h3&gt;

&lt;p&gt;It turns out that it is generally the best choice to use a clustered index
instead of a heap, for the scenario where you need only one index on
one or more indexing columns. For this scenario, a clustered index takes up
less space, performs better overall, and releases space better when records
get deleted.&lt;/p&gt;

&lt;p&gt;The short answer to why this is the case is that for a clustered index, the
data is the index, so lookups on the clustering key finds the relevant records
directly (after climbing the index B-tree), and alterations affecting the clustering key requires alterations to
one structure - the clustered index. For a heap plus one nonclustered index
however, lookups given the index key is a two-step process. First the nonclustered
index is queried to find the RID, the uniquely identifying key for each heap
row corresponding to the clustering key, and
then the RID is used to fetch the record from the heap.
In addition to this, although alterations involving the index key might not require much work on the heap,
the nonclustered index needs to be updated - again a two-step process.&lt;/p&gt;

&lt;p&gt;Nevertheless, there is a great case to be made for choosing a heap over a
clustered index, which we&amp;rsquo;ll get to shortly, but for the scenario as explained
above (one index on the clustering key), a clustered index is the better choice.&lt;/p&gt;

&lt;p&gt;See this &lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc917672.aspx&#34;&gt;Microsoft best practices&lt;/a&gt; white paper, but be
warned that it is proving the superiority of choosing a clustered index in
favor of a heap in a scenario
where a clustered index is the  best choice.
Do not be fooled by this article into thinking heaps are overall inferior to
clustered indexes, as a casual reading might lead you into believing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at the various DML operations quickly on clusterd index and heap&amp;hellip;&lt;/p&gt;

&lt;h4 id=&#34;insert:810306c57b772cfaef22459bbb55c3d7&#34;&gt;INSERT&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;INSERT&lt;/em&gt; into a heap is simply a case of appending where there is space
available in a data page (or creating a new page at the end). After this, a
second write operation is required - insertion of the index key into the
B-tree  of the nonclustered index.&lt;/p&gt;

&lt;p&gt;Now for a clustered index, the new record
must be inserted in the correct location. If this correct location is on a
data page that is full, that page needs to be split in two .
This might be more time-consuming than the simple insert on the heap, but for
a clustered index there is no second write operation to maintain the index -
the data and index are one. Note that this might even turn out to make a
clustered index perform better overall since it only requires one write
operation.&lt;/p&gt;

&lt;p&gt;Any additional nonclustered indexes on a clustered index or heap would take
roughly the same time to update or keep in sync with the actual data, so we
can ignore their maintenance penalty when comparing.&lt;/p&gt;

&lt;p&gt;But surely we should be able to gain performance with inserts by choosing
either of the two table layouts,   given that heaps
and clustered indexes differ so fundamentally?
In this particular scenario they are  equally good because both
options still require that indexes be kept up to date. If we were to choose a
heap and define absolutely no nonclustered index on it we will gain the fastest
insert performance since inserting would simply be adding data on anywhere
where there is space.&lt;/p&gt;

&lt;p&gt;There is actually a great use case for this: logs and other archival type of
storage that do not require immediate querying.&lt;/p&gt;

&lt;p&gt;But optimisation is a tricky problem, since even for this use case, if many
concurrent inserts are expected, it might   very well be better to choose a
clustered index instead, on some non unique but range-like clustering key that
will jump around a bit, to
achieve an effect of inserting in different locations to prevent everything
from trying to insert in the same data page all the time (as would be the
case for a heap).&lt;/p&gt;

&lt;h4 id=&#34;select-update-and-delete:810306c57b772cfaef22459bbb55c3d7&#34;&gt;SELECT, UPDATE and DELETE&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;SELECT&lt;/em&gt;, &lt;em&gt;UPDATE&lt;/em&gt; and &lt;em&gt;DELETE&lt;/em&gt; first requires locating where the records to
operate on need to be physically
found, followed by the actual action on the data.&lt;/p&gt;

&lt;p&gt;Assuming that the operation simply applies to data given a single primary key
value that is the clustering key, the finding
or first part of this operation
is slightly  less efficient for a heap compared to  a clustered index. For
a clustered index, after the tree is climbed the information is there and
ready to be retrieved, while for a heap, after climbing the tree of the
nonclustered index, you only get the RID, and then require a second
operation to (all be it directly) get at the data in the heap - one additional
level of indirection.&lt;/p&gt;

&lt;p&gt;There is an option to &lt;em&gt;include&lt;/em&gt; columns of the heap or clustered index in the
nonclustered indexes. The effect of this is that, after climbing the tree of
the nonclustered index, those &lt;em&gt;included&lt;/em&gt; column&amp;rsquo;s data is immediately
available - a mini clustered index in the form of a nonclustered index with
included columns. All very straight forward and unambiguous  wouldn&amp;rsquo;t you say?&lt;/p&gt;

&lt;p&gt;Except for &lt;em&gt;SELECT&lt;/em&gt;, the efficiency of the second part, the action part of the operation
can vary much more between heap and clustered index.
For &lt;em&gt;UPDATE&lt;/em&gt;, if the column being updated happens to be one or more of the
columns comprising the clustering key, and the table is a clustered index, then
in order to keep the data  sorted, the system might have to
do page splits.
This in turn mean that potentially, large amounts of data need to be copied around.
For a heap this is never the case, and the columns  can simply be updated in
place - order is of no importance.&lt;/p&gt;

&lt;p&gt;For both heap and clustered index, if any of the columns are part of any
defined nonclustered indexes then altering them might have nonclustered index
maintenance time as a further performance penalty.&lt;/p&gt;

&lt;p&gt;Fortunately, for both heap and clustered index, if the columns being updated are not part
of the clustering key the efficiency of the action part of the operation is nearly similar.&lt;/p&gt;

&lt;p&gt;Performing a &lt;em&gt;DELETE&lt;/em&gt; on a heap or clustered index should be simply a case of marking that record
as deleted and making the space available for potential future inserts. For a
clustered index, no index maintenance is yet again needed while for the heap,
the nonclustered index needs updating.&lt;/p&gt;

&lt;h2 id=&#34;welcome-to-the-real-world-where-tree-climbing-is-to-be-avoided-the-multiple-indexes-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Welcome to the real world, where tree climbing is to be avoided - the multiple indexes scenario&lt;/h2&gt;

&lt;p&gt;The single index, primary key as clustering key and lookup scenario described earlier
might appear  early on, and a lot in most models, but very soon you
will also want to efficiently query on other columns on wider  (more columns)
and deeper (more rows) tables.&lt;/p&gt;

&lt;p&gt;To prevent full table scans, you start adding nonclustered indexes,
and this is where heaps start to become the more attractive alternative.&lt;/p&gt;

&lt;p&gt;Suppose for a moment that your  table (let&amp;rsquo;s call it table &lt;em&gt;T&lt;/em&gt;) that became wide and deep overnight is a
clustered index, and the primary key (&lt;em&gt;K&lt;/em&gt;) is also the clustering key.&lt;/p&gt;

&lt;p&gt;Each additional nonclustered index (&lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip;) on &lt;em&gt;T&lt;/em&gt; will store in its leaf nodes the
values of &lt;em&gt;K&lt;/em&gt;.
This means that a query on &lt;em&gt;T&lt;/em&gt; utilising  some nonclustered index &lt;em&gt;N&lt;/em&gt;  results
in a tree climb of &lt;em&gt;N&lt;/em&gt; that yields some value of &lt;em&gt;K&lt;/em&gt;. Following this we
require another tree climb of the clustered index that
is &lt;em&gt;T&lt;/em&gt;, given a value for &lt;em&gt;K&lt;/em&gt;, and only then is the actual data reached.&lt;/p&gt;

&lt;p&gt;On the other hand, suppose  now that your wide and deep table  &lt;em&gt;H&lt;/em&gt; is a heap
instead, with one or more nonclustered indexes &lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip; and so on.
This time, each nonclustered index &lt;em&gt;N&lt;/em&gt; will store at the leaf node the RID
of the relevant row, and not simply  yet another key into a further index.
This means that if some query on &lt;em&gt;H&lt;/em&gt; utilise one of the nonclustered indexes,
only one tree climb of that nonclustered index is required, after which the
RID is obtained, and unlike a clustering key, a RID represents the
physical position of the record in the heap, and thus can be directly accessed
- no further tree climbing required.&lt;/p&gt;

&lt;p&gt;For a more in-depth look at this, and some hard numbers comprising a
compelling case,  do yourself a favor and read Markus
Wienand&amp;rsquo;s fine article,
&lt;a href=&#34;http://use-the-index-luke.com/blog/2014-01/unreasonable-defaults-primary-key-clustering-key&#34;&gt;Unreasonable Defaults: Primary Key as Clustering
Key&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary-and-closing-thoughts-optimising-performance-is-an-interesting-and-very-relevant-problem:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Summary and closing thoughts - Optimising performance is an  interesting and very relevant problem&lt;/h2&gt;
&#34;Premature optimisation is the root of all evil&#34;
 -- Donald E. Knuth

&lt;p&gt;As much as I concur with that statement, especially how it applies to code, I do think that a good understanding
of the options available to you when turning a data model into an actual
database schema can proactively prevent vicious      cycles of poor performing
monster database servers.
Yes, there is a lot of things one can do, and the precise case where one
technique or option would be the better option is hard to identify, but the
better your understanding of the internals, the more likely you are to get it
right first time,
and the more it will start to happen that you are writing a query and you
suddenly realise that a specific index would benefit that query tremendously.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve rambled a bit, but to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are two main choices for the physical layout of data for tables.&lt;/li&gt;
&lt;li&gt;If only one index on the primary key is required it&amp;rsquo;s probably the best to choose a clustered index.&lt;/li&gt;
&lt;li&gt;For a many index scenario choose a heap.&lt;/li&gt;
&lt;li&gt;For best insert performance on high  loads choose a heap with no nonclustered indexes.&lt;/li&gt;
&lt;li&gt;Use the include columns feature of nonclustered indexes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Today there are exciting new alternative data storage technologies like fully
in memory databases, distributed systems such  as Hadoop, Google&amp;rsquo;s BigTable
approach, and document oriented noSQL
options such as ElasticSearch to name only a very few.
These alternative solutions to the problem of working with large data sets
have and will continue to be applied more and more, but if the last decade is
anything to go by,  the relational database is going to stick around for quite
some time still, so investing time into learning how to optimise it is
time well spent.&lt;/p&gt;

&lt;p&gt;The reason for SQL systems remaining central to all serious data storage
applications  is not by accident. There is a theoretical reason, routed in the
so-called CAP theorem.
For an overview of how the CAP theorem restricted the growth and adoption of
noSQL systems, have a look at
&lt;a href=&#34;http://use-the-index-luke.com/blog/2013-04/whats-left-of-nosql&#34;&gt;Whats left of NoSQL?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An interesting response to this is FoundationDB (see
&lt;a href=&#34;http://www.theregister.co.uk/Print/2012/11/22/foundationdb_fear_of_cap_theorem/&#34;&gt;NoSQL&amp;rsquo;s CAP theorem busters: We don&amp;rsquo;t drop ACID&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;An exciting emerging trend is to harness the strengths of both the traditional
RDBMS and the more recent big data distributed, more normalised data storage
technologies. For an interesting application of this, see
&lt;a href=&#34;http://msdn.microsoft.com/en-gb/magazine/dn802606.aspx&#34;&gt;Use Updatable Tables for Responsive Real-Time Reporting&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I hope this short info burst tickled your interest enough that you will go
ahead and look into all of it a bit more.
Personally I have been pleasantly surprised by the depth of this subject area.&lt;/p&gt;

&lt;p&gt;I can highly recommend the book
&lt;a href=&#34;http://sql-performance-explained.com/?utm_source=UTIL&amp;amp;utm_medium=main&amp;amp;utm_campaign=second&#34;&gt;SQL Performance Optimisation&lt;/a&gt; for an in-depth look at this
subject, and the
&lt;a href=&#34;http://use-the-index-luke.com/&#34;&gt;Use The Index Luke&lt;/a&gt; site.&lt;/p&gt;

&lt;p&gt;This article also appears on &lt;a href=&#34;http://www.inivit.com/blog/&#34;&gt;Inivit&amp;rsquo;s blog&lt;/a&gt; along with some other fine posts from former colleagues.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>