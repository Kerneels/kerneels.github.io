<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Optimisation on Good Fast</title>
    <link>https://kerneels.github.io/categories/optimisation/</link>
    <description>Recent content in Optimisation on Good Fast</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Oct 2016 22:01:20 +0200</lastBuildDate>
    <atom:link href="https://kerneels.github.io/categories/optimisation/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Knapsack Bitwise</title>
      <link>https://kerneels.github.io/post/knapsack-bitwise/</link>
      <pubDate>Wed, 12 Oct 2016 22:01:20 +0200</pubDate>
      
      <guid>https://kerneels.github.io/post/knapsack-bitwise/</guid>
      <description>

&lt;p&gt;An interesting bit of computer science, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Knapsack_problem&#34;&gt;knapsack problem&lt;/a&gt; has been studied for over a century, and according to Wikipedia, seems to be quite  popular - as these sort of things go.
For the first post in this series I&amp;rsquo;ll present a solution to the &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;0&lt;/sub&gt;, or binary version of this famous problem
I designed in 2015.&lt;/p&gt;

&lt;p&gt;I was immediately intrigued by it when I first read the problem statement.
It&amp;rsquo;s application to anything requireing optimal resource allocation was
very clear, and my mind started obsessively thinking of how to solve this efficiently.
Now with a heavily studied problem like this, there are of course already many algorithms
developed, but since this problem was a test, I did not look anything up, and just started coding a solution as soon as I had one.&lt;/p&gt;

&lt;p&gt;The full solution is &lt;a href=&#34;https://github.com/Kerneels/knapsack&#34;&gt;available here on GitHub&lt;/a&gt;, but you are encouraged to copy and paste from this article, into your own project, to understand the whole thing bit by bit.&lt;/p&gt;

&lt;h2 id=&#34;what-is-it:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;What is it?&lt;/h2&gt;

&lt;p&gt;The 0/1 or binary variant is very simple. Given a set of items, where each item has a weight and value, determine the optimal
selection of items such that the sum of the weight of all the items do not exceed some limit,
while the sum of the value of all of the items is maximised.&lt;/p&gt;

&lt;p&gt;The 0/1 or binary part of the name comes from the restriction that only one of each item may be chosen.
The &amp;ldquo;knapsack&amp;rdquo; in the name refers to a ficticious rugsack or bag that can only contain a given weight.
The complexity of the problem lies in the exponential explosion of all the possible selection of items.&lt;/p&gt;

&lt;h2 id=&#34;a-concrete-example:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;A concrete example&lt;/h2&gt;

&lt;p&gt;Suppose we have the following set of 15 items:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Number&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Weight Grams&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;map&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;compass&lt;/td&gt;
&lt;td&gt;130&lt;/td&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;water&lt;/td&gt;
&lt;td&gt;1530&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Gold bar&lt;/td&gt;
&lt;td&gt;3000&lt;/td&gt;
&lt;td&gt;130&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;sandwich&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;glucose&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;tin&lt;/td&gt;
&lt;td&gt;680&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;banana&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;apple&lt;/td&gt;
&lt;td&gt;390&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;cheese&lt;/td&gt;
&lt;td&gt;230&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;beer&lt;/td&gt;
&lt;td&gt;620&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;suntan cream&lt;/td&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;camera&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;T-shirt&lt;/td&gt;
&lt;td&gt;240&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;trousers&lt;/td&gt;
&lt;td&gt;480&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Our knapsack can only hold a maximum of 4 Kg or 4000 grams, but we want to choose  a selection of items (or inventory) with the highest possible value.&lt;/p&gt;

&lt;h2 id=&#34;how-many-possible-inventories-exist:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;How many possible inventories exist?&lt;/h2&gt;

&lt;p&gt;Given we have &lt;code&gt;n&lt;/code&gt; items to choose from, each with a weight &lt;code&gt;w&lt;/code&gt; and value &lt;code&gt;v&lt;/code&gt;, we observe that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We will never be choosing zero items.&lt;/li&gt;
&lt;li&gt;We can either choose to include an item or not.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We take a stab at a formula for calculating the number of choices: &lt;code&gt;c = 2^n - 1&lt;/code&gt;. The &lt;code&gt;2^n&lt;/code&gt; is because each item can either be chosen or not chosen, and the &lt;code&gt;- 1&lt;/code&gt; is to eliminate the selection of not choosing anything.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how it holds for one item: &lt;code&gt;c = 2^1 - 1 = 2 - 1 = 1&lt;/code&gt;, obvious, since with one item you only have one choice - this can be our base case, so for &lt;code&gt;n = 1&lt;/code&gt; the formula holds.
Now we consider &lt;code&gt;n + 1&lt;/code&gt;, so for 2: &lt;code&gt;c = 2^2 - 1 = 4 - 1 = 3&lt;/code&gt;, and this makes sense since you can either choose one of the items or both, so for &lt;code&gt;n + 1&lt;/code&gt; the formula also holds.
So, by mathematical induction, our formula is proven, although we knew it was going to be right intuetively.&lt;/p&gt;

&lt;p&gt;Using our formula, we conclude that there are &lt;code&gt;c = 2^15 - 1 = 32768 - 1 = 32767&lt;/code&gt; possible inventories.&lt;br /&gt;
This number is neglegeable in computer terms, yet already far too big for by hand calculation.&lt;/p&gt;

&lt;h2 id=&#34;brute-force-solution:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Brute force solution&lt;/h2&gt;

&lt;p&gt;Since our particular senario is so small we will simply do a brute force search to find the best inventories (if any) that satisfy the 4000 grams weight limit.&lt;/p&gt;

&lt;p&gt;Any inventory can only have a maximum of 15 items, so we will represent an arbitrary inventory with a
bit mask, where each bit will represent a particular item, with 0 meaning the item is not chosen, and 1 meaning the item was chosen.&lt;/p&gt;

&lt;p&gt;The bit mask will drive a calculation function to determine the total weight and value of a given inventory.
Considering all possible inventories is now reduced to iterating over the number 1 through 32767 possible options,
calculating the sum of the weights and values each time, and retaining those inventories where the weight limit is satisfied.&lt;/p&gt;

&lt;h2 id=&#34;complexity:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Complexity&lt;/h2&gt;

&lt;p&gt;Using our formula, we can say that our solution will have &lt;code&gt;O(2^n - 1)&lt;/code&gt; time complexity, and similarly, worst case &lt;code&gt;O(2^n - 1)&lt;/code&gt;
space complexity.&lt;/p&gt;

&lt;h2 id=&#34;implementation:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Implementation&lt;/h2&gt;

&lt;h3 id=&#34;belonging-and-inventory-class:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Belonging and Inventory class&lt;/h3&gt;

&lt;p&gt;Looking at the table of belongings, we realise we would need a class to
represent individual belongings (we&amp;rsquo;ll call it &lt;code&gt;class Belonging&lt;/code&gt;), as well as the set of all   of them (&lt;code&gt;class Inventory&lt;/code&gt;).&lt;/p&gt;
namespace Knapsack
{
public class Belonging
	{
		public byte Number { get; set; }
		public string Name { get; set; }
		public int GramsWeight { get; set; }
		public int Value { get; set; }
	}

	public class Inventory
	{
		public static List&lt;Belonging&gt; AllGear { get; set; }

		public uint Gear { get; set; }
	}
}

&lt;h3 id=&#34;functions-for-belonging-class:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Functions for Belonging class&lt;/h3&gt;

&lt;p&gt;Now that we&amp;rsquo;ve dealt with the data needs, let&amp;rsquo;s add some functionality to each of the classes.
In true &lt;a href=&#34;http://agiledata.org/essays/tdd.html&#34;&gt;test driven development&lt;/a&gt; style (also see &lt;a href=&#34;https://en.wikipedia.org/wiki/Test-driven_development&#34;&gt;Wikipedia on TDD&lt;/a&gt;), let&amp;rsquo;s first create unit tests for each function, followed by an implementation.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll wrap all the test classes in a general &lt;code&gt;TestKnapsack&lt;/code&gt; class which we can use to do setup and teardown actions if required.&lt;/p&gt;
namespace Knapsack
{
	[TestClass]
	public class TestKnapsack
	{
		[TestClass]
		public class TestBelonging : TestKnapsack
		{
			[TestMethod]
			public void ShouldAddRemoveAndConfirmItIsInGear()
			{
				uint gear = 0; // empty set of gear
				var testBelonging = new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 };

				Assert.IsFalse(testBelonging.IsInGear(gear));
				uint gearAfterAdd = testBelonging.AddToGear(gear);
				Assert.IsFalse(testBelonging.IsInGear(gear));
				Assert.IsTrue(testBelonging.IsInGear(gearAfterAdd));
				Assert.AreNotEqual(gearAfterAdd, gear);

				uint gearAfterRemove= testBelonging.RemoveFromGear(gearAfterAdd);
				Assert.AreEqual(gearAfterRemove, gear);
				Assert.IsFalse(testBelonging.IsInGear(gearAfterRemove));
			}
		}
	}
}

&lt;p&gt;Returning to our &lt;code&gt;Belonging&lt;/code&gt; class,  let&amp;rsquo;s implement the methods we described in our unit test class.&lt;/p&gt;

&lt;p&gt;We will make heavy use of the bitwise operators:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; : bit shifts, which moves all the bits in a number left or right by the given count&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; : bitwise AND and OR which combines the two numbers bit by bit and returns the resulting number&lt;/li&gt;
&lt;li&gt;&lt;code&gt;~&lt;/code&gt; : the compliment or negation unary operator that inverts all bits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Make sure you are completely familiar with all the &lt;a href=&#34;https://www.tutorialspoint.com/csharp/csharp_bitwise_operators.htm&#34;&gt;C# bit wise operators&lt;/a&gt; before proceeding.&lt;/p&gt;

&lt;p&gt;Here are the functions we add to &lt;code&gt;Belonging&lt;/code&gt;:&lt;/p&gt;
public uint AddToGear(uint gear)
		{
			return gear | (uint)(1 &lt;&lt; this.Number);
		}
		public uint RemoveFromGear(uint gear)
		{
			return gear &amp; ~(uint)(1 &lt;&lt; this.Number);
		}
		public bool IsInGear(uint gear)
		{
			return (gear &amp; (1 &lt;&lt; this.Number)) == (1 &lt;&lt; this.Number);
		}
		public int GramsWeightInGear(uint gear)
		{
			return this.IsInGear(gear) ? this.GramsWeight : 0;
		}
		public int ValueInGear (uint gear)
		{
			return this.IsInGear(gear) ? this.Value : 0;
		}

&lt;p&gt;Success! Our tests all pass, and we can proceed to testing and developing &lt;code&gt;Inventory&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;functions-for-inventory:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Functions for Inventory&lt;/h3&gt;

&lt;p&gt;We start by testing the very simple summation functions.
Our test creates a sample set of belongings (&lt;code&gt;allKit&lt;/code&gt;) with 3 items in it.
We then add the first and last item from &lt;code&gt;allKit&lt;/code&gt; and ensure we get the correct totals when calling the summation functions on
&lt;code&gt;Inventory&lt;/code&gt;:&lt;/p&gt;
		[TestClass]
		public class TestInventory : TestKnapsack
		{
			[TestMethod]
			public void ShouldSumProperly()
			{
				var allKit = new List&lt;Belonging&gt;();
				allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
				allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
				allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });

				Inventory.AllGear = allKit;
				var testInventory = new Inventory { Gear = 0 };
				testInventory.Gear = allKit[0].AddToGear(
					testInventory.Gear);
				testInventory.Gear = allKit[2].AddToGear(
					testInventory.Gear);

				Assert.AreEqual(allKit[0].GramsWeight + allKit[2].GramsWeight,
					testInventory.TotalGramsWeight);
				Assert.AreEqual(allKit[0].Value + allKit[2].Value,
					testInventory.TotalValue);
			}
		}

&lt;p&gt;Here are the summation functions to add to &lt;code&gt;Inventory&lt;/code&gt;:&lt;/p&gt;

		public int TotalGramsWeight { get { return Inventory.TotalGramsWeightForGear(this.Gear); } }
		public int TotalValue { get { return Inventory.TotalValueForGear(this.Gear); } }

		public static int TotalGramsWeightForGear (uint gear)
		{
				return AllGear.Select(o =&gt; 
					o.GramsWeightInGear(gear))
					.Sum();
			}

		public static int TotalValueForGear(uint gear)
		{
				return AllGear.Select(o =&gt; 
					o.ValueInGear(gear))
					.Sum();
			}

&lt;p&gt;Nothing too complicated, and the tests all still pass. Next we move on to the actual search&lt;br /&gt;
for the valid inventories:&lt;/p&gt;

&lt;h3 id=&#34;search-functions-for-inventory:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Search functions for Inventory&lt;/h3&gt;

			[TestMethod]
			public void TestFirstBestInventory()
			{
				var allKit = new List&lt;Belonging&gt;();
				allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
				allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
				allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });
				Inventory.AllGear = allKit;

				Assert.IsTrue(
					allKit[1].IsInGear(
					Inventory.FirstBestInventory(220).Gear));

				Assert.IsFalse(
					allKit[2].IsInGear(
					Inventory.FirstBestInventory(220).Gear));
			}

			[TestMethod]
			public void TestValidInventories()
			{
				var allKit = new List&lt;Belonging&gt;();
				allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
				allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
				allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });
				Inventory.AllGear = allKit;

				// nothing except the empty inventory - so only one possible empty inventory
				Assert.AreEqual(1, Inventory.ValidInventories(5).Count());

				// since the upper weight limit is so large we end up with all possible inventories over 3 belongings, which is 8 including the empty inventory
				Assert.AreEqual(8, Inventory.ValidInventories(5000).Count());
			}

&lt;p&gt;We implement the new functions on &lt;code&gt;Invetory&lt;/code&gt; as follows:&lt;/p&gt;
		public static uint NumberOfCombinations
		{
			get
			{
				return (~(uint)0 % (uint)(1 &lt;&lt; AllGear.Count()));
			}
		}

		public static Inventory FirstBestInventory(int maxGramsWeight)
		{
			var numberOfCombinations = Inventory.NumberOfCombinations;
			int currentMaxValue = 0, overallBestValue = 0;
			var bestInventory = new Inventory();

			for (uint g = 0; g &lt;= numberOfCombinations; g++)
				if (Inventory.TotalGramsWeightForGear(g) &lt;= maxGramsWeight &amp;&amp;
					(currentMaxValue = Inventory.TotalValueForGear(g)) &gt; overallBestValue)
				{
					bestInventory = new Inventory
					{
						Gear = g
					};
					overallBestValue = currentMaxValue;
				}

			return bestInventory;
		}

		public static IQueryable&lt;Inventory&gt; ValidInventories(int maxGramsWeight)
		{
			var numberOfCombinations = Inventory.NumberOfCombinations;
			var validInventories = new List&lt;Inventory&gt;();

			for (uint g = 0; g &lt;= numberOfCombinations; g++)
				if (Inventory.TotalGramsWeightForGear(g) &lt;= maxGramsWeight)
					validInventories.Add(new Inventory
					{
						Gear = g
					}
					);

			return validInventories.AsQueryable();
		}

&lt;h3 id=&#34;final-wrapup:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Final wrapup&lt;/h3&gt;

&lt;p&gt;After testing everything again and getting all tests to still pass, we are very nearly done.
Our final work will be to create nice printout &lt;code&gt;ToString()&lt;/code&gt; methods, and putting it all together in a &lt;code&gt;main&lt;/code&gt; method which will be the entry point for the console application.&lt;/p&gt;

&lt;p&gt;We add the following to &lt;code&gt;Belonging&lt;/code&gt;:&lt;/p&gt;

		public override string ToString()
		{
			return string.Format(&#34;{0}, {1} g, valued at {2}&#34;,
				this.Name,
				this.GramsWeight,
				this.Value);
		}

&lt;p&gt;We want nice printout for &lt;code&gt;Inventory&lt;/code&gt; also, so we add the following to it:&lt;/p&gt;
public override string ToString()
		{
			StringBuilder sb = new StringBuilder();
			sb.AppendLine(&#34;---- Inventory Start:&#34;);
			AllGear.Where(o =&gt; o.IsInGear(this.Gear))
				.ToList()
				.ForEach(o =&gt; sb.AppendLine(o.ToString()));
			sb.AppendFormat(&#34;---- Inventory End: Total Weight: {0} g, Total Value: {1}&#34;,
				this.TotalGramsWeight,
				this.TotalValue);
			return sb.ToString();
		}

&lt;p&gt;Finally, we add the &lt;code&gt;main&lt;/code&gt; method, along with a little helper method for setting up our set of all kit:&lt;/p&gt;
class Program
	{
		static void Main(string[] args)
		{
			var allKit = LoadData();

			Inventory.AllGear = allKit;

			var bestInventories = Inventory
				.ValidInventories(maxGramsWeight: 4000)
				.OrderByDescending(o =&gt; o.TotalValue)
				.Take(5)
				.ToList();

			if (bestInventories.Count() == 0)
				Console.WriteLine(&#34;No inventory match the requirements.&#34;);
			else
			Console.WriteLine(string.Format(&#34;Best {0} inventories in descending order of value are:&#34;,bestInventories.Count()));
			bestInventories.ForEach(o =&gt;
				Console.WriteLine(o)
				);

			var firstBestInventory = Inventory.FirstBestInventory( maxGramsWeight: 4000 );
			if (firstBestInventory == null)
				Console.WriteLine(&#34;No first best inventory found.&#34;);
			else
			{
				Console.WriteLine(&#34;Best inventory found: &#34;);
				Console.WriteLine(firstBestInventory);
			}
		}

		private static List&lt;Belonging&gt; LoadData()
		{
			var allKit = new List&lt;Belonging&gt;();
			allKit.Add(new Belonging { Number = 0, Name = &#34;map&#34;, GramsWeight = 90, Value = 150 });
			allKit.Add(new Belonging { Number = 1, Name = &#34;compass&#34;, GramsWeight = 130, Value = 35 });
			allKit.Add(new Belonging { Number = 2, Name = &#34;water&#34;, GramsWeight = 1530, Value = 300 });
			allKit.Add(new Belonging { Number = 3, Name = &#34;Gold bar&#34;, GramsWeight = 3000, Value = 130 });
			allKit.Add(new Belonging { Number = 4, Name = &#34;sandwich&#34;, GramsWeight = 500, Value = 160 });
			allKit.Add(new Belonging { Number = 5, Name = &#34;glucose&#34;, GramsWeight = 150, Value = 60 });
			allKit.Add(new Belonging { Number = 6, Name = &#34;tin&#34;, GramsWeight = 680, Value = 45 });
			allKit.Add(new Belonging { Number = 7, Name = &#34;banana&#34;, GramsWeight = 270, Value = 60 });
			allKit.Add(new Belonging { Number = 8, Name = &#34;apple&#34;, GramsWeight = 390, Value = 40 });
			allKit.Add(new Belonging { Number = 9, Name = &#34;cheese&#34;, GramsWeight = 230, Value = 30 });
			allKit.Add(new Belonging { Number = 10, Name = &#34;beer&#34;, GramsWeight = 620, Value = 10 });
			allKit.Add(new Belonging { Number = 11, Name = &#34;suntan cream&#34;, GramsWeight = 110, Value = 70 });
			allKit.Add(new Belonging { Number = 12, Name = &#34;camera&#34;, GramsWeight = 320, Value = 30 });
			allKit.Add(new Belonging { Number = 13, Name = &#34;T-shirt&#34;, GramsWeight = 240, Value = 15 });
			allKit.Add(new Belonging { Number = 14, Name = &#34;trousers&#34;, GramsWeight = 480, Value = 10 });

			return allKit;
		}
	}

&lt;h2 id=&#34;finished-let-s-give-it-a-spin:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Finished! Let&amp;rsquo;s give it a spin&amp;hellip;&lt;/h2&gt;

&lt;p&gt;We are done! Let&amp;rsquo;s see what we get if we run it all:&lt;/p&gt;
Best 5 inventories in descending order of value are:
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
camera, 320 g, valued at 30
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3960 g, Total Value: 950
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
camera, 320 g, valued at 30
---- Inventory End: Total Weight: 3720 g, Total Value: 935
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
tin, 680 g, valued at 45
banana, 270 g, valued at 60
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3930 g, Total Value: 925
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
tin, 680 g, valued at 45
banana, 270 g, valued at 60
apple, 390 g, valued at 40
suntan cream, 110 g, valued at 70
---- Inventory End: Total Weight: 3850 g, Total Value: 920
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3640 g, Total Value: 920
Best inventory found: 
---- Inventory Start:
map, 90 g, valued at 150
compass, 130 g, valued at 35
water, 1530 g, valued at 300
sandwich, 500 g, valued at 160
glucose, 150 g, valued at 60
banana, 270 g, valued at 60
apple, 390 g, valued at 40
cheese, 230 g, valued at 30
suntan cream, 110 g, valued at 70
camera, 320 g, valued at 30
T-shirt, 240 g, valued at 15
---- Inventory End: Total Weight: 3960 g, Total Value: 950


&lt;h2 id=&#34;conclusion:de8f46f1e4cc39c152533cf3328f55db&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have solved the 0/1  version of the knapsack problem, and it has been fun!
Not only is our solution well tested, but it runs fast, for small numbers of items.&lt;/p&gt;

&lt;p&gt;In subsequent articles in this series we will expand on our solution, test it for larger number of items, and hopefully tackle the
other, harder versions of this interesting computer science problem.&lt;/p&gt;

&lt;p&gt;Who knows, perhaps we&amp;rsquo;ll even take a stab at a quantum algorithm!&lt;/p&gt;

&lt;p&gt;If only we could do this type of programming all day long&amp;hellip;&lt;/p&gt;

&lt;p&gt;The full solution is &lt;a href=&#34;https://github.com/Kerneels/knapsack&#34;&gt;available here, on GitHub&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>now make it fast</title>
      <link>https://kerneels.github.io/post/now-make-it-fast/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://kerneels.github.io/post/now-make-it-fast/</guid>
      <description>

&lt;p&gt;&amp;ldquo;He began to copy one of our tables, which contained partial user information, including email IDs, hashed passwords, and last tested URL. His copy operation
    locked the database table, which raised alerts on our monitoring system. On receiving the alerts, we checked the logs, saw an unrecognized IP, and blocked
    it right away. In that time, the hacker had been able to retrieve only a
    portion of the data.&amp;rdquo;
    &amp;ndash; From the postmortem of the
    &lt;a href=&#34;http://www.browserstack.com/attack-and-downtime-on-9-November&#34;&gt;Browser Stack hack of 9th November, 2014 at 23:30 GMT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since relational database management systems (RDBMS) have been
&lt;a href=&#34;http://en.wikipedia.org/wiki/Micro_DBMS&#34;&gt;used in production environments since 1970 (Micro DBMS)&lt;/a&gt;, and the theory on which they run was
developed in the preceding decade,and perfected in the three remaining decades
of the previous century (long  ago), it was not surprising  that, as  a subject,
it received very little attention
in our curriculum - at least where I studied.&lt;/p&gt;

&lt;p&gt;To further degrade the already &lt;em&gt;apparently&lt;/em&gt; low relevance  of the poor subject, we
had to cope with a very thick and overly theoretical textbook we could not yet appreciate,
and subscribe to, if you wanted to be cool and smart, to the snooty perception
held by many peers that the lowly database was useful, but entirely boring.&lt;/p&gt;

&lt;p&gt;A database was considered the type of thing  you rigged up for a family member in Access in a
few hours. Any larger system was the domain of &amp;lsquo;informatics&amp;rsquo; - the
less inspired, more practical brother field of study to the sexy &amp;lsquo;computer science&amp;rsquo; with its focus on AI, advanced programming in C++, the  new Java and the emerging world of Linux and Open Source.&lt;/p&gt;

&lt;p&gt;Stepping outside into the real world, and the rest was history - databases
everywhere, for everything!
In my working career since 2001, every single business system I worked on had
a RDBMS - primarily MS SQL Server, and sometimes MySQL or Oracle as the persistence store.
Through the years I accumulated skill in modelling domains and
manipulating the information through SQL, but optimisation of the storage
structures for efficiency was a task I chose to ignore. I reasoned along the
lines of it is the DBA&amp;rsquo;s / RDBMS&amp;rsquo;s work, or
the mostly false assumption that computers are fast and how much data will
the system  realistically have anyways?&lt;/p&gt;

&lt;p&gt;Really? Is that professional, to say this far will I go and no further,
while you as the programmer is directly responsible for those
horribly slow queries?
Yes, the system works, some things are slow, but hey, they have a lot of data
you say, it&amp;rsquo;s going to be slow at times!
Now you are suppressing   that little voice inside of you, quietly telling you it&amp;rsquo;s wrong
aren&amp;rsquo;t you - it &lt;em&gt;can&lt;/em&gt; be faster&amp;hellip;
Think of the waiting user, the wasted time, times hundreds for internal
systems, times thousands for customers, times  hundreds of thousands or even
millions in the case of the web&amp;hellip;
All that wasted time, all that wasted energy, all the trees&amp;hellip;
Don&amp;rsquo;t think about it too much.
You feel pretty bad by now don&amp;rsquo;t you&amp;hellip;&lt;/p&gt;

&lt;p&gt;Well, I have more bad news for you. At the end of this article you are going to feel even worse, because you
are going to see how simple it is to start to turn things around.&lt;/p&gt;

&lt;p&gt;Dramatics aside, we will have a quick look at the  choices  for table
physical layout and ponder the implications on performance.&lt;/p&gt;

&lt;p&gt;Be warned that we will of course only scratch the surface of SQL performance  optimisation
on SQL Server, and that this article is aimed at software developers
with limited skill in this area, so I&amp;rsquo;m going to explain a bit
- i.e. sit down, this might
take a while.&lt;/p&gt;

&lt;p&gt;Many folks have written fine articles on this subject, and I&amp;rsquo;ll refer to
some of those as we go along.&lt;/p&gt;

&lt;p&gt;My primary justification for writing this article is that I&amp;rsquo;d like to cement
this stuff in my mind, and the best way for me is to write it all out. I hope
you can gain something from this also, and please comment if you disagree or
whatever.&lt;/p&gt;

&lt;h2 id=&#34;it-s-out-there-somewhere-but-has-it-order:810306c57b772cfaef22459bbb55c3d7&#34;&gt;It&amp;rsquo;s out there&amp;hellip; Somewhere&amp;hellip; But has it order?&lt;/h2&gt;

&lt;p&gt;Contrary to popular current belief, the data isn&amp;rsquo;t somewhere in a [cloud]
but it actually resides on one or more  physical storage media (think disk
drives), and
importantly, it&amp;rsquo;s laid down either sorted or just as it was received - for all
practical purposes, unsorted.&lt;/p&gt;

&lt;p&gt;You probably knew this already, but I needed that sentence
because it contained the word &lt;em&gt;cloud&lt;/em&gt; so that I had an excuse to quote
Stallman on cloud computing from 2008:&lt;/p&gt;
&#34;It&#39;s stupidity. It&#39;s worse than stupidity: it&#39;s a marketing hype campaign,&#34;

&lt;p&gt;Back to tables&amp;hellip; It is probably because SQL Server&amp;rsquo;s default is to choose the
physically sorted way of laying down the data, when you define a primary key on
a new table, that this table organisation
prevails and is common in most systems.
There is nothing like one size fits all and although the
ordered layout of records is a great fit most of the time, it is not the best
layout in all cases (more on that later).
However, keep in mind there is wisdom in the choice of this default none
the less.&lt;/p&gt;

&lt;h2 id=&#34;some-terminology:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Some terminology&lt;/h2&gt;

&lt;p&gt;For the purpose  of the discussion we&amp;rsquo;ll stick to the most widely used
terminology in the SQL Server world and call the ordered layout of records a
&lt;em&gt;clustered index&lt;/em&gt;, and the
layout without any physical ordering a &lt;em&gt;heap table&lt;/em&gt;, or simply a heap.
The structure  existing purely to speed up locating records  we will refer to
as a &lt;em&gt;nonclustered index&lt;/em&gt;, and how the &lt;em&gt;clustered index&lt;/em&gt;
should be ordered we will call the &lt;em&gt;clustering key&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;clustered-index-the-sorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustered index - the sorted one&lt;/h3&gt;

&lt;p&gt;The term clustered index is unfortunate, and probably the reason a
lot of people develop a blurred notion of cluster vs. non clustered, and table
vs. index.
You could think of a &lt;em&gt;clustered index&lt;/em&gt; as a
database table for maintaining data in an ordered fashion (ordered by one or more
columns) thereby giving efficient access to all the data for one record if the
values of some of the sorting columns are known.  Alternatively you could think of a &lt;em&gt;clustered index&lt;/em&gt; as an
database index to
efficiently gain access to more data,
organised by a subset of  all the columns, yet it has all that added
data in itself.
Either way, Viewed as an ordered tabular representation of data, or an index existing for
efficient access to itself - the important point is the physical sorted
layout, and that the most efficient way to retrieve records from it is via the
index that is itself given the primary key.&lt;/p&gt;

&lt;h3 id=&#34;heap-the-unsorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Heap - the unsorted one&lt;/h3&gt;

&lt;p&gt;A heap is simpler to understand.  Think of a file that grows by repeatedly appending new
lines to it without trying to maintain any kind of order.
Obviously, for efficient access to the data in a heap we need one or more
nonclustered indexes targeting the heap&amp;rsquo;s data, and built up from one or more components
of that data.&lt;/p&gt;

&lt;p&gt;When comparing clustered indexes with heaps we will assume that at least for
the primary key, a nonclustered index is defined on the heap that would make the clustered
index and heap nearly equally  efficient when retrieving a record, given a single primary key value.&lt;/p&gt;

&lt;h3 id=&#34;nonclustered-index-the-real-index:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Nonclustered index - the real index&lt;/h3&gt;

&lt;p&gt;The term &lt;em&gt;nonclustered index&lt;/em&gt; we will use to refer to the structure
that exists primarily to provide   indexed lookup to clustered indexes and heaps
alike.  In SQL Server a heap or clustered index can have many nonclustered
indexes targeting it.&lt;/p&gt;

&lt;h3 id=&#34;clustering-key-how-the-clustered-index-is-clustered-or-sorted:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustering key - how the clustered index is clustered, or sorted&lt;/h3&gt;

&lt;p&gt;Apart from the notion of a primary key, for a clustered index, the &lt;em&gt;clustering
key&lt;/em&gt; defines the subset of columns that instruct the system on how to cluster
or sort the records.
A clustering key should be chosen to be as unique  as possible, but uniqueness
is not required (unlike for the primary key),since the system will add four bytes  called the uniquifier to
the clustering key if it is not indicated as being unique already.&lt;/p&gt;

&lt;h2 id=&#34;implications-of-heap-vs-clustered-index-for-dml:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Implications of heap vs. clustered index for DML&lt;/h2&gt;

&lt;p&gt;To compare the efficiency implications of DML statements on these two broad ways of physical data layout we have to look closer
at the nature of the operations we would like to perform.&lt;/p&gt;

&lt;p&gt;It is fairly intuitive  to reason about  this if we keep in mind:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The physical layout of a heap vs. a
clustered index.&lt;/li&gt;
&lt;li&gt;Indexes should stay up to date after the operation completes.&lt;/li&gt;
&lt;li&gt;Is the operation on one or on multiple records.&lt;/li&gt;
&lt;li&gt;What piece of data is required for the fastest retrieval of records.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;the-single-index-primary-key-as-clustering-key-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;The single index, primary key as clustering key scenario.&lt;/h3&gt;

&lt;p&gt;It turns out that it is generally the best choice to use a clustered index
instead of a heap, for the scenario where you need only one index on
one or more indexing columns. For this scenario, a clustered index takes up
less space, performs better overall, and releases space better when records
get deleted.&lt;/p&gt;

&lt;p&gt;The short answer to why this is the case is that for a clustered index, the
data is the index, so lookups on the clustering key finds the relevant records
directly (after climbing the index B-tree), and alterations affecting the clustering key requires alterations to
one structure - the clustered index. For a heap plus one nonclustered index
however, lookups given the index key is a two-step process. First the nonclustered
index is queried to find the RID, the uniquely identifying key for each heap
row corresponding to the clustering key, and
then the RID is used to fetch the record from the heap.
In addition to this, although alterations involving the index key might not require much work on the heap,
the nonclustered index needs to be updated - again a two-step process.&lt;/p&gt;

&lt;p&gt;Nevertheless, there is a great case to be made for choosing a heap over a
clustered index, which we&amp;rsquo;ll get to shortly, but for the scenario as explained
above (one index on the clustering key), a clustered index is the better choice.&lt;/p&gt;

&lt;p&gt;See this &lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc917672.aspx&#34;&gt;Microsoft best practices&lt;/a&gt; white paper, but be
warned that it is proving the superiority of choosing a clustered index in
favor of a heap in a scenario
where a clustered index is the  best choice.
Do not be fooled by this article into thinking heaps are overall inferior to
clustered indexes, as a casual reading might lead you into believing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at the various DML operations quickly on clusterd index and heap&amp;hellip;&lt;/p&gt;

&lt;h4 id=&#34;insert:810306c57b772cfaef22459bbb55c3d7&#34;&gt;INSERT&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;INSERT&lt;/em&gt; into a heap is simply a case of appending where there is space
available in a data page (or creating a new page at the end). After this, a
second write operation is required - insertion of the index key into the
B-tree  of the nonclustered index.&lt;/p&gt;

&lt;p&gt;Now for a clustered index, the new record
must be inserted in the correct location. If this correct location is on a
data page that is full, that page needs to be split in two .
This might be more time-consuming than the simple insert on the heap, but for
a clustered index there is no second write operation to maintain the index -
the data and index are one. Note that this might even turn out to make a
clustered index perform better overall since it only requires one write
operation.&lt;/p&gt;

&lt;p&gt;Any additional nonclustered indexes on a clustered index or heap would take
roughly the same time to update or keep in sync with the actual data, so we
can ignore their maintenance penalty when comparing.&lt;/p&gt;

&lt;p&gt;But surely we should be able to gain performance with inserts by choosing
either of the two table layouts,   given that heaps
and clustered indexes differ so fundamentally?
In this particular scenario they are  equally good because both
options still require that indexes be kept up to date. If we were to choose a
heap and define absolutely no nonclustered index on it we will gain the fastest
insert performance since inserting would simply be adding data on anywhere
where there is space.&lt;/p&gt;

&lt;p&gt;There is actually a great use case for this: logs and other archival type of
storage that do not require immediate querying.&lt;/p&gt;

&lt;p&gt;But optimisation is a tricky problem, since even for this use case, if many
concurrent inserts are expected, it might   very well be better to choose a
clustered index instead, on some non unique but range-like clustering key that
will jump around a bit, to
achieve an effect of inserting in different locations to prevent everything
from trying to insert in the same data page all the time (as would be the
case for a heap).&lt;/p&gt;

&lt;h4 id=&#34;select-update-and-delete:810306c57b772cfaef22459bbb55c3d7&#34;&gt;SELECT, UPDATE and DELETE&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;SELECT&lt;/em&gt;, &lt;em&gt;UPDATE&lt;/em&gt; and &lt;em&gt;DELETE&lt;/em&gt; first requires locating where the records to
operate on need to be physically
found, followed by the actual action on the data.&lt;/p&gt;

&lt;p&gt;Assuming that the operation simply applies to data given a single primary key
value that is the clustering key, the finding
or first part of this operation
is slightly  less efficient for a heap compared to  a clustered index. For
a clustered index, after the tree is climbed the information is there and
ready to be retrieved, while for a heap, after climbing the tree of the
nonclustered index, you only get the RID, and then require a second
operation to (all be it directly) get at the data in the heap - one additional
level of indirection.&lt;/p&gt;

&lt;p&gt;There is an option to &lt;em&gt;include&lt;/em&gt; columns of the heap or clustered index in the
nonclustered indexes. The effect of this is that, after climbing the tree of
the nonclustered index, those &lt;em&gt;included&lt;/em&gt; column&amp;rsquo;s data is immediately
available - a mini clustered index in the form of a nonclustered index with
included columns. All very straight forward and unambiguous  wouldn&amp;rsquo;t you say?&lt;/p&gt;

&lt;p&gt;Except for &lt;em&gt;SELECT&lt;/em&gt;, the efficiency of the second part, the action part of the operation
can vary much more between heap and clustered index.
For &lt;em&gt;UPDATE&lt;/em&gt;, if the column being updated happens to be one or more of the
columns comprising the clustering key, and the table is a clustered index, then
in order to keep the data  sorted, the system might have to
do page splits.
This in turn mean that potentially, large amounts of data need to be copied around.
For a heap this is never the case, and the columns  can simply be updated in
place - order is of no importance.&lt;/p&gt;

&lt;p&gt;For both heap and clustered index, if any of the columns are part of any
defined nonclustered indexes then altering them might have nonclustered index
maintenance time as a further performance penalty.&lt;/p&gt;

&lt;p&gt;Fortunately, for both heap and clustered index, if the columns being updated are not part
of the clustering key the efficiency of the action part of the operation is nearly similar.&lt;/p&gt;

&lt;p&gt;Performing a &lt;em&gt;DELETE&lt;/em&gt; on a heap or clustered index should be simply a case of marking that record
as deleted and making the space available for potential future inserts. For a
clustered index, no index maintenance is yet again needed while for the heap,
the nonclustered index needs updating.&lt;/p&gt;

&lt;h2 id=&#34;welcome-to-the-real-world-where-tree-climbing-is-to-be-avoided-the-multiple-indexes-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Welcome to the real world, where tree climbing is to be avoided - the multiple indexes scenario&lt;/h2&gt;

&lt;p&gt;The single index, primary key as clustering key and lookup scenario described earlier
might appear  early on, and a lot in most models, but very soon you
will also want to efficiently query on other columns on wider  (more columns)
and deeper (more rows) tables.&lt;/p&gt;

&lt;p&gt;To prevent full table scans, you start adding nonclustered indexes,
and this is where heaps start to become the more attractive alternative.&lt;/p&gt;

&lt;p&gt;Suppose for a moment that your  table (let&amp;rsquo;s call it table &lt;em&gt;T&lt;/em&gt;) that became wide and deep overnight is a
clustered index, and the primary key (&lt;em&gt;K&lt;/em&gt;) is also the clustering key.&lt;/p&gt;

&lt;p&gt;Each additional nonclustered index (&lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip;) on &lt;em&gt;T&lt;/em&gt; will store in its leaf nodes the
values of &lt;em&gt;K&lt;/em&gt;.
This means that a query on &lt;em&gt;T&lt;/em&gt; utilising  some nonclustered index &lt;em&gt;N&lt;/em&gt;  results
in a tree climb of &lt;em&gt;N&lt;/em&gt; that yields some value of &lt;em&gt;K&lt;/em&gt;. Following this we
require another tree climb of the clustered index that
is &lt;em&gt;T&lt;/em&gt;, given a value for &lt;em&gt;K&lt;/em&gt;, and only then is the actual data reached.&lt;/p&gt;

&lt;p&gt;On the other hand, suppose  now that your wide and deep table  &lt;em&gt;H&lt;/em&gt; is a heap
instead, with one or more nonclustered indexes &lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip; and so on.
This time, each nonclustered index &lt;em&gt;N&lt;/em&gt; will store at the leaf node the RID
of the relevant row, and not simply  yet another key into a further index.
This means that if some query on &lt;em&gt;H&lt;/em&gt; utilise one of the nonclustered indexes,
only one tree climb of that nonclustered index is required, after which the
RID is obtained, and unlike a clustering key, a RID represents the
physical position of the record in the heap, and thus can be directly accessed
- no further tree climbing required.&lt;/p&gt;

&lt;p&gt;For a more in-depth look at this, and some hard numbers comprising a
compelling case,  do yourself a favor and read Markus
Wienand&amp;rsquo;s fine article,
&lt;a href=&#34;http://use-the-index-luke.com/blog/2014-01/unreasonable-defaults-primary-key-clustering-key&#34;&gt;Unreasonable Defaults: Primary Key as Clustering
Key&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary-and-closing-thoughts-optimising-performance-is-an-interesting-and-very-relevant-problem:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Summary and closing thoughts - Optimising performance is an  interesting and very relevant problem&lt;/h2&gt;
&#34;Premature optimisation is the root of all evil&#34;
 -- Donald E. Knuth

&lt;p&gt;As much as I concur with that statement, especially how it applies to code, I do think that a good understanding
of the options available to you when turning a data model into an actual
database schema can proactively prevent vicious      cycles of poor performing
monster database servers.
Yes, there is a lot of things one can do, and the precise case where one
technique or option would be the better option is hard to identify, but the
better your understanding of the internals, the more likely you are to get it
right first time,
and the more it will start to happen that you are writing a query and you
suddenly realise that a specific index would benefit that query tremendously.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve rambled a bit, but to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are two main choices for the physical layout of data for tables.&lt;/li&gt;
&lt;li&gt;If only one index on the primary key is required it&amp;rsquo;s probably the best to choose a clustered index.&lt;/li&gt;
&lt;li&gt;For a many index scenario choose a heap.&lt;/li&gt;
&lt;li&gt;For best insert performance on high  loads choose a heap with no nonclustered indexes.&lt;/li&gt;
&lt;li&gt;Use the include columns feature of nonclustered indexes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Today there are exciting new alternative data storage technologies like fully
in memory databases, distributed systems such  as Hadoop, Google&amp;rsquo;s BigTable
approach, and document oriented noSQL
options such as ElasticSearch to name only a very few.
These alternative solutions to the problem of working with large data sets
have and will continue to be applied more and more, but if the last decade is
anything to go by,  the relational database is going to stick around for quite
some time still, so investing time into learning how to optimise it is
time well spent.&lt;/p&gt;

&lt;p&gt;The reason for SQL systems remaining central to all serious data storage
applications  is not by accident. There is a theoretical reason, routed in the
so-called CAP theorem.
For an overview of how the CAP theorem restricted the growth and adoption of
noSQL systems, have a look at
&lt;a href=&#34;http://use-the-index-luke.com/blog/2013-04/whats-left-of-nosql&#34;&gt;What’s left of NoSQL?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An interesting response to this is FoundationDB (see
&lt;a href=&#34;http://www.theregister.co.uk/Print/2012/11/22/foundationdb_fear_of_cap_theorem/&#34;&gt;NoSQL&amp;rsquo;s CAP theorem busters: We don&amp;rsquo;t drop ACID&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;An exciting emerging trend is to harness the strengths of both the traditional
RDBMS and the more recent big data distributed, more normalised data storage
technologies. For an interesting application of this, see
&lt;a href=&#34;http://msdn.microsoft.com/en-gb/magazine/dn802606.aspx&#34;&gt;Use Updatable Tables for Responsive Real-Time Reporting&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I hope this short info burst tickled your interest enough that you will go
ahead and look into all of it a bit more.
Personally I have been pleasantly surprised by the depth of this subject area.&lt;/p&gt;

&lt;p&gt;I can highly recommend the book
&lt;a href=&#34;http://sql-performance-explained.com/?utm_source=UTIL&amp;amp;utm_medium=main&amp;amp;utm_campaign=second&#34;&gt;SQL Performance Optimisation&lt;/a&gt; for an in-depth look at this
subject, and the
&lt;a href=&#34;http://use-the-index-luke.com/&#34;&gt;Use The Index Luke&lt;/a&gt; site.&lt;/p&gt;

&lt;p&gt;This article also appears on &lt;a href=&#34;http://www.inivit.com/blog/&#34;&gt;Inivit&amp;rsquo;s blog&lt;/a&gt; along with some other fine posts from former colleagues.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>