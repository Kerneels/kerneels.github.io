<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>T Sql on Good Fast</title>
    <link>https://blog.goodfast.info/categories/t-sql/</link>
    <description>Recent content in T Sql on Good Fast</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Sep 2017 08:31:17 +0200</lastBuildDate>
    <atom:link href="https://blog.goodfast.info/categories/t-sql/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to use the transitive closure over a set of relations for fast path finding in SQL</title>
      <link>https://blog.goodfast.info/post/transitive-closures/</link>
      <pubDate>Tue, 26 Sep 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/transitive-closures/</guid>
      <description>

&lt;p&gt;In a &lt;a href=&#34;http://goodfast.info/post/speed-up-views-through-custom-materialization/&#34;&gt;previous post&lt;/a&gt;, I wrote about how we make sense of the world by modelling relationships between things as tree-like hierarchies.
This time we will add to this hierarchical data structure, a representation derived by calculating all possible paths.
This set of paths is referred to as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Transitive_closurejj&#34;&gt;transitive closure&lt;/a&gt;, and can be thought of as the set of all paths if you start at each node in the tree.&lt;/p&gt;

&lt;p&gt;I wish I could tell you that it is as simple as Mr. Eby in &lt;a href=&#34;http://dirtsimple.org/2010/11/simplest-way-to-do-tree-based-queries.html&#34;&gt;this article&lt;/a&gt; makes it out to be, but
alas, when I got right down implementing a full solution, things got quite involved.
It tends to be like that.
None the less, credit where credit is due; go read that article first I can highly recommend it, and then come back here for more!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve developed the code for SQL Server, so it is immediately T-SQL compatible, but you can surely alter it for any decent database.&lt;/p&gt;

&lt;h2 id=&#34;background:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;Trees and hierarchies in general can get quite complicated, so why would we choose to add to this complexity yet another data structure?
The reason is so that we can apply &lt;a href=&#34;https://en.wikipedia.org/wiki/Memoization&#34;&gt;memoization&lt;/a&gt;; we buy time with space.
At the cost of the time to compute the transitive closure once, and the cost of the space required to persist it, we gain the time it would have taken to calculate it each time it is needed.&lt;/p&gt;

&lt;p&gt;Previously I wrote about how one can go about to materialize an entire complex and expensive view.
The use of the transitive closure can also be thought of as a kind of materialization, but it is far smarter and promises to be more useful.&lt;/p&gt;

&lt;p&gt;The [transitive] part in the name refers to a property that a relation can exhibit.
Since you are related to your father, and your child is related to you, your child is also related to your father.
We can say that the inheritance relation is transitive.
Since 9 &amp;gt; 5, and 5 &amp;gt; 3, it is also true that 9 &amp;gt; 5 &amp;gt; 3 and 9 &amp;gt; 3;
the &amp;ldquo;greater than&amp;rdquo; relation is transitive.&lt;/p&gt;

&lt;h2 id=&#34;representing-the-hierarchy:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Representing the hierarchy&lt;/h2&gt;

&lt;p&gt;The hierarchy we will work with is a simple one:&lt;/p&gt;


&lt;img srcset=&#34;../../ct_tree.svg&#34; src=&#34;../../ct_tree.png&#34; alt=&#34;Diagram of the tree with node a at root, nodes b and c below it, below node b is node d and e, below node c is node f, below node d is node g.&#34;&gt; 



&lt;p&gt;The usual way to represent such a hierarchy in a table is through self referencing records:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;id&lt;/th&gt;
&lt;th&gt;parent_id&lt;/th&gt;
&lt;th&gt;label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;lsquo;a&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;lsquo;b&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;lsquo;c&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&amp;rsquo;d&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&amp;lsquo;e&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&amp;lsquo;f&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&amp;lsquo;g&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Such self referencing records in a single table makes manipulation of the hierarchy very simple.
For example, to move the sub tree rooted in node 2 and make it fall under node 6, we simply update the parent_id of node 2 to reference node 6.
This simplicity, however, comes at a cost.
When you want to traverse the hierarchy, you require iteration or recursion which is generally expensive.
This is especially so if all that you are after is only a portion of the tree, or worse, only the path from the root to a particular intermediate or leaf node.&lt;/p&gt;

&lt;p&gt;Suppose you want to find out the path from  node &amp;lsquo;g&amp;rsquo; to the root.
After finding the entry for node &amp;lsquo;g&amp;rsquo;, you have to repeatedly find the parent until there is no more parent.
Suppose you want to get the path from the root for each node in the tree.
The database has to do this process for every node.&lt;/p&gt;

&lt;p&gt;A transitive closure over all the relations in the base table gives you a ready-made set of paths which you can index and query, just like any other set of records.
No more need for recursive CTE&amp;rsquo;s each time you want path information, or worse still, multiple queries!&lt;/p&gt;

&lt;h2 id=&#34;setup:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Setup&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;
-- create table node to represent the relations
create table node (id int, parent_id int, label varchar(50));

-- load noad
insert into node values 
(1,0,&#39;a&#39;),
(2,1,&#39;b&#39;),
(3,1,&#39;c&#39;),
(4,2,&#39;d&#39;),
(5,2,&#39;e&#39;),
(6,3,&#39;f&#39;),
(7,4,&#39;g&#39;);

-- table to hold the transitive closure over nodes
create table closure (parent_id int, child_id int, depth int, route varchar(100));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Table node is the standard self referencing structure, and table closure will contain the paths that node represent.
Column node.label is only the label that applies to the particular node row, but column closure.route will contain a nice chain of all the labels from closure.parent_id to closure.child_id.
In a database that supports arrays, such as PostgreSQL, we can even go so far as to store the actual id values of the whole path.
In column closure.depth we want to store how many hops it takes to go from the node at parent_id to the node at child_id.&lt;/p&gt;

&lt;p&gt;What we want to achieve is to calculate all possible paths in the tree and represent them like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;parent_id&lt;/th&gt;
&lt;th&gt;child_id&lt;/th&gt;
&lt;th&gt;depth&lt;/th&gt;
&lt;th&gt;route&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a &amp;gt; b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a &amp;gt; c&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;c &amp;gt; f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; c &amp;gt; f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b &amp;gt; d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b &amp;gt; e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;b &amp;gt; d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Notice the identity paths, or paths starting and ending in the same node, with depth of zero.
Later on it will become apparent why we require these.&lt;/p&gt;

&lt;h3 id=&#34;closure-insert:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure insert&lt;/h3&gt;

&lt;p&gt;A new entry in table node can only be one of the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;a new, additional root - there exists no parent for it&lt;/li&gt;
&lt;li&gt;a new, additional child - there exists a parent for it&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For either type of new node, table closure requires a new identity path.
Furthermore, if the new node has a parent, we also need to add entries for all paths ending in the new node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_insert @parent_id int, @child_id int, @route varchar(50) as 
begin
-- always insert identity
insert into closure (parent_id,child_id,depth,route) 
values (@child_id,@child_id,0, @route);
if  @parent_id &amp;lt;&amp;gt; @child_id and @parent_id is not null
insert into closure (parent_id,child_id,depth, route) 
select parent.parent_id, child.child_id,
parent.depth + child.depth + 1, 
parent.route + &#39; &amp;gt; &#39; + child.route 
from closure as parent cross join closure as child 
where parent.child_id = @parent_id and child.parent_id = @child_id;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;closure-delete:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure delete&lt;/h3&gt;

&lt;p&gt;Deleting a relation (suppose it is the node with id @child_id)from the node table requires us to delete all paths from the closure table that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;start out from @child_id,&lt;/li&gt;
&lt;li&gt;end in @child_id,&lt;/li&gt;
&lt;li&gt;runs through @child_id, so any path that starts at any of @child_id&amp;rsquo;s parents,and any path that ends in any of @child_id&amp;rsquo;s children&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_delete @child_id int as begin
delete from link  
from closure as parent, closure as link, closure as child, closure as to_delete 
where 
parent.parent_id = link.parent_id and child.child_id = link.child_id
and parent.child_id = to_delete.parent_id and child.parent_id = to_delete.child_id
and (to_delete.child_id = @child_id or to_delete.parent_id = @child_id) 
and to_delete.depth &amp;lt; 2;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;closure-update:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure update&lt;/h3&gt;

&lt;p&gt;Similar to the delete operation, if a node changes, all paths that start from, or end in, or go through that node needs to be updated since the label of the node might have changed and thus the routes need to be built again.
If a node moved; if it is now a sub node of another node then all paths starting from, ending in and going through the node that moved needs to be deleted, and new paths added.&lt;/p&gt;

&lt;p&gt;To simplify matters we are going to delete all such starting from, ending in, and going through paths, and re-insert the individual nodes again in the correct order.
Before we delete all the paths we first will temporarily store all the nodes we will be re-inserting afresh after the delete.
Then we will do the delete, followed by successively calling the insert proc to insert everything again.&lt;/p&gt;

&lt;p&gt;But what if we insert a node (think add all paths involving this node) before we inserted the node&amp;rsquo;s parent(s)?
If you take a look at our insert proc you&amp;rsquo;ll see very quickly that this will result in problems; we will not find the parent paths, and thus add too few actual paths.
For this reason it is essential that we first insert all nodes without parents, then followed by the rest of the set that do have a parent, which will already be inserted.
The insert proc will take care of adding all required paths, but only if we insert in the correct order.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_update @child_id int as begin
-- temp storage of nodes to insert after the deletion
declare @t as table (id int primary key, parent_id int, label varchar(50));

insert into @t (id,parent_id,label) 
select link.child_id,n.parent_id,n.label 
from closure as link join node as n on n.id = link.child_id
where  link.parent_id = @child_id;

delete from link 
from closure as link join @t as t
on link.child_id = t.id or link.child_id = @child_id;

-- repeatedly call the insert proc in the correct order, 
-- which is ensured by the recursive CTE over the set of nodes to insert
declare @_p int, @_c int, @_l varchar(50);
declare cur cursor fast_forward for 
with  to_insert as (
	select parent_id, id, label from @t
),  to_insert_ordered  as (
-- the anchor for the recursion
	select ti.parent_id, ti.id, ti.label 
	from to_insert as ti
	where ti.parent_id = 0
	or ti.id = @child_id
	or ti.parent_id not in (
		select id from to_insert
	)
	union all
	select ti.parent_id, ti.id, ti.label 
	from to_insert_ordered as tio
	join to_insert as ti
	on ti.parent_id = tio.id
) select parent_id, id, label from to_insert_ordered;
open cur;
fetch next from cur into @_p, @_c, @_l;
while @@FETCH_STATUS = 0 begin
	exec closure_insert @_p,@_c,@_l;
fetch next from cur into @_p, @_c, @_l;
end close cur deallocate cur;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The insertion cursor query follows the usual pattern for a recursive CTE:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;establish the data to recurse over: to_insert&lt;/li&gt;
&lt;li&gt;establish the anchor, or starting point: to_insert_ordered&lt;/li&gt;
&lt;li&gt;union with a join onto itself and the whole list to_insert&lt;/li&gt;
&lt;li&gt;select the result&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;closure-refresh:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure refresh&lt;/h3&gt;

&lt;p&gt;To do the initial load, and in case something goes wrong, we add one more proc; a full refresh proc, which is simply successive calls to the insert proc, but in the correct order as described in the previous section.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_refresh as begin
truncate table closure;

declare @p int, @c int, @l varchar(50);
declare cur cursor fast_forward for with  to_insert as (
	select parent_id, id, label from node
),  to_insert_ordered  as (
	select ti.parent_id, ti.id, ti.label 
	from to_insert as ti
	where ti.parent_id = 0 or
	ti.parent_id not in (
		select id from to_insert
	)
	union all
	select ti.parent_id, ti.id, ti.label 
	from to_insert_ordered as tio
	join to_insert as ti
	on ti.parent_id = tio.id
) select parent_id, id, label from to_insert_ordered;
open cur;
fetch next from cur into @p, @c, @l;
while @@FETCH_STATUS = 0 begin
	exec closure_insert @p,@c,@l;
fetch next from cur into @p, @c, @l;
end close cur deallocate cur;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;Here are some sample queries which you can use to test your solution. I&amp;rsquo;ve done it, not going to paste 50 tables of output here, try all sorts and see what happens:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;-- start clean
exec closure_refresh;
-- check that everything is in order
select * from closure;
-- relabel node 2
update node set label = &#39;z&#39; where id = 2;
-- do what the after update trigger will do:
exec closure_update 2;
-- not only node 2&#39;s identity entry, 
-- but all paths involving it should now say &#39;z&#39; instead of &#39;b&#39;
select * from closure;
-- fix node 2 again:
update node set label = &#39;b&#39; where id = 2;
exec closure_update 2;
-- things should be back to what it was initially...
select * from closure;
-- move node 2, or &#39;b&#39; under node 6
update node set parent_id = 6 where id = 2;
exec closure_update 2;
-- node 2 should now be under node 6
select * from closure;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;go-forth-and-conquer:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Go forth and conquer!&lt;/h2&gt;

&lt;p&gt;Armed with the procedures we developed thus far, we can proceed to hook them up with after triggers on the node table.
When the node table change, the after triggers fire and the closure table stays up to date.&lt;/p&gt;

&lt;p&gt;When we want to answer questions such as, does there exist a path from X to Y, what is the longest path from X to Y (for obtaining the full path from the root), and many more such queries, we can simply perform fast selects against the closure table.
We can index parent_id, child_id, and (parent_id,child_id) and so on in order to speed things up.
We can create a few custom views for quickly determining all the paths from the roots to the individual intermediate and leaf nodes.&lt;/p&gt;

&lt;p&gt;The recursive CTE that is needed for the update procedure is unfortunate, but luckily it only operates on the set of nodes that need to change.
Unless you are altering root nodes all the time, this will generally be limited to a small number, and not the entire set of data as is the case in a full materialization of the whole tree.
This means that the maintenance overhead will generally be far less.&lt;/p&gt;

&lt;p&gt;This is only a proof of concept, but you can go forth from here and use it as a reference to implement more complex and advanced transitive closures.
I hope it helps you maintain performance on queries on hierarchies, and I hope you&amp;rsquo;ve learned something new along the way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speed up slow views through custom materialization</title>
      <link>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</link>
      <pubDate>Thu, 08 Jun 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</guid>
      <description>

&lt;p&gt;SQL views are aluring as a means of abstraction; a &amp;ldquo;building block&amp;rdquo; to hide away commonly used complexity.
It is no wonder then that us developers will try them out, and before you know it, your clever recursive CTE view on that hierarchy is used everywhere, by everyone, but how is it affecting overall database performance&amp;hellip;&lt;/p&gt;

&lt;p&gt;They look like tables, can be joined on, selected from, and in some cases even updated just like tables, yet the reality is that they are not like tables.
So, you cannot consider a view to be a type of stored procedure, and you can also not consider a view to be a type of table or index; it is something in between.&lt;/p&gt;

&lt;p&gt;It is possible for the query planner to &amp;ldquo;reach into&amp;rdquo; a view, and discover which indexes to use in order to access information in the best way, but this quikcly breaks down once you perform any kind of complicated thing, such as a CTE, UNION statement, or anything else that breaks up the link from the source tables to the result set of the view.
When exactly you break this ability of the query planner to use appropriate indexes is a great idea for a future post - I have not found anything that directly states this as of yet.
Intuitivly  it makes sense that some kinds of data mangling will just make it impossible for the query planner to find indexes to use.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Note that it&#39;s of course always best to first inspect the query plan before concluding that a fiew is or is not making use of a particular index. I have made the mistake before of making grand statements on how poor the query planner is at choosing an index when dealing with a view, only to be shown that it in fact can do a bit more than what you might expect!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My goal in this post is simply to make you aware of the possability that complex views might be causing your database to perform sub optimally, and then to offer an in place, zero downtime solution to the problem.&lt;/p&gt;

&lt;h3 id=&#34;when-to-use-views:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;When to use views&lt;/h3&gt;

&lt;p&gt;The way I currently understand it,  you should use views when you want different &lt;em&gt;views&lt;/em&gt; on the same table, or simple connected set of tables; i.e. you want to include/exclude certain columns/rows, so in other words, as a means of information hiding, a means of performing restricted access to the information in the underlying tables; a different take on the same data.
It is debatable how many new systems are developed, that would choose to deligate security, access restriction type of functionality to the database, but there is a fair chance that it is happening out in the wild, since a recent SQL Server feature is row-level access, and data masking.&lt;/p&gt;

&lt;h3 id=&#34;discovering-the-problem:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Discovering the problem&lt;/h3&gt;

&lt;p&gt;It was while I was performance tuning a very busy Azure Database, that I discovered a collection of particularly slow executing queries, spending most of their time in CPU.
The data volume involved could not account for the poor performance, being in the mere tens of thousands of small rows.
As far as I could determine, most of the appropriate indexes existed that would normally make things perform acceptably.
Something else was up&amp;hellip;&lt;/p&gt;

&lt;p&gt;Turning to the query plans, a pattern started emerging; slow, very slow views were joined on.
The views themselves were not very complex, but they did something interesting: they were recursive CTEs designed to traverse
a hierarchy, essentially a tree structure, and produce a full fan out of the entire tree.&lt;/p&gt;

&lt;h3 id=&#34;solution:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Solution&lt;/h3&gt;

&lt;p&gt;My first inclination was to have SQL Server materialize these views for me. Materialized (or indexed) views is an &lt;a href=&#34;http://sqlmag.com/database-performance-tuning/introducing-indexed-views&#34;&gt;old feature of the server&lt;/a&gt;, dating back to SQL Server 2000 if I&amp;rsquo;m not mistaking, so surely in 2017 this should be completely possible.
Well, it turns out that in order for a view to be materialized, &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/relational-databases/views/create-indexed-views&#34;&gt;a whole list of requirements&lt;/a&gt; need to be satisfied. For example, something as innocently looking as a LEFT JOIN in the view query would put a quick end to this solution path.&lt;br /&gt;
Researching it a bit further shed some light on why all these restrictions apply, but although it does make you be a bit more understanding, it still feels like this is something that should be possible, no matter how complex the view is.&lt;/p&gt;

&lt;p&gt;Completely redesigning the underlying hierarchical representation, with something like transative closures was not really an option, so the next best idea was to custom materialize these views.
The data access characteristics of the hierarchy and supporting tables was that they did not change all that often, yet they were queried all the time.
This was great news, since it meant that even if the materialization process took a bit of time, this would quickly be compensated for by the much, much faster query times.
Having the previously computed data now reside in a proper table also meant that it could be appropriately indexed, clustered, and even partitioned (although the volume was far too low for this need).&lt;/p&gt;

&lt;h4 id=&#34;the-procedure:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;The procedure&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Create a table (we&amp;rsquo;ll call it the Working) that structurally mirrors the result of querying the view (the View, later to be renamed to the Origin).&lt;/li&gt;
&lt;li&gt;Create a stored procedure (RefreshWorking) that will make use of Origin to refresh Working.&lt;/li&gt;
&lt;li&gt;Create AFTER triggers for all tables referenced by Origin, that will call RefreshWorking.&lt;/li&gt;
&lt;li&gt;Make the triggers intelligent in that they will only call RefreshWorking when the DML operation of the source table would actually affect the outcome of the Origin view.&lt;/li&gt;
&lt;li&gt;Optionally pass the source table name and the key values, through a table valued parameter to RefreshWorking, so that the procedure can more intelligently pick out which parts of Working will need refreshing.&lt;/li&gt;
&lt;li&gt;Create a view, CheckWorkingAndOrigin, that FULL JOIN view Origin and table Working, to ensure that they are identical.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once all of this is done, it is time to test.
For your testing I can highly recommend &lt;a href=&#34;http://tsqlt.org/&#34;&gt;tSQLt&lt;/a&gt;; a completely T-SQL based unit testing system.
When you have assured yourself that RefreshWorking properly updates table Working, it is time for the deployment.&lt;/p&gt;

&lt;p&gt;In one transaction, rename  the original, slow view to Origin, create a synonym with the same name as the original slow view, and point the synonym at the Working table.
As the last step of the transaction, run RefreshWorking procedure so that the Working table will get properly updated and be primed for showtime.&lt;/p&gt;

&lt;h3 id=&#34;reward:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Reward&lt;/h3&gt;

&lt;p&gt;After we implemented this procedure on a heavily queried complex view, we saw a query plan simplification going from
over 70 steps, to only 3 steps. More impressive is that the plan now took 481 times less CPU time!
The RefreshWorking procedure still called the original, slow, complex view, but it did this only when the source tables changed and in particular ways.
The procedure also minimized writes to the Working table, to prevent table locking for the heavy reading on it.&lt;/p&gt;

&lt;h3 id=&#34;summary:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Summary&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;One way we make sense of the world is by modelling relationships between things as a tree-like hierarchy.&lt;/li&gt;
&lt;li&gt;Most database systems, however, are essentially flat when it comes to the most basic collection of storage; the table.&lt;/li&gt;
&lt;li&gt;We overcome the flatness in one particular way, the simpler  of possible ways, by representing the unbounded nested characteristic of hierarchies by self referencing records in a table.&lt;/li&gt;
&lt;li&gt;Self referencing records in a single table makes manipulation of the hierarchy a very simple operation, but this simplicity comes at a cost when you want to traverse the hierarchy.&lt;/li&gt;
&lt;li&gt;For traversing arbitrarily  deep hierarchies represented by self referencing records, you inevitably require recursion.&lt;/li&gt;
&lt;li&gt;Recursive CTEs break the essential link between the source tables and the view result set, making it impossible for the query planner to do anything but perform the entire process of the view&amp;rsquo;s query, even when you only desire a small subset.&lt;/li&gt;
&lt;li&gt;In the scenario where the source tables for the complex view are written to less than they are read from, you can optimize the complex view by materializing it into a concrete table.&lt;/li&gt;
&lt;li&gt;The materialized table can then be properly indexed for maximum query performance, at a relatively small index maintenance cost at write time.&lt;/li&gt;
&lt;li&gt;With this approach you are trading computation time for storage space.&lt;/li&gt;
&lt;li&gt;The writing of the materialized table happens on DML AFTER triggers, so that you first have the change written to the source tables before the materialized table is updated.&lt;/li&gt;
&lt;li&gt;Updating of the materialized table need not be a complete rewrite; the AFTER triggers can be programmed so that they only fire when columns that partake in the SELECT list for the origin view query change.&lt;/li&gt;
&lt;li&gt;A further optimization can be made where by the refresh stored procedure recieves a list of keys of rows that changed, and can then use this info to only update the materialized table where it actually needs to change.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope that you will try out this procedure on slow views on your databases; it has really helped us a lot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CTE : simplify those nested sub queries</title>
      <link>https://blog.goodfast.info/post/cte-sub-queries-simplified/</link>
      <pubDate>Sun, 09 Oct 2016 21:26:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/cte-sub-queries-simplified/</guid>
      <description>

&lt;p&gt;This article examines how sub queries can be substituted for the far more readable common table expressions, or CTEs available in many RDBMS systems.
I was motivated to write this article when a friend who is fairly new to SQL expressed difficulty in grasping queries containing nested sub queries.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;ve never heard of CTEs before and you want to get the most out of this article, I recommend you get &lt;a href=&#34;https://msftdbprodsamples.codeplex.com/releases/view/125550&#34;&gt;AdventureWorks2014&lt;/a&gt; sample database and experiment a little with the queries below.
Adventureworks is a nice sample database designed to demonstrate SQL Server features, and many examples online makes use of it.
If on the other hand CTEs are old hat to you a quick skim reading of the below will suffice, but pay attention since there is something
interesting below which you might never have known, or maybe forgotten.&lt;/p&gt;

&lt;h2 id=&#34;is-it-just-me-or-is-this-strange:0dc054d968169eec76f7161b9d98121b&#34;&gt;Is it just me, or is this strange?&lt;/h2&gt;

&lt;p&gt;No-no, this thing came from long, long ago, was what I was thinking
the first time I was confronted with SQL SELECT syntax.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s all the wrong way round.
Take the most basic form of  a SELECT statement for example&amp;hellip;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You start out with what you want in the column list while starting the SELECT statement.&lt;/li&gt;
&lt;li&gt;Next you proceed to say from where you want the data  to come from, in the FROM clause.&lt;/li&gt;
&lt;li&gt;Then you stipulate  how  you want to filter what is returned when you write the WHERE clause.&lt;/li&gt;
&lt;li&gt;Finally  you end off with how stuff should be ordered in the ORDER BY clause.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, I do realise that there are (were) probably very good reasons why this particular order was chosen, but
would it not make more sense to rather start out with saying where you want to draw info from (FROM clause), proceed to filter that with the WHERE clause, followed by your preferred ordering in the ORDER BY, and finishing up with the list of which columns you are interested in, e.g:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;FROM&lt;/li&gt;
&lt;li&gt;WHERE&lt;/li&gt;
&lt;li&gt;ORDER BY&lt;/li&gt;
&lt;li&gt;SELECT&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Well, it is really of no consequence now - we have come too far along this road, however, note that the fluent way of constructing queries
with .NET C# Linq offer exactly this; a more logical flow of steps.&lt;/p&gt;

&lt;p&gt;This post is about simplifying subqueries, so getting back on track, it turns out there is a powerful feature of SQL99 called
Common Table Expressions, or CTEs which offers a great help when you are dealing with a monster query with little minions of subqueries, nested deeper in there than  your TV remote that sunk into the couch folds.&lt;/p&gt;

&lt;p&gt;Whereas sub query syntax quickly gets confusing and convoluted, using the CTE approach breaks down complexity and deals with each step of the workload in separate parts before moving on to
constructing a more intertwined picture.&lt;/p&gt;

&lt;p&gt;To me, it somewhat moves towards my &amp;ldquo;more ideal&amp;rdquo; ordering of statements, all be it no real alteration to the standard SELECT syntax.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at an example.&lt;/p&gt;

&lt;h2 id=&#34;adventureworks-customers-and-their-addresses:0dc054d968169eec76f7161b9d98121b&#34;&gt;Adventureworks customers and their addresses&lt;/h2&gt;

&lt;p&gt;In Adventureworks, customers are modeled in the person.BusinessEntity table, and their addresses in the person.Address table.
The relation between customers and their addresses is maintained in the person.BusinessEntityAddress table, which adds a type
for each address (Billing, Home, Main Office etc).&lt;/p&gt;

&lt;h2 id=&#34;we-want-to-know-how-many-customers-have-one-two-and-three-or-more-addresses:0dc054d968169eec76f7161b9d98121b&#34;&gt;We want to know how many customers have one, two and three or more addresses.&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Since the purpose of this article is to discuss subquery vs CTEs, excuse the somewhat contrived query.
At the end of this article I&#39;ll show  a simpler way to accomplish the tast at hand.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sub-query-building-block:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query building block&lt;/h2&gt;

&lt;p&gt;We can establish that no BusinessEntity has more than two addresses by the empty result set returned by:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select ba.BusinessEntityId, count(ba.AddressId)
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &amp;gt; 2;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll assume for the remainder of this article that  this query is our best attempt, and that
it is a &amp;ldquo;black box&amp;rdquo; with which we cannot tamper, save for adjusting the count in the HAVING clause.&lt;/p&gt;

&lt;p&gt;Note that technically we do not need the additional join:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;however this ensures we do not consider addresses in isolation - we ensure all addresses actually have an existing business tied to them.&lt;/p&gt;

&lt;h2 id=&#34;sub-query-with-count-of-zero-instead-of-nothing:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query with count of zero instead of nothing&lt;/h2&gt;

&lt;p&gt;Now we would like to improve this result a bit, by rather returning a count of zero instead of nothing.
We do this by wrapping the above query in a super query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select 
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &amp;gt; 2
) as businessEntitiesWithMoreThanTwoAddresses;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not too bad in terms of readability, and since we alias the sub query we always know where the result for the outer select column list comes from.
But what if we would like to augment this query to also return the number of businesses with only one, and two addresses respectively?
Or to cover all possabilities, the number of businesses with one, two and three or more addresses respectively.
Let&amp;rsquo;s try with the sub query approach:&lt;/p&gt;

&lt;h2 id=&#34;combined-sub-query-approach:0dc054d968169eec76f7161b9d98121b&#34;&gt;Combined sub query approach&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress,
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses,
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &amp;gt; 2
) as businessEntitiesWithMoreThanTwoAddresses;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hmm&amp;hellip; We get zero for everything. Something has gone wrong.
Running each super/sub query pair separately we obtain a number for the one and two address scenario, but nothing for the three or more one.
Somehow, the zero result from the three or more address scenario is forcing the other two to become zero as well.
Well, where else would two hole numbers and zero uniformly combined yeald zero?
Multiplication of course, and observing how we unwittingly join the three subqueries with a comma (&amp;ldquo;,&amp;rdquo;) which is nothing but a
cartesian or cross join, everything start to make sense!
The cross join is causing our result set to first be the number of rows returned by the first subquery,  but then these rows are &amp;ldquo;multiplied&amp;rdquo; in the first cross join with the two address subquery, at which point we are already off track, and finally, to make matters even
worse, the last subquery returns zero rows, and anything multiplied by zero is of course again zero.&lt;/p&gt;

&lt;h2 id=&#34;fixup-by-throwing-more-sub-queries-at-the-problem:0dc054d968169eec76f7161b9d98121b&#34;&gt;Fixup by throwing more sub queries at the problem&lt;/h2&gt;

&lt;p&gt;Now, let&amp;rsquo;s first remedy this situation by cross joining on only one row each time.
Hold on, it is going to get a bit messy since in order to do this, we have to introduce yet another subquery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select 
A.NumberOfBusinessesWithOneAddress,
B.NumberOfBusinessesWithTwoAddresses,
C.NumberOfBusinessesWithMoreThanTwoAddresses
from
(select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress) as A,
(select
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses) as B,
(select
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &amp;gt; 2
) as businessEntitiesWithMoreThanTwoAddresses) as C;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Success, but at quite a complexity and readability cost.&lt;/p&gt;

&lt;h2 id=&#34;first-cte-attempt:0dc054d968169eec76f7161b9d98121b&#34;&gt;First CTE attempt&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s see if we can simplify this into something more readable by using a few CTEs instead of so many subqueries.&lt;/p&gt;

&lt;p&gt;Our query will consist of 3 parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First, the three innermost sub queries are stated as CTEs named businessEntitiesWithOneAddress, businessEntitiesWithTwoAddresses and businessEntitiesWithMoreThanTwoAddresses.&lt;/li&gt;
&lt;li&gt;Next, the encapsulating super queries that reduce the sub queries to  single values are also stated as CTEs named A, B and C.&lt;/li&gt;
&lt;li&gt;Finally we simply select from the aggregation CTEs A, B and C.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- First, the three innermost sub queries are stated as CTEs named businessEntitiesWithOneAddress, businessEntitiesWithTwoAddresses and businessEntitiesWithMoreThanTwoAddresses.
with  businessEntitiesWithOneAddress as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
), businessEntitiesWithTwoAddresses as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
), businessEntitiesWithMoreThanTwoAddresses  as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &amp;gt; 2
), 

-- Next, the encapsulating super queries that reduce the sub queries to  single values are also stated as CTEs named A, B and C.
A as
(select count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress
from businessEntitiesWithOneAddress
), B as
(select count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses
from businessEntitiesWithTwoAddresses
), C as
(select count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from businessEntitiesWithMoreThanTwoAddresses
)

-- Finally we simply select from the aggregation CTEs A, B and C.
select
A.NumberOfBusinessesWithOneAddress,
B.NumberOfBusinessesWithTwoAddresses,
C.NumberOfBusinessesWithMoreThanTwoAddresses
from A, B, C;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;is-this-not-so-much-more-readable:0dc054d968169eec76f7161b9d98121b&#34;&gt;Is this not so much more readable?&lt;/h2&gt;

&lt;p&gt;This last query (with CTEs only) is functionally equivalent   to the full query with only sub queries, yet, personally I find the CTE-based one
significnatly more readable and understandable.
For me there are many reasons for this, but I list a few:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instead of a nested style, the CTEs flow in a top to bottom manner.&lt;/li&gt;
&lt;li&gt;Each similar set of queries are grouped together.&lt;/li&gt;
&lt;li&gt;The CTEs are named upfront, so you have an idea of what is being done right from the start.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;cte-support:0dc054d968169eec76f7161b9d98121b&#34;&gt;CTE support&lt;/h2&gt;

&lt;p&gt;CTE is well supported among database systems, and I was surprised to learn that even SqLite supports it!&lt;/p&gt;

&lt;p&gt;Wikipedia says:&lt;/p&gt;

&lt;p&gt;Recursive CTEs are also supported by&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Microsoft SQL Server,&lt;/li&gt;
&lt;li&gt;Firebird 2.1,&lt;/li&gt;
&lt;li&gt;PostgreSQL 8.4+,&lt;/li&gt;
&lt;li&gt;SQLite 3.8.3+,&lt;/li&gt;
&lt;li&gt;Oracle 11g Release 2,&lt;/li&gt;
&lt;li&gt;IBM Informix version 11.50+ and&lt;/li&gt;
&lt;li&gt;CUBRID.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CTE support has been lacking from MySQL for some time now, however, derived tables can be used successfully to achieve standard or non recursive CTE functionality.
The good news is that &lt;a href=&#34;http://mysqlserverteam.com/mysql-8-0-labs-recursive-common-table-expressions-in-mysql-ctes/&#34;&gt;full featured CTE support, including recursiveness&lt;/a&gt; is not far off for MySQL.&lt;/p&gt;

&lt;h2 id=&#34;but-can-we-not-just-join-better:0dc054d968169eec76f7161b9d98121b&#34;&gt;But, can we not just join better?&lt;/h2&gt;

&lt;p&gt;Now that I&amp;rsquo;ve shown how the use of CTEs can eliminate sub queries, lets briefly return to the problem of the cartesian join.
I really did not like the additional sub query, and it&amp;rsquo;s additional CTE counterpart, for reducing the result sets into something that could
cross join without problems (those A, B and C).
Is there not perhaps a way to join without throwing anything away?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider the &lt;a href=&#34;http://sqlmag.com/t-sql/t-sql-join-types&#34;&gt;possible join types&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;CROSS JOIN :  We already saw this does not work.&lt;/li&gt;
&lt;li&gt;INNER JOIN : This will also not work since per definition our three sets are disjoint / contain no similar items.&lt;/li&gt;
&lt;li&gt;LEFT, RIGHT JOIN : We have the same problem; our sets are disjoint.&lt;/li&gt;
&lt;li&gt;FULL JOIN : Bingo! Full joins attempt to join on the join condition, but thereafter behaves like a LEFT and RIGHT JOIN combined; nothing is lost, and unmatched rows are still returned.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Did you know about the full join? I will admit that although I knew there had to be something like it, I did not know / forgotten all about it.&lt;/p&gt;

&lt;h2 id=&#34;sub-query-approach-simplified-with-a-full-join:0dc054d968169eec76f7161b9d98121b&#34;&gt;Sub query approach simplified with a FULL JOIN&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
) as businessEntitiesWithOneAddress 
full join
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
) as businessEntitiesWithTwoAddresses 
on
businessEntitiesWithOneAddress.BusinessEntityId = 
businessEntitiesWithTwoAddresses.BusinessEntityId  
full join
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &amp;gt; 2
) as businessEntitiesWithMoreThanTwoAddresses
on
businessEntitiesWithTwoAddresses.BusinessEntityId   = 
businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yes, we do away with one level of sub queries, but it&amp;rsquo;s still not great.&lt;/p&gt;

&lt;h2 id=&#34;cte-approach-simplified-with-full-join:0dc054d968169eec76f7161b9d98121b&#34;&gt;CTE approach simplified with FULL JOIN&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;with  businessEntitiesWithOneAddress as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 1
), businessEntitiesWithTwoAddresses as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) = 2
), businessEntitiesWithMoreThanTwoAddresses  as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
having count(ba.AddressId) &amp;gt; 2
)
select 
count(businessEntitiesWithOneAddress.BusinessEntityId) as NumberOfBusinessesWithOneAddress,
count(businessEntitiesWithTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithTwoAddresses,
count(businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId) as NumberOfBusinessesWithMoreThanTwoAddresses
from businessEntitiesWithOneAddress
full join businessEntitiesWithTwoAddresses
on businessEntitiesWithOneAddress.BusinessEntityId = businessEntitiesWithTwoAddresses.BusinessEntityId
full join businessEntitiesWithMoreThanTwoAddresses
on businessEntitiesWithTwoAddresses.BusinessEntityId = businessEntitiesWithMoreThanTwoAddresses.BusinessEntityId
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;simplest-solution:0dc054d968169eec76f7161b9d98121b&#34;&gt;Simplest solution&lt;/h2&gt;

&lt;p&gt;Finally, if we relax the requirement (which was self imposed in the first place) to have the results in one row,
as well as not worrying about returning the count for the three or more scenario,
we can simplify everything very nice like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;with  businessEntitiesAddressCount as
(select ba.BusinessEntityId, 
count(ba.AddressId) as NumberOfAddresses
from person.BusinessEntity b
join person.BusinessEntityAddress ba on ba.BusinessEntityId = b.BusinessEntityId
group by ba.BusinessEntityId
)
select 
count(BusinessEntityId) as NumberOfBusinesses,
NumberOfAddresses 
from businessEntitiesAddressCount 
group by NumberOfAddresses;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Best of all; we still get to use a wonderful CTE!&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts:0dc054d968169eec76f7161b9d98121b&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;The first time I realised that it is possible to nest queries arbitrarily it seemed like the sky would be the limit to what wonderfully complex and deep stuff one could do,
all be it with a heavy performance penalty if you were not careful.
As time went on and I wrote larger and larger queries, often times with many levels of nested sub queries, it became
apparent that the readability of the code drastically reduced.
In general purpose programming languages one could avoid deep nested structures quite successfully, but in SQL there seemed to be no way around it
save for making use of temporary tables, or breaking the work up into smaller parts.&lt;/p&gt;

&lt;p&gt;When I found out about CTEs I was very impressed - what an elegant solution!
Yes, a carefully thought out join can often times remove the need for a sub query or CTE, but in cases where sub query is required I now opt for the CTE approach instead.&lt;/p&gt;

&lt;p&gt;Some prominant SQL experts even &lt;a href=&#34;http://modern-sql.com/use-case/literate-sql&#34;&gt;equate the use of CTEs / the WITH clause to literate programming&lt;/a&gt;, and I concur as far as readability and logical flow is concerned.
There is much more to CTEs, especially when considering the recursive variaty, but that is a story for another time.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s been a bit of a ramble, but if you made it this far, you&amp;rsquo;ll do well to give CTEs a try,
and maybe even find it as useful as I do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>now make it fast</title>
      <link>https://blog.goodfast.info/post/now-make-it-fast/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://blog.goodfast.info/post/now-make-it-fast/</guid>
      <description>

&lt;p&gt;&amp;ldquo;He began to copy one of our tables, which contained partial user information, including email IDs, hashed passwords, and last tested URL. His copy operation
    locked the database table, which raised alerts on our monitoring system. On receiving the alerts, we checked the logs, saw an unrecognized IP, and blocked
    it right away. In that time, the hacker had been able to retrieve only a
    portion of the data.&amp;rdquo;
    &amp;ndash; From the postmortem of the
    &lt;a href=&#34;http://www.browserstack.com/attack-and-downtime-on-9-November&#34;&gt;Browser Stack hack of 9th November, 2014 at 23:30 GMT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since relational database management systems (RDBMS) have been
&lt;a href=&#34;http://en.wikipedia.org/wiki/Micro_DBMS&#34;&gt;used in production environments since 1970 (Micro DBMS)&lt;/a&gt;, and the theory on which they run was
developed in the preceding decade,and perfected in the three remaining decades
of the previous century (long  ago), it was not surprising  that, as  a subject,
it received very little attention
in our curriculum - at least where I studied.&lt;/p&gt;

&lt;p&gt;To further degrade the already &lt;em&gt;apparently&lt;/em&gt; low relevance  of the poor subject, we
had to cope with a very thick and overly theoretical textbook we could not yet appreciate,
and subscribe to, if you wanted to be cool and smart, to the snooty perception
held by many peers that the lowly database was useful, but entirely boring.&lt;/p&gt;

&lt;p&gt;A database was considered the type of thing  you rigged up for a family member in Access in a
few hours. Any larger system was the domain of &amp;lsquo;informatics&amp;rsquo; - the
less inspired, more practical brother field of study to the sexy &amp;lsquo;computer science&amp;rsquo; with its focus on AI, advanced programming in C++, the  new Java and the emerging world of Linux and Open Source.&lt;/p&gt;

&lt;p&gt;Stepping outside into the real world, and the rest was history - databases
everywhere, for everything!
In my working career since 2001, every single business system I worked on had
a RDBMS - primarily MS SQL Server, and sometimes MySQL or Oracle as the persistence store.
Through the years I accumulated skill in modelling domains and
manipulating the information through SQL, but optimisation of the storage
structures for efficiency was a task I chose to ignore. I reasoned along the
lines of it is the DBA&amp;rsquo;s / RDBMS&amp;rsquo;s work, or
the mostly false assumption that computers are fast and how much data will
the system  realistically have anyways?&lt;/p&gt;

&lt;p&gt;Really? Is that professional, to say this far will I go and no further,
while you as the programmer is directly responsible for those
horribly slow queries?
Yes, the system works, some things are slow, but hey, they have a lot of data
you say, it&amp;rsquo;s going to be slow at times!
Now you are suppressing   that little voice inside of you, quietly telling you it&amp;rsquo;s wrong
aren&amp;rsquo;t you - it &lt;em&gt;can&lt;/em&gt; be faster&amp;hellip;
Think of the waiting user, the wasted time, times hundreds for internal
systems, times thousands for customers, times  hundreds of thousands or even
millions in the case of the web&amp;hellip;
All that wasted time, all that wasted energy, all the trees&amp;hellip;
Don&amp;rsquo;t think about it too much.
You feel pretty bad by now don&amp;rsquo;t you&amp;hellip;&lt;/p&gt;

&lt;p&gt;Well, I have more bad news for you. At the end of this article you are going to feel even worse, because you
are going to see how simple it is to start to turn things around.&lt;/p&gt;

&lt;p&gt;Dramatics aside, we will have a quick look at the  choices  for table
physical layout and ponder the implications on performance.&lt;/p&gt;

&lt;p&gt;Be warned that we will of course only scratch the surface of SQL performance  optimisation
on SQL Server, and that this article is aimed at software developers
with limited skill in this area, so I&amp;rsquo;m going to explain a bit
- i.e. sit down, this might
take a while.&lt;/p&gt;

&lt;p&gt;Many folks have written fine articles on this subject, and I&amp;rsquo;ll refer to
some of those as we go along.&lt;/p&gt;

&lt;p&gt;My primary justification for writing this article is that I&amp;rsquo;d like to cement
this stuff in my mind, and the best way for me is to write it all out. I hope
you can gain something from this also, and please comment if you disagree or
whatever.&lt;/p&gt;

&lt;h2 id=&#34;it-s-out-there-somewhere-but-has-it-order:810306c57b772cfaef22459bbb55c3d7&#34;&gt;It&amp;rsquo;s out there&amp;hellip; Somewhere&amp;hellip; But has it order?&lt;/h2&gt;

&lt;p&gt;Contrary to popular current belief, the data isn&amp;rsquo;t somewhere in a [cloud]
but it actually resides on one or more  physical storage media (think disk
drives), and
importantly, it&amp;rsquo;s laid down either sorted or just as it was received - for all
practical purposes, unsorted.&lt;/p&gt;

&lt;p&gt;You probably knew this already, but I needed that sentence
because it contained the word &lt;em&gt;cloud&lt;/em&gt; so that I had an excuse to quote
Stallman on cloud computing from 2008:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;It&#39;s stupidity. It&#39;s worse than stupidity: it&#39;s a marketing hype campaign,&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Back to tables&amp;hellip; It is probably because SQL Server&amp;rsquo;s default is to choose the
physically sorted way of laying down the data, when you define a primary key on
a new table, that this table organisation
prevails and is common in most systems.
There is nothing like one size fits all and although the
ordered layout of records is a great fit most of the time, it is not the best
layout in all cases (more on that later).
However, keep in mind there is wisdom in the choice of this default none
the less.&lt;/p&gt;

&lt;h2 id=&#34;some-terminology:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Some terminology&lt;/h2&gt;

&lt;p&gt;For the purpose  of the discussion we&amp;rsquo;ll stick to the most widely used
terminology in the SQL Server world and call the ordered layout of records a
&lt;em&gt;clustered index&lt;/em&gt;, and the
layout without any physical ordering a &lt;em&gt;heap table&lt;/em&gt;, or simply a heap.
The structure  existing purely to speed up locating records  we will refer to
as a &lt;em&gt;nonclustered index&lt;/em&gt;, and how the &lt;em&gt;clustered index&lt;/em&gt;
should be ordered we will call the &lt;em&gt;clustering key&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;clustered-index-the-sorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustered index - the sorted one&lt;/h3&gt;

&lt;p&gt;The term clustered index is unfortunate, and probably the reason a
lot of people develop a blurred notion of cluster vs. non clustered, and table
vs. index.
You could think of a &lt;em&gt;clustered index&lt;/em&gt; as a
database table for maintaining data in an ordered fashion (ordered by one or more
columns) thereby giving efficient access to all the data for one record if the
values of some of the sorting columns are known.  Alternatively you could think of a &lt;em&gt;clustered index&lt;/em&gt; as an
database index to
efficiently gain access to more data,
organised by a subset of  all the columns, yet it has all that added
data in itself.
Either way, Viewed as an ordered tabular representation of data, or an index existing for
efficient access to itself - the important point is the physical sorted
layout, and that the most efficient way to retrieve records from it is via the
index that is itself given the primary key.&lt;/p&gt;

&lt;h3 id=&#34;heap-the-unsorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Heap - the unsorted one&lt;/h3&gt;

&lt;p&gt;A heap is simpler to understand.  Think of a file that grows by repeatedly appending new
lines to it without trying to maintain any kind of order.
Obviously, for efficient access to the data in a heap we need one or more
nonclustered indexes targeting the heap&amp;rsquo;s data, and built up from one or more components
of that data.&lt;/p&gt;

&lt;p&gt;When comparing clustered indexes with heaps we will assume that at least for
the primary key, a nonclustered index is defined on the heap that would make the clustered
index and heap nearly equally  efficient when retrieving a record, given a single primary key value.&lt;/p&gt;

&lt;h3 id=&#34;nonclustered-index-the-real-index:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Nonclustered index - the real index&lt;/h3&gt;

&lt;p&gt;The term &lt;em&gt;nonclustered index&lt;/em&gt; we will use to refer to the structure
that exists primarily to provide   indexed lookup to clustered indexes and heaps
alike.  In SQL Server a heap or clustered index can have many nonclustered
indexes targeting it.&lt;/p&gt;

&lt;h3 id=&#34;clustering-key-how-the-clustered-index-is-clustered-or-sorted:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustering key - how the clustered index is clustered, or sorted&lt;/h3&gt;

&lt;p&gt;Apart from the notion of a primary key, for a clustered index, the &lt;em&gt;clustering
key&lt;/em&gt; defines the subset of columns that instruct the system on how to cluster
or sort the records.
A clustering key should be chosen to be as unique  as possible, but uniqueness
is not required (unlike for the primary key),since the system will add four bytes  called the uniquifier to
the clustering key if it is not indicated as being unique already.&lt;/p&gt;

&lt;h2 id=&#34;implications-of-heap-vs-clustered-index-for-dml:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Implications of heap vs. clustered index for DML&lt;/h2&gt;

&lt;p&gt;To compare the efficiency implications of DML statements on these two broad ways of physical data layout we have to look closer
at the nature of the operations we would like to perform.&lt;/p&gt;

&lt;p&gt;It is fairly intuitive  to reason about  this if we keep in mind:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The physical layout of a heap vs. a
clustered index.&lt;/li&gt;
&lt;li&gt;Indexes should stay up to date after the operation completes.&lt;/li&gt;
&lt;li&gt;Is the operation on one or on multiple records.&lt;/li&gt;
&lt;li&gt;What piece of data is required for the fastest retrieval of records.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;the-single-index-primary-key-as-clustering-key-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;The single index, primary key as clustering key scenario.&lt;/h3&gt;

&lt;p&gt;It turns out that it is generally the best choice to use a clustered index
instead of a heap, for the scenario where you need only one index on
one or more indexing columns. For this scenario, a clustered index takes up
less space, performs better overall, and releases space better when records
get deleted.&lt;/p&gt;

&lt;p&gt;The short answer to why this is the case is that for a clustered index, the
data is the index, so lookups on the clustering key finds the relevant records
directly (after climbing the index B-tree), and alterations affecting the clustering key requires alterations to
one structure - the clustered index. For a heap plus one nonclustered index
however, lookups given the index key is a two-step process. First the nonclustered
index is queried to find the RID, the uniquely identifying key for each heap
row corresponding to the clustering key, and
then the RID is used to fetch the record from the heap.
In addition to this, although alterations involving the index key might not require much work on the heap,
the nonclustered index needs to be updated - again a two-step process.&lt;/p&gt;

&lt;p&gt;Nevertheless, there is a great case to be made for choosing a heap over a
clustered index, which we&amp;rsquo;ll get to shortly, but for the scenario as explained
above (one index on the clustering key), a clustered index is the better choice.&lt;/p&gt;

&lt;p&gt;See this &lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc917672.aspx&#34;&gt;Microsoft best practices&lt;/a&gt; white paper, but be
warned that it is proving the superiority of choosing a clustered index in
favor of a heap in a scenario
where a clustered index is the  best choice.
Do not be fooled by this article into thinking heaps are overall inferior to
clustered indexes, as a casual reading might lead you into believing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at the various DML operations quickly on clusterd index and heap&amp;hellip;&lt;/p&gt;

&lt;h4 id=&#34;insert:810306c57b772cfaef22459bbb55c3d7&#34;&gt;INSERT&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;INSERT&lt;/em&gt; into a heap is simply a case of appending where there is space
available in a data page (or creating a new page at the end). After this, a
second write operation is required - insertion of the index key into the
B-tree  of the nonclustered index.&lt;/p&gt;

&lt;p&gt;Now for a clustered index, the new record
must be inserted in the correct location. If this correct location is on a
data page that is full, that page needs to be split in two .
This might be more time-consuming than the simple insert on the heap, but for
a clustered index there is no second write operation to maintain the index -
the data and index are one. Note that this might even turn out to make a
clustered index perform better overall since it only requires one write
operation.&lt;/p&gt;

&lt;p&gt;Any additional nonclustered indexes on a clustered index or heap would take
roughly the same time to update or keep in sync with the actual data, so we
can ignore their maintenance penalty when comparing.&lt;/p&gt;

&lt;p&gt;But surely we should be able to gain performance with inserts by choosing
either of the two table layouts,   given that heaps
and clustered indexes differ so fundamentally?
In this particular scenario they are  equally good because both
options still require that indexes be kept up to date. If we were to choose a
heap and define absolutely no nonclustered index on it we will gain the fastest
insert performance since inserting would simply be adding data on anywhere
where there is space.&lt;/p&gt;

&lt;p&gt;There is actually a great use case for this: logs and other archival type of
storage that do not require immediate querying.&lt;/p&gt;

&lt;p&gt;But optimisation is a tricky problem, since even for this use case, if many
concurrent inserts are expected, it might   very well be better to choose a
clustered index instead, on some non unique but range-like clustering key that
will jump around a bit, to
achieve an effect of inserting in different locations to prevent everything
from trying to insert in the same data page all the time (as would be the
case for a heap).&lt;/p&gt;

&lt;h4 id=&#34;select-update-and-delete:810306c57b772cfaef22459bbb55c3d7&#34;&gt;SELECT, UPDATE and DELETE&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;SELECT&lt;/em&gt;, &lt;em&gt;UPDATE&lt;/em&gt; and &lt;em&gt;DELETE&lt;/em&gt; first requires locating where the records to
operate on need to be physically
found, followed by the actual action on the data.&lt;/p&gt;

&lt;p&gt;Assuming that the operation simply applies to data given a single primary key
value that is the clustering key, the finding
or first part of this operation
is slightly  less efficient for a heap compared to  a clustered index. For
a clustered index, after the tree is climbed the information is there and
ready to be retrieved, while for a heap, after climbing the tree of the
nonclustered index, you only get the RID, and then require a second
operation to (all be it directly) get at the data in the heap - one additional
level of indirection.&lt;/p&gt;

&lt;p&gt;There is an option to &lt;em&gt;include&lt;/em&gt; columns of the heap or clustered index in the
nonclustered indexes. The effect of this is that, after climbing the tree of
the nonclustered index, those &lt;em&gt;included&lt;/em&gt; column&amp;rsquo;s data is immediately
available - a mini clustered index in the form of a nonclustered index with
included columns. All very straight forward and unambiguous  wouldn&amp;rsquo;t you say?&lt;/p&gt;

&lt;p&gt;Except for &lt;em&gt;SELECT&lt;/em&gt;, the efficiency of the second part, the action part of the operation
can vary much more between heap and clustered index.
For &lt;em&gt;UPDATE&lt;/em&gt;, if the column being updated happens to be one or more of the
columns comprising the clustering key, and the table is a clustered index, then
in order to keep the data  sorted, the system might have to
do page splits.
This in turn mean that potentially, large amounts of data need to be copied around.
For a heap this is never the case, and the columns  can simply be updated in
place - order is of no importance.&lt;/p&gt;

&lt;p&gt;For both heap and clustered index, if any of the columns are part of any
defined nonclustered indexes then altering them might have nonclustered index
maintenance time as a further performance penalty.&lt;/p&gt;

&lt;p&gt;Fortunately, for both heap and clustered index, if the columns being updated are not part
of the clustering key the efficiency of the action part of the operation is nearly similar.&lt;/p&gt;

&lt;p&gt;Performing a &lt;em&gt;DELETE&lt;/em&gt; on a heap or clustered index should be simply a case of marking that record
as deleted and making the space available for potential future inserts. For a
clustered index, no index maintenance is yet again needed while for the heap,
the nonclustered index needs updating.&lt;/p&gt;

&lt;h2 id=&#34;welcome-to-the-real-world-where-tree-climbing-is-to-be-avoided-the-multiple-indexes-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Welcome to the real world, where tree climbing is to be avoided - the multiple indexes scenario&lt;/h2&gt;

&lt;p&gt;The single index, primary key as clustering key and lookup scenario described earlier
might appear  early on, and a lot in most models, but very soon you
will also want to efficiently query on other columns on wider  (more columns)
and deeper (more rows) tables.&lt;/p&gt;

&lt;p&gt;To prevent full table scans, you start adding nonclustered indexes,
and this is where heaps start to become the more attractive alternative.&lt;/p&gt;

&lt;p&gt;Suppose for a moment that your  table (let&amp;rsquo;s call it table &lt;em&gt;T&lt;/em&gt;) that became wide and deep overnight is a
clustered index, and the primary key (&lt;em&gt;K&lt;/em&gt;) is also the clustering key.&lt;/p&gt;

&lt;p&gt;Each additional nonclustered index (&lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip;) on &lt;em&gt;T&lt;/em&gt; will store in its leaf nodes the
values of &lt;em&gt;K&lt;/em&gt;.
This means that a query on &lt;em&gt;T&lt;/em&gt; utilising  some nonclustered index &lt;em&gt;N&lt;/em&gt;  results
in a tree climb of &lt;em&gt;N&lt;/em&gt; that yields some value of &lt;em&gt;K&lt;/em&gt;. Following this we
require another tree climb of the clustered index that
is &lt;em&gt;T&lt;/em&gt;, given a value for &lt;em&gt;K&lt;/em&gt;, and only then is the actual data reached.&lt;/p&gt;

&lt;p&gt;On the other hand, suppose  now that your wide and deep table  &lt;em&gt;H&lt;/em&gt; is a heap
instead, with one or more nonclustered indexes &lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip; and so on.
This time, each nonclustered index &lt;em&gt;N&lt;/em&gt; will store at the leaf node the RID
of the relevant row, and not simply  yet another key into a further index.
This means that if some query on &lt;em&gt;H&lt;/em&gt; utilise one of the nonclustered indexes,
only one tree climb of that nonclustered index is required, after which the
RID is obtained, and unlike a clustering key, a RID represents the
physical position of the record in the heap, and thus can be directly accessed
- no further tree climbing required.&lt;/p&gt;

&lt;p&gt;For a more in-depth look at this, and some hard numbers comprising a
compelling case,  do yourself a favor and read Markus
Wienand&amp;rsquo;s fine article,
&lt;a href=&#34;http://use-the-index-luke.com/blog/2014-01/unreasonable-defaults-primary-key-clustering-key&#34;&gt;Unreasonable Defaults: Primary Key as Clustering
Key&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary-and-closing-thoughts-optimising-performance-is-an-interesting-and-very-relevant-problem:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Summary and closing thoughts - Optimising performance is an  interesting and very relevant problem&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;Premature optimisation is the root of all evil&amp;quot;
 -- Donald E. Knuth
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As much as I concur with that statement, especially how it applies to code, I do think that a good understanding
of the options available to you when turning a data model into an actual
database schema can proactively prevent vicious      cycles of poor performing
monster database servers.
Yes, there is a lot of things one can do, and the precise case where one
technique or option would be the better option is hard to identify, but the
better your understanding of the internals, the more likely you are to get it
right first time,
and the more it will start to happen that you are writing a query and you
suddenly realise that a specific index would benefit that query tremendously.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve rambled a bit, but to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are two main choices for the physical layout of data for tables.&lt;/li&gt;
&lt;li&gt;If only one index on the primary key is required it&amp;rsquo;s probably the best to choose a clustered index.&lt;/li&gt;
&lt;li&gt;For a many index scenario choose a heap.&lt;/li&gt;
&lt;li&gt;For best insert performance on high  loads choose a heap with no nonclustered indexes.&lt;/li&gt;
&lt;li&gt;Use the include columns feature of nonclustered indexes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Today there are exciting new alternative data storage technologies like fully
in memory databases, distributed systems such  as Hadoop, Google&amp;rsquo;s BigTable
approach, and document oriented noSQL
options such as ElasticSearch to name only a very few.
These alternative solutions to the problem of working with large data sets
have and will continue to be applied more and more, but if the last decade is
anything to go by,  the relational database is going to stick around for quite
some time still, so investing time into learning how to optimise it is
time well spent.&lt;/p&gt;

&lt;p&gt;The reason for SQL systems remaining central to all serious data storage
applications  is not by accident. There is a theoretical reason, routed in the
so-called CAP theorem.
For an overview of how the CAP theorem restricted the growth and adoption of
noSQL systems, have a look at
&lt;a href=&#34;http://use-the-index-luke.com/blog/2013-04/whats-left-of-nosql&#34;&gt;Whats left of NoSQL?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An interesting response to this is FoundationDB (see
&lt;a href=&#34;http://www.theregister.co.uk/Print/2012/11/22/foundationdb_fear_of_cap_theorem/&#34;&gt;NoSQL&amp;rsquo;s CAP theorem busters: We don&amp;rsquo;t drop ACID&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;An exciting emerging trend is to harness the strengths of both the traditional
RDBMS and the more recent big data distributed, more normalised data storage
technologies. For an interesting application of this, see
&lt;a href=&#34;http://msdn.microsoft.com/en-gb/magazine/dn802606.aspx&#34;&gt;Use Updatable Tables for Responsive Real-Time Reporting&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I hope this short info burst tickled your interest enough that you will go
ahead and look into all of it a bit more.
Personally I have been pleasantly surprised by the depth of this subject area.&lt;/p&gt;

&lt;p&gt;I can highly recommend the book
&lt;a href=&#34;http://sql-performance-explained.com/?utm_source=UTIL&amp;amp;utm_medium=main&amp;amp;utm_campaign=second&#34;&gt;SQL Performance Optimisation&lt;/a&gt; for an in-depth look at this
subject, and the
&lt;a href=&#34;http://use-the-index-luke.com/&#34;&gt;Use The Index Luke&lt;/a&gt; site.&lt;/p&gt;

&lt;p&gt;This article also appears on &lt;a href=&#34;http://www.inivit.com/blog/&#34;&gt;Inivit&amp;rsquo;s blog&lt;/a&gt; along with some other fine posts from former colleagues.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>