<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Performance on Good Fast</title>
    <link>https://blog.goodfast.info/categories/performance/</link>
    <description>Recent content in Performance on Good Fast</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Sep 2017 08:31:17 +0200</lastBuildDate>
    <atom:link href="https://blog.goodfast.info/categories/performance/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to use the transitive closure over a set of relations for fast path finding in SQL</title>
      <link>https://blog.goodfast.info/post/transitive-closures/</link>
      <pubDate>Tue, 26 Sep 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/transitive-closures/</guid>
      <description>

&lt;p&gt;In a &lt;a href=&#34;http://goodfast.info/post/speed-up-views-through-custom-materialization/&#34;&gt;previous post&lt;/a&gt;, I wrote about how we make sense of the world by modelling relationships between things as tree-like hierarchies.
This time we will add to this hierarchical data structure, a representation derived by calculating all possible paths.
This set of paths is referred to as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Transitive_closurejj&#34;&gt;transitive closure&lt;/a&gt;, and can be thought of as the set of all paths if you start at each node in the tree.&lt;/p&gt;

&lt;p&gt;I wish I could tell you that it is as simple as Mr. Eby in &lt;a href=&#34;http://dirtsimple.org/2010/11/simplest-way-to-do-tree-based-queries.html&#34;&gt;this article&lt;/a&gt; makes it out to be, but
alas, when I got right down implementing a full solution, things got quite involved.
It tends to be like that.
None the less, credit where credit is due; go read that article first I can highly recommend it, and then come back here for more!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve developed the code for SQL Server, so it is immediately T-SQL compatible, but you can surely alter it for any decent database.&lt;/p&gt;

&lt;h2 id=&#34;background:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;Trees and hierarchies in general can get quite complicated, so why would we choose to add to this complexity yet another data structure?
The reason is so that we can apply &lt;a href=&#34;https://en.wikipedia.org/wiki/Memoization&#34;&gt;memoization&lt;/a&gt;; we buy time with space.
At the cost of the time to compute the transitive closure once, and the cost of the space required to persist it, we gain the time it would have taken to calculate it each time it is needed.&lt;/p&gt;

&lt;p&gt;Previously I wrote about how one can go about to materialize an entire complex and expensive view.
The use of the transitive closure can also be thought of as a kind of materialization, but it is far smarter and promises to be more useful.&lt;/p&gt;

&lt;p&gt;The [transitive] part in the name refers to a property that a relation can exhibit.
Since you are related to your father, and your child is related to you, your child is also related to your father.
We can say that the inheritance relation is transitive.
Since 9 &amp;gt; 5, and 5 &amp;gt; 3, it is also true that 9 &amp;gt; 5 &amp;gt; 3 and 9 &amp;gt; 3;
the &amp;ldquo;greater than&amp;rdquo; relation is transitive.&lt;/p&gt;

&lt;h2 id=&#34;representing-the-hierarchy:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Representing the hierarchy&lt;/h2&gt;

&lt;p&gt;The hierarchy we will work with is a simple one:&lt;/p&gt;


&lt;img srcset=&#34;../../ct_tree.svg&#34; src=&#34;../../ct_tree.png&#34; alt=&#34;Diagram of the tree with node a at root, nodes b and c below it, below node b is node d and e, below node c is node f, below node d is node g.&#34;&gt; 



&lt;p&gt;The usual way to represent such a hierarchy in a table is through self referencing records:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;id&lt;/th&gt;
&lt;th&gt;parent_id&lt;/th&gt;
&lt;th&gt;label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;lsquo;a&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;lsquo;b&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;lsquo;c&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&amp;rsquo;d&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&amp;lsquo;e&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&amp;lsquo;f&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&amp;lsquo;g&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Such self referencing records in a single table makes manipulation of the hierarchy very simple.
For example, to move the sub tree rooted in node 2 and make it fall under node 6, we simply update the parent_id of node 2 to reference node 6.
This simplicity, however, comes at a cost.
When you want to traverse the hierarchy, you require iteration or recursion which is generally expensive.
This is especially so if all that you are after is only a portion of the tree, or worse, only the path from the root to a particular intermediate or leaf node.&lt;/p&gt;

&lt;p&gt;Suppose you want to find out the path from  node &amp;lsquo;g&amp;rsquo; to the root.
After finding the entry for node &amp;lsquo;g&amp;rsquo;, you have to repeatedly find the parent until there is no more parent.
Suppose you want to get the path from the root for each node in the tree.
The database has to do this process for every node.&lt;/p&gt;

&lt;p&gt;A transitive closure over all the relations in the base table gives you a ready-made set of paths which you can index and query, just like any other set of records.
No more need for recursive CTE&amp;rsquo;s each time you want path information, or worse still, multiple queries!&lt;/p&gt;

&lt;h2 id=&#34;setup:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Setup&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;
-- create table node to represent the relations
create table node (id int, parent_id int, label varchar(50));

-- load noad
insert into node values 
(1,0,&#39;a&#39;),
(2,1,&#39;b&#39;),
(3,1,&#39;c&#39;),
(4,2,&#39;d&#39;),
(5,2,&#39;e&#39;),
(6,3,&#39;f&#39;),
(7,4,&#39;g&#39;);

-- table to hold the transitive closure over nodes
create table closure (parent_id int, child_id int, depth int, route varchar(100));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Table node is the standard self referencing structure, and table closure will contain the paths that node represent.
Column node.label is only the label that applies to the particular node row, but column closure.route will contain a nice chain of all the labels from closure.parent_id to closure.child_id.
In a database that supports arrays, such as PostgreSQL, we can even go so far as to store the actual id values of the whole path.
In column closure.depth we want to store how many hops it takes to go from the node at parent_id to the node at child_id.&lt;/p&gt;

&lt;p&gt;What we want to achieve is to calculate all possible paths in the tree and represent them like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;parent_id&lt;/th&gt;
&lt;th&gt;child_id&lt;/th&gt;
&lt;th&gt;depth&lt;/th&gt;
&lt;th&gt;route&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a &amp;gt; b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a &amp;gt; c&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;c &amp;gt; f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; c &amp;gt; f&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b &amp;gt; d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; d&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b &amp;gt; e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; e&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;b &amp;gt; d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;a &amp;gt; b &amp;gt; d &amp;gt; g&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Notice the identity paths, or paths starting and ending in the same node, with depth of zero.
Later on it will become apparent why we require these.&lt;/p&gt;

&lt;h3 id=&#34;closure-insert:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure insert&lt;/h3&gt;

&lt;p&gt;A new entry in table node can only be one of the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;a new, additional root - there exists no parent for it&lt;/li&gt;
&lt;li&gt;a new, additional child - there exists a parent for it&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For either type of new node, table closure requires a new identity path.
Furthermore, if the new node has a parent, we also need to add entries for all paths ending in the new node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_insert @parent_id int, @child_id int, @route varchar(50) as 
begin
-- always insert identity
insert into closure (parent_id,child_id,depth,route) 
values (@child_id,@child_id,0, @route);
if  @parent_id &amp;lt;&amp;gt; @child_id and @parent_id is not null
insert into closure (parent_id,child_id,depth, route) 
select parent.parent_id, child.child_id,
parent.depth + child.depth + 1, 
parent.route + &#39; &amp;gt; &#39; + child.route 
from closure as parent cross join closure as child 
where parent.child_id = @parent_id and child.parent_id = @child_id;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;closure-delete:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure delete&lt;/h3&gt;

&lt;p&gt;Deleting a relation (suppose it is the node with id @child_id)from the node table requires us to delete all paths from the closure table that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;start out from @child_id,&lt;/li&gt;
&lt;li&gt;end in @child_id,&lt;/li&gt;
&lt;li&gt;runs through @child_id, so any path that starts at any of @child_id&amp;rsquo;s parents,and any path that ends in any of @child_id&amp;rsquo;s children&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_delete @child_id int as begin
delete from link  
from closure as parent, closure as link, closure as child, closure as to_delete 
where 
parent.parent_id = link.parent_id and child.child_id = link.child_id
and parent.child_id = to_delete.parent_id and child.parent_id = to_delete.child_id
and (to_delete.child_id = @child_id or to_delete.parent_id = @child_id) 
and to_delete.depth &amp;lt; 2;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;closure-update:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure update&lt;/h3&gt;

&lt;p&gt;Similar to the delete operation, if a node changes, all paths that start from, or end in, or go through that node needs to be updated since the label of the node might have changed and thus the routes need to be built again.
If a node moved; if it is now a sub node of another node then all paths starting from, ending in and going through the node that moved needs to be deleted, and new paths added.&lt;/p&gt;

&lt;p&gt;To simplify matters we are going to delete all such starting from, ending in, and going through paths, and re-insert the individual nodes again in the correct order.
Before we delete all the paths we first will temporarily store all the nodes we will be re-inserting afresh after the delete.
Then we will do the delete, followed by successively calling the insert proc to insert everything again.&lt;/p&gt;

&lt;p&gt;But what if we insert a node (think add all paths involving this node) before we inserted the node&amp;rsquo;s parent(s)?
If you take a look at our insert proc you&amp;rsquo;ll see very quickly that this will result in problems; we will not find the parent paths, and thus add too few actual paths.
For this reason it is essential that we first insert all nodes without parents, then followed by the rest of the set that do have a parent, which will already be inserted.
The insert proc will take care of adding all required paths, but only if we insert in the correct order.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_update @child_id int as begin
-- temp storage of nodes to insert after the deletion
declare @t as table (id int primary key, parent_id int, label varchar(50));

insert into @t (id,parent_id,label) 
select link.child_id,n.parent_id,n.label 
from closure as link join node as n on n.id = link.child_id
where  link.parent_id = @child_id;

delete from link 
from closure as link join @t as t
on link.child_id = t.id or link.child_id = @child_id;

-- repeatedly call the insert proc in the correct order, 
-- which is ensured by the recursive CTE over the set of nodes to insert
declare @_p int, @_c int, @_l varchar(50);
declare cur cursor fast_forward for 
with  to_insert as (
	select parent_id, id, label from @t
),  to_insert_ordered  as (
-- the anchor for the recursion
	select ti.parent_id, ti.id, ti.label 
	from to_insert as ti
	where ti.parent_id = 0
	or ti.id = @child_id
	or ti.parent_id not in (
		select id from to_insert
	)
	union all
	select ti.parent_id, ti.id, ti.label 
	from to_insert_ordered as tio
	join to_insert as ti
	on ti.parent_id = tio.id
) select parent_id, id, label from to_insert_ordered;
open cur;
fetch next from cur into @_p, @_c, @_l;
while @@FETCH_STATUS = 0 begin
	exec closure_insert @_p,@_c,@_l;
fetch next from cur into @_p, @_c, @_l;
end close cur deallocate cur;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The insertion cursor query follows the usual pattern for a recursive CTE:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;establish the data to recurse over: to_insert&lt;/li&gt;
&lt;li&gt;establish the anchor, or starting point: to_insert_ordered&lt;/li&gt;
&lt;li&gt;union with a join onto itself and the whole list to_insert&lt;/li&gt;
&lt;li&gt;select the result&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;closure-refresh:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Closure refresh&lt;/h3&gt;

&lt;p&gt;To do the initial load, and in case something goes wrong, we add one more proc; a full refresh proc, which is simply successive calls to the insert proc, but in the correct order as described in the previous section.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;create proc closure_refresh as begin
truncate table closure;

declare @p int, @c int, @l varchar(50);
declare cur cursor fast_forward for with  to_insert as (
	select parent_id, id, label from node
),  to_insert_ordered  as (
	select ti.parent_id, ti.id, ti.label 
	from to_insert as ti
	where ti.parent_id = 0 or
	ti.parent_id not in (
		select id from to_insert
	)
	union all
	select ti.parent_id, ti.id, ti.label 
	from to_insert_ordered as tio
	join to_insert as ti
	on ti.parent_id = tio.id
) select parent_id, id, label from to_insert_ordered;
open cur;
fetch next from cur into @p, @c, @l;
while @@FETCH_STATUS = 0 begin
	exec closure_insert @p,@c,@l;
fetch next from cur into @p, @c, @l;
end close cur deallocate cur;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;Here are some sample queries which you can use to test your solution. I&amp;rsquo;ve done it, not going to paste 50 tables of output here, try all sorts and see what happens:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;-- start clean
exec closure_refresh;
-- check that everything is in order
select * from closure;
-- relabel node 2
update node set label = &#39;z&#39; where id = 2;
-- do what the after update trigger will do:
exec closure_update 2;
-- not only node 2&#39;s identity entry, 
-- but all paths involving it should now say &#39;z&#39; instead of &#39;b&#39;
select * from closure;
-- fix node 2 again:
update node set label = &#39;b&#39; where id = 2;
exec closure_update 2;
-- things should be back to what it was initially...
select * from closure;
-- move node 2, or &#39;b&#39; under node 6
update node set parent_id = 6 where id = 2;
exec closure_update 2;
-- node 2 should now be under node 6
select * from closure;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;go-forth-and-conquer:594089c9c61d3c8bcdb4d6b1895da11a&#34;&gt;Go forth and conquer!&lt;/h2&gt;

&lt;p&gt;Armed with the procedures we developed thus far, we can proceed to hook them up with after triggers on the node table.
When the node table change, the after triggers fire and the closure table stays up to date.&lt;/p&gt;

&lt;p&gt;When we want to answer questions such as, does there exist a path from X to Y, what is the longest path from X to Y (for obtaining the full path from the root), and many more such queries, we can simply perform fast selects against the closure table.
We can index parent_id, child_id, and (parent_id,child_id) and so on in order to speed things up.
We can create a few custom views for quickly determining all the paths from the roots to the individual intermediate and leaf nodes.&lt;/p&gt;

&lt;p&gt;The recursive CTE that is needed for the update procedure is unfortunate, but luckily it only operates on the set of nodes that need to change.
Unless you are altering root nodes all the time, this will generally be limited to a small number, and not the entire set of data as is the case in a full materialization of the whole tree.
This means that the maintenance overhead will generally be far less.&lt;/p&gt;

&lt;p&gt;This is only a proof of concept, but you can go forth from here and use it as a reference to implement more complex and advanced transitive closures.
I hope it helps you maintain performance on queries on hierarchies, and I hope you&amp;rsquo;ve learned something new along the way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wizen up a bit : solve problems bitwize</title>
      <link>https://blog.goodfast.info/post/wizen-up-a-bit/</link>
      <pubDate>Wed, 20 Sep 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/wizen-up-a-bit/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m rather obsessed with bits. All sorts of bits, at various times, but in particular, the digital bit of the Binary system. Notice the capitalization of &amp;ldquo;Binary&amp;rdquo; - it is intended.
Efficient bit representations of information is purity ; ever more compact representations elegance itself,
so for this post I invite you to come with me, way back to 2013, when a nice couple of  &lt;a href=&#34;https://en.wikipedia.org/wiki/Bitwise_operation&#34;&gt;bitwise&lt;/a&gt; operations flaunted their power and expressiveness.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s tough for me when a value that will only ever require N distinct states gets over represented by vastly more bits than is needed.
A 4-byte integer to store a human age, a UUID, with it&amp;rsquo;s crazy 16 bytes, and the list goes on and on: we can do better!
Think of the real, actual small value, arriving inside the 64-bit CPU register, and the wasteful runs of zeros.
Think of it, happening billions and billions of times a second.
We can only hope that somewhere along the way, things get packed in a bit to mitigate those wasted runs of zeros.&lt;/p&gt;

&lt;p&gt;Once I read that using a short, or 2-byte database column type or variable is actually not more performant than just using a 4-byte integer.
Due to the fact that the smallest addressable space is anyways 4 bytes large, it took exactly the same resources to process 4, 2 or even a 1 byte word.
The exact context of that claim escapes me.
I put it down to my confirmation bias, operating sub consciously, blocking it out.
I like my confirmation bias working for me like that.&lt;/p&gt;

&lt;h2 id=&#34;the-back-story:d352b201a1938ed46d030d55323cc26d&#34;&gt;The back story&lt;/h2&gt;

&lt;p&gt;It was the weekend of the Google CodeJam qualifier.
Having heard of it some weeks before, I planned to check out the rules properly, and do some practice problems. It never happened.
Arriving home in the afternoon on the Saturday, I thought I did not have much of a chance at it due to the time constraint, but I could not help myself giving it a go anyways.
So it happened. Pacing around in my flat, beer in hand, I chose &lt;a href=&#34;https://code.google.com/codejam/contest/2270488/dashboard#s=p0&#34;&gt;this little problem&lt;/a&gt; to solve.
It was a sort of tic-tac-toe variant, but only the board state identifier part.&lt;/p&gt;

&lt;p&gt;Given a 4-x-4 board, determine if either player is in the winning position, or if a tie occurred.
The &amp;rsquo;T&amp;rsquo; symbol being neutral in that it could count in either player&amp;rsquo;s favor.&lt;/p&gt;

&lt;p&gt;Yes, you can loop over the elements or throw a Linq expression at it, but hey, we can have more fun than that, with bit masks and bit shifts.
This was bit mask land, and I was in my element!
For more details about the problem, visit the &lt;a href=&#34;https://code.google.com/codejam/contest/2270488/dashboard#s=p0&#34;&gt;problem statement&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;bit-boards:d352b201a1938ed46d030d55323cc26d&#34;&gt;Bit boards&lt;/h2&gt;

&lt;p&gt;We choose an integer, 4 bytes, to store a bit mask of the player O, player X, how a column looks, how each of the two diagonals look and so on. We could have used a 2-byte unsigned  short instead of an int, sadly we didn&amp;rsquo;t. Well, got to leave something for next time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;        public static int column, diagonal1, diagonal2, fullboard;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;            int xboard = 0, oboard = 0, tboard = 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We assign the static patterns we&amp;rsquo;ll use for column, the two diagonals and the full board:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;            column = Convert.ToInt32(&amp;quot;1000100010001&amp;quot;, 2);
            diagonal1 = Convert.ToInt32(&amp;quot;1000010000100001&amp;quot;, 2);
            diagonal2 = Convert.ToInt32(&amp;quot;0001001001001000&amp;quot;, 2);
            fullboard = Convert.ToInt32(&amp;quot;1111111111111111&amp;quot;, 2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m not going to bother you with file IO, so we skip to where we set the X, O and T boards:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;                        xboard = StringBoardToBitBoard(line, &#39;X&#39;);
                        oboard = StringBoardToBitBoard(line, &#39;O&#39;);
                        tboard = StringBoardToBitBoard(line, &#39;T&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Function StringBoardToBitBoard does exactly what  you would imagine - it assigns a &amp;lsquo;1&amp;rsquo; in the bit position if the char occurs and a &amp;lsquo;0&amp;rsquo; otherwise - see below for the full program.&lt;/p&gt;

&lt;h2 id=&#34;decision-and-output:d352b201a1938ed46d030d55323cc26d&#34;&gt;Decision and output&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;                        if (Won(xboard, tboard))
                            sw.WriteLine(&amp;quot;Case #{0}: X won&amp;quot;, i);
                        else if (Won(oboard, tboard))
                            sw.WriteLine(&amp;quot;Case #{0}: O won&amp;quot;, i);
                        else if (((xboard | oboard | tboard) &amp;amp; fullboard) == fullboard)
                            sw.WriteLine(&amp;quot;Case #{0}: Draw&amp;quot;, i);
                        else
                            sw.WriteLine(&amp;quot;Case #{0}: Game has not completed&amp;quot;, i);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-real-magic-bitwise-operators-and-shifts:d352b201a1938ed46d030d55323cc26d&#34;&gt;The real magic: bitwise operators and shifts&lt;/h2&gt;

&lt;p&gt;And finally, let&amp;rsquo;s have a look at the elegant part; function Won and friends:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;        public static bool Won(int board, int tboard)
        {
            board |= tboard;  // apply the T position
            return LinesMatch(board) || ColumnsMatch(board) || DiagonalsMatch(board);
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;makes sense doesn&amp;rsquo;t it; you win if you have a line, a column or a diagonal.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;        public static bool LinesMatch(int board)
        {
            return board == (board | 15)
                || board == (board | (15 &amp;lt;&amp;lt; 4))
                || board == (board | (15 &amp;lt;&amp;lt; 8))
                || board == (board | (15 &amp;lt;&amp;lt; 12));
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You have a lines match if your board position
is logical OR-ed (the &amp;lsquo;|&amp;rsquo; operator)  with any of the line representations, and still is equal to itself.
For the 4 line representations we could have used constants, to avoid the bit shift (&amp;rsquo;&amp;lt;&amp;lt;&amp;lsquo;) but defining them; that would be too laborious, but would probably have been better.
Also, the 15 magic number in there, for the first lines representation; should really live in a constant.&lt;/p&gt;

&lt;p&gt;In the same way, but slightly different, the columns and diagonals are checked:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;        public static bool ColumnsMatch(int board)
        {
            return
                board == (board | column)
                || board == (board | (column &amp;lt;&amp;lt; 1))
                || board == (board | (column &amp;lt;&amp;lt; 2))
                || board == (board | (column &amp;lt;&amp;lt; 3));
        }
        public static bool DiagonalsMatch(int board)
        {
            return board == (board | diagonal1)
                || board == (board | diagonal2);
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The diagonals checking could have happened all at once, why is this possible?&lt;/p&gt;

&lt;h2 id=&#34;conclusion:d352b201a1938ed46d030d55323cc26d&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Well, contrary to the confusion to what a casual glance of the code might  evoke in you, it&amp;rsquo;s really not terribly convoluted, but possibly slightly so.
The heart of the program is only one loop over the input set, while the win checking is a small constant set of finite bit operations, and bit operations can definitely be implemented as single machine instructions.
It surely ran fast on the small and large inputs, and so this question I got right.&lt;/p&gt;

&lt;p&gt;Not realizing that one only needed to get some of the problems correct in order to qualify for the actual contest, I happily shut down my computer and went to sleep.
Yay, I managed to do one of the CodeJam qualifier challenges, but there were no time to do all of them!
The next week when I realized my mistake, I was upset. I just had to get one or two more right and I could possibly have qualified!&lt;/p&gt;

&lt;p&gt;Less is more.  The saxophonist  &lt;a href=&#34;https://en.wikipedia.org/wiki/Jan_Garbarek&#34;&gt;Jan Garbarek&lt;/a&gt; has been credited with his &amp;ldquo;generous use of silence&amp;rdquo;.
I like that. May it be that, when people survey our code and data representations, may it be that they would reflect upon how few bits was used, or how few lines of code were written&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;full-program:d352b201a1938ed46d030d55323cc26d&#34;&gt;Full program&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-C#&#34;&gt;using System;
using System.IO;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace ConsoleApplication1
{
    class Program
    {
        public static int column, diagonal1, diagonal2, fullboard;

        static void Main(string[] args)
        {
            column = Convert.ToInt32(&amp;quot;1000100010001&amp;quot;, 2);
            diagonal1 = Convert.ToInt32(&amp;quot;1000010000100001&amp;quot;, 2);
            diagonal2 = Convert.ToInt32(&amp;quot;0001001001001000&amp;quot;, 2);
            fullboard = Convert.ToInt32(&amp;quot;1111111111111111&amp;quot;, 2);

            int xboard = 0, oboard = 0, tboard = 0;

            try
            {
                string infile = &amp;quot;A-large.in&amp;quot;, outfile = &amp;quot;A-large.out&amp;quot;, line = &amp;quot;&amp;quot;;
                using (StreamReader sr = new StreamReader(infile))
                {
                    using (StreamWriter sw = new StreamWriter(outfile))
                    {
                    int numberOfProblems = Int32.Parse(sr.ReadLine());
                    for (int i = 1; i &amp;lt;= numberOfProblems; i++)
                    {
                        line = sr.ReadLine();
                        line += sr.ReadLine();
                        line += sr.ReadLine();
                        line += sr.ReadLine();

                        xboard = StringBoardToBitBoard(line, &#39;X&#39;);
                        oboard = StringBoardToBitBoard(line, &#39;O&#39;);
                        tboard = StringBoardToBitBoard(line, &#39;T&#39;);
                        if (Won(xboard, tboard))
                            sw.WriteLine(&amp;quot;Case #{0}: X won&amp;quot;, i);
                        else if (Won(oboard, tboard))
                            sw.WriteLine(&amp;quot;Case #{0}: O won&amp;quot;, i);
                        else if (((xboard | oboard | tboard) &amp;amp; fullboard) == fullboard)
                            sw.WriteLine(&amp;quot;Case #{0}: Draw&amp;quot;, i);
                        else
                            sw.WriteLine(&amp;quot;Case #{0}: Game has not completed&amp;quot;, i);

                        line = sr.ReadLine();
                    }
                    }
                    //Console.WriteLine(line);
                }
            }
            catch (Exception e)
            {

            }
        }

        public static bool Won(int board, int tboard)
        {
            board |= tboard;  // apply the T position
            return LinesMatch(board) || ColumnsMatch(board) || DiagonalsMatch(board);
        }
        public static bool LinesMatch(int board)
        {
            return board == (board | 15)
                || board == (board | (15 &amp;lt;&amp;lt; 4))
                || board == (board | (15 &amp;lt;&amp;lt; 8))
                || board == (board | (15 &amp;lt;&amp;lt; 12));
        }
        public static bool ColumnsMatch(int board)
        {
            return
                board == (board | column)
                || board == (board | (column &amp;lt;&amp;lt; 1))
                || board == (board | (column &amp;lt;&amp;lt; 2))
                || board == (board | (column &amp;lt;&amp;lt; 3));
        }
        public static bool DiagonalsMatch(int board)
        {
            return board == (board | diagonal1)
                || board == (board | diagonal2);
        }

        public static int StringBoardToBitBoard(string stringBoard, char oneChar)
        {
            string bitBoard = new String(stringBoard.Select(
                x =&amp;gt; x == oneChar ? &#39;1&#39; : &#39;0&#39;)
                .ToArray());
            return Convert.ToInt32(bitBoard, 2);
        }
    }
}

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Speed up slow views through custom materialization</title>
      <link>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</link>
      <pubDate>Thu, 08 Jun 2017 08:31:17 +0200</pubDate>
      
      <guid>https://blog.goodfast.info/post/speed-up-views-through-custom-materialization/</guid>
      <description>

&lt;p&gt;SQL views are aluring as a means of abstraction; a &amp;ldquo;building block&amp;rdquo; to hide away commonly used complexity.
It is no wonder then that us developers will try them out, and before you know it, your clever recursive CTE view on that hierarchy is used everywhere, by everyone, but how is it affecting overall database performance&amp;hellip;&lt;/p&gt;

&lt;p&gt;They look like tables, can be joined on, selected from, and in some cases even updated just like tables, yet the reality is that they are not like tables.
So, you cannot consider a view to be a type of stored procedure, and you can also not consider a view to be a type of table or index; it is something in between.&lt;/p&gt;

&lt;p&gt;It is possible for the query planner to &amp;ldquo;reach into&amp;rdquo; a view, and discover which indexes to use in order to access information in the best way, but this quikcly breaks down once you perform any kind of complicated thing, such as a CTE, UNION statement, or anything else that breaks up the link from the source tables to the result set of the view.
When exactly you break this ability of the query planner to use appropriate indexes is a great idea for a future post - I have not found anything that directly states this as of yet.
Intuitivly  it makes sense that some kinds of data mangling will just make it impossible for the query planner to find indexes to use.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Note that it&#39;s of course always best to first inspect the query plan before concluding that a fiew is or is not making use of a particular index. I have made the mistake before of making grand statements on how poor the query planner is at choosing an index when dealing with a view, only to be shown that it in fact can do a bit more than what you might expect!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My goal in this post is simply to make you aware of the possability that complex views might be causing your database to perform sub optimally, and then to offer an in place, zero downtime solution to the problem.&lt;/p&gt;

&lt;h3 id=&#34;when-to-use-views:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;When to use views&lt;/h3&gt;

&lt;p&gt;The way I currently understand it,  you should use views when you want different &lt;em&gt;views&lt;/em&gt; on the same table, or simple connected set of tables; i.e. you want to include/exclude certain columns/rows, so in other words, as a means of information hiding, a means of performing restricted access to the information in the underlying tables; a different take on the same data.
It is debatable how many new systems are developed, that would choose to deligate security, access restriction type of functionality to the database, but there is a fair chance that it is happening out in the wild, since a recent SQL Server feature is row-level access, and data masking.&lt;/p&gt;

&lt;h3 id=&#34;discovering-the-problem:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Discovering the problem&lt;/h3&gt;

&lt;p&gt;It was while I was performance tuning a very busy Azure Database, that I discovered a collection of particularly slow executing queries, spending most of their time in CPU.
The data volume involved could not account for the poor performance, being in the mere tens of thousands of small rows.
As far as I could determine, most of the appropriate indexes existed that would normally make things perform acceptably.
Something else was up&amp;hellip;&lt;/p&gt;

&lt;p&gt;Turning to the query plans, a pattern started emerging; slow, very slow views were joined on.
The views themselves were not very complex, but they did something interesting: they were recursive CTEs designed to traverse
a hierarchy, essentially a tree structure, and produce a full fan out of the entire tree.&lt;/p&gt;

&lt;h3 id=&#34;solution:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Solution&lt;/h3&gt;

&lt;p&gt;My first inclination was to have SQL Server materialize these views for me. Materialized (or indexed) views is an &lt;a href=&#34;http://sqlmag.com/database-performance-tuning/introducing-indexed-views&#34;&gt;old feature of the server&lt;/a&gt;, dating back to SQL Server 2000 if I&amp;rsquo;m not mistaking, so surely in 2017 this should be completely possible.
Well, it turns out that in order for a view to be materialized, &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/relational-databases/views/create-indexed-views&#34;&gt;a whole list of requirements&lt;/a&gt; need to be satisfied. For example, something as innocently looking as a LEFT JOIN in the view query would put a quick end to this solution path.&lt;br /&gt;
Researching it a bit further shed some light on why all these restrictions apply, but although it does make you be a bit more understanding, it still feels like this is something that should be possible, no matter how complex the view is.&lt;/p&gt;

&lt;p&gt;Completely redesigning the underlying hierarchical representation, with something like transative closures was not really an option, so the next best idea was to custom materialize these views.
The data access characteristics of the hierarchy and supporting tables was that they did not change all that often, yet they were queried all the time.
This was great news, since it meant that even if the materialization process took a bit of time, this would quickly be compensated for by the much, much faster query times.
Having the previously computed data now reside in a proper table also meant that it could be appropriately indexed, clustered, and even partitioned (although the volume was far too low for this need).&lt;/p&gt;

&lt;h4 id=&#34;the-procedure:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;The procedure&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Create a table (we&amp;rsquo;ll call it the Working) that structurally mirrors the result of querying the view (the View, later to be renamed to the Origin).&lt;/li&gt;
&lt;li&gt;Create a stored procedure (RefreshWorking) that will make use of Origin to refresh Working.&lt;/li&gt;
&lt;li&gt;Create AFTER triggers for all tables referenced by Origin, that will call RefreshWorking.&lt;/li&gt;
&lt;li&gt;Make the triggers intelligent in that they will only call RefreshWorking when the DML operation of the source table would actually affect the outcome of the Origin view.&lt;/li&gt;
&lt;li&gt;Optionally pass the source table name and the key values, through a table valued parameter to RefreshWorking, so that the procedure can more intelligently pick out which parts of Working will need refreshing.&lt;/li&gt;
&lt;li&gt;Create a view, CheckWorkingAndOrigin, that FULL JOIN view Origin and table Working, to ensure that they are identical.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once all of this is done, it is time to test.
For your testing I can highly recommend &lt;a href=&#34;http://tsqlt.org/&#34;&gt;tSQLt&lt;/a&gt;; a completely T-SQL based unit testing system.
When you have assured yourself that RefreshWorking properly updates table Working, it is time for the deployment.&lt;/p&gt;

&lt;p&gt;In one transaction, rename  the original, slow view to Origin, create a synonym with the same name as the original slow view, and point the synonym at the Working table.
As the last step of the transaction, run RefreshWorking procedure so that the Working table will get properly updated and be primed for showtime.&lt;/p&gt;

&lt;h3 id=&#34;reward:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Reward&lt;/h3&gt;

&lt;p&gt;After we implemented this procedure on a heavily queried complex view, we saw a query plan simplification going from
over 70 steps, to only 3 steps. More impressive is that the plan now took 481 times less CPU time!
The RefreshWorking procedure still called the original, slow, complex view, but it did this only when the source tables changed and in particular ways.
The procedure also minimized writes to the Working table, to prevent table locking for the heavy reading on it.&lt;/p&gt;

&lt;h3 id=&#34;summary:0d7721be2247c9da7d04dc0c01e0fcf2&#34;&gt;Summary&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;One way we make sense of the world is by modelling relationships between things as a tree-like hierarchy.&lt;/li&gt;
&lt;li&gt;Most database systems, however, are essentially flat when it comes to the most basic collection of storage; the table.&lt;/li&gt;
&lt;li&gt;We overcome the flatness in one particular way, the simpler  of possible ways, by representing the unbounded nested characteristic of hierarchies by self referencing records in a table.&lt;/li&gt;
&lt;li&gt;Self referencing records in a single table makes manipulation of the hierarchy a very simple operation, but this simplicity comes at a cost when you want to traverse the hierarchy.&lt;/li&gt;
&lt;li&gt;For traversing arbitrarily  deep hierarchies represented by self referencing records, you inevitably require recursion.&lt;/li&gt;
&lt;li&gt;Recursive CTEs break the essential link between the source tables and the view result set, making it impossible for the query planner to do anything but perform the entire process of the view&amp;rsquo;s query, even when you only desire a small subset.&lt;/li&gt;
&lt;li&gt;In the scenario where the source tables for the complex view are written to less than they are read from, you can optimize the complex view by materializing it into a concrete table.&lt;/li&gt;
&lt;li&gt;The materialized table can then be properly indexed for maximum query performance, at a relatively small index maintenance cost at write time.&lt;/li&gt;
&lt;li&gt;With this approach you are trading computation time for storage space.&lt;/li&gt;
&lt;li&gt;The writing of the materialized table happens on DML AFTER triggers, so that you first have the change written to the source tables before the materialized table is updated.&lt;/li&gt;
&lt;li&gt;Updating of the materialized table need not be a complete rewrite; the AFTER triggers can be programmed so that they only fire when columns that partake in the SELECT list for the origin view query change.&lt;/li&gt;
&lt;li&gt;A further optimization can be made where by the refresh stored procedure recieves a list of keys of rows that changed, and can then use this info to only update the materialized table where it actually needs to change.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope that you will try out this procedure on slow views on your databases; it has really helped us a lot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>now make it fast</title>
      <link>https://blog.goodfast.info/post/now-make-it-fast/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://blog.goodfast.info/post/now-make-it-fast/</guid>
      <description>

&lt;p&gt;&amp;ldquo;He began to copy one of our tables, which contained partial user information, including email IDs, hashed passwords, and last tested URL. His copy operation
    locked the database table, which raised alerts on our monitoring system. On receiving the alerts, we checked the logs, saw an unrecognized IP, and blocked
    it right away. In that time, the hacker had been able to retrieve only a
    portion of the data.&amp;rdquo;
    &amp;ndash; From the postmortem of the
    &lt;a href=&#34;http://www.browserstack.com/attack-and-downtime-on-9-November&#34;&gt;Browser Stack hack of 9th November, 2014 at 23:30 GMT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since relational database management systems (RDBMS) have been
&lt;a href=&#34;http://en.wikipedia.org/wiki/Micro_DBMS&#34;&gt;used in production environments since 1970 (Micro DBMS)&lt;/a&gt;, and the theory on which they run was
developed in the preceding decade,and perfected in the three remaining decades
of the previous century (long  ago), it was not surprising  that, as  a subject,
it received very little attention
in our curriculum - at least where I studied.&lt;/p&gt;

&lt;p&gt;To further degrade the already &lt;em&gt;apparently&lt;/em&gt; low relevance  of the poor subject, we
had to cope with a very thick and overly theoretical textbook we could not yet appreciate,
and subscribe to, if you wanted to be cool and smart, to the snooty perception
held by many peers that the lowly database was useful, but entirely boring.&lt;/p&gt;

&lt;p&gt;A database was considered the type of thing  you rigged up for a family member in Access in a
few hours. Any larger system was the domain of &amp;lsquo;informatics&amp;rsquo; - the
less inspired, more practical brother field of study to the sexy &amp;lsquo;computer science&amp;rsquo; with its focus on AI, advanced programming in C++, the  new Java and the emerging world of Linux and Open Source.&lt;/p&gt;

&lt;p&gt;Stepping outside into the real world, and the rest was history - databases
everywhere, for everything!
In my working career since 2001, every single business system I worked on had
a RDBMS - primarily MS SQL Server, and sometimes MySQL or Oracle as the persistence store.
Through the years I accumulated skill in modelling domains and
manipulating the information through SQL, but optimisation of the storage
structures for efficiency was a task I chose to ignore. I reasoned along the
lines of it is the DBA&amp;rsquo;s / RDBMS&amp;rsquo;s work, or
the mostly false assumption that computers are fast and how much data will
the system  realistically have anyways?&lt;/p&gt;

&lt;p&gt;Really? Is that professional, to say this far will I go and no further,
while you as the programmer is directly responsible for those
horribly slow queries?
Yes, the system works, some things are slow, but hey, they have a lot of data
you say, it&amp;rsquo;s going to be slow at times!
Now you are suppressing   that little voice inside of you, quietly telling you it&amp;rsquo;s wrong
aren&amp;rsquo;t you - it &lt;em&gt;can&lt;/em&gt; be faster&amp;hellip;
Think of the waiting user, the wasted time, times hundreds for internal
systems, times thousands for customers, times  hundreds of thousands or even
millions in the case of the web&amp;hellip;
All that wasted time, all that wasted energy, all the trees&amp;hellip;
Don&amp;rsquo;t think about it too much.
You feel pretty bad by now don&amp;rsquo;t you&amp;hellip;&lt;/p&gt;

&lt;p&gt;Well, I have more bad news for you. At the end of this article you are going to feel even worse, because you
are going to see how simple it is to start to turn things around.&lt;/p&gt;

&lt;p&gt;Dramatics aside, we will have a quick look at the  choices  for table
physical layout and ponder the implications on performance.&lt;/p&gt;

&lt;p&gt;Be warned that we will of course only scratch the surface of SQL performance  optimisation
on SQL Server, and that this article is aimed at software developers
with limited skill in this area, so I&amp;rsquo;m going to explain a bit
- i.e. sit down, this might
take a while.&lt;/p&gt;

&lt;p&gt;Many folks have written fine articles on this subject, and I&amp;rsquo;ll refer to
some of those as we go along.&lt;/p&gt;

&lt;p&gt;My primary justification for writing this article is that I&amp;rsquo;d like to cement
this stuff in my mind, and the best way for me is to write it all out. I hope
you can gain something from this also, and please comment if you disagree or
whatever.&lt;/p&gt;

&lt;h2 id=&#34;it-s-out-there-somewhere-but-has-it-order:810306c57b772cfaef22459bbb55c3d7&#34;&gt;It&amp;rsquo;s out there&amp;hellip; Somewhere&amp;hellip; But has it order?&lt;/h2&gt;

&lt;p&gt;Contrary to popular current belief, the data isn&amp;rsquo;t somewhere in a [cloud]
but it actually resides on one or more  physical storage media (think disk
drives), and
importantly, it&amp;rsquo;s laid down either sorted or just as it was received - for all
practical purposes, unsorted.&lt;/p&gt;

&lt;p&gt;You probably knew this already, but I needed that sentence
because it contained the word &lt;em&gt;cloud&lt;/em&gt; so that I had an excuse to quote
Stallman on cloud computing from 2008:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;It&#39;s stupidity. It&#39;s worse than stupidity: it&#39;s a marketing hype campaign,&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Back to tables&amp;hellip; It is probably because SQL Server&amp;rsquo;s default is to choose the
physically sorted way of laying down the data, when you define a primary key on
a new table, that this table organisation
prevails and is common in most systems.
There is nothing like one size fits all and although the
ordered layout of records is a great fit most of the time, it is not the best
layout in all cases (more on that later).
However, keep in mind there is wisdom in the choice of this default none
the less.&lt;/p&gt;

&lt;h2 id=&#34;some-terminology:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Some terminology&lt;/h2&gt;

&lt;p&gt;For the purpose  of the discussion we&amp;rsquo;ll stick to the most widely used
terminology in the SQL Server world and call the ordered layout of records a
&lt;em&gt;clustered index&lt;/em&gt;, and the
layout without any physical ordering a &lt;em&gt;heap table&lt;/em&gt;, or simply a heap.
The structure  existing purely to speed up locating records  we will refer to
as a &lt;em&gt;nonclustered index&lt;/em&gt;, and how the &lt;em&gt;clustered index&lt;/em&gt;
should be ordered we will call the &lt;em&gt;clustering key&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;clustered-index-the-sorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustered index - the sorted one&lt;/h3&gt;

&lt;p&gt;The term clustered index is unfortunate, and probably the reason a
lot of people develop a blurred notion of cluster vs. non clustered, and table
vs. index.
You could think of a &lt;em&gt;clustered index&lt;/em&gt; as a
database table for maintaining data in an ordered fashion (ordered by one or more
columns) thereby giving efficient access to all the data for one record if the
values of some of the sorting columns are known.  Alternatively you could think of a &lt;em&gt;clustered index&lt;/em&gt; as an
database index to
efficiently gain access to more data,
organised by a subset of  all the columns, yet it has all that added
data in itself.
Either way, Viewed as an ordered tabular representation of data, or an index existing for
efficient access to itself - the important point is the physical sorted
layout, and that the most efficient way to retrieve records from it is via the
index that is itself given the primary key.&lt;/p&gt;

&lt;h3 id=&#34;heap-the-unsorted-one:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Heap - the unsorted one&lt;/h3&gt;

&lt;p&gt;A heap is simpler to understand.  Think of a file that grows by repeatedly appending new
lines to it without trying to maintain any kind of order.
Obviously, for efficient access to the data in a heap we need one or more
nonclustered indexes targeting the heap&amp;rsquo;s data, and built up from one or more components
of that data.&lt;/p&gt;

&lt;p&gt;When comparing clustered indexes with heaps we will assume that at least for
the primary key, a nonclustered index is defined on the heap that would make the clustered
index and heap nearly equally  efficient when retrieving a record, given a single primary key value.&lt;/p&gt;

&lt;h3 id=&#34;nonclustered-index-the-real-index:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Nonclustered index - the real index&lt;/h3&gt;

&lt;p&gt;The term &lt;em&gt;nonclustered index&lt;/em&gt; we will use to refer to the structure
that exists primarily to provide   indexed lookup to clustered indexes and heaps
alike.  In SQL Server a heap or clustered index can have many nonclustered
indexes targeting it.&lt;/p&gt;

&lt;h3 id=&#34;clustering-key-how-the-clustered-index-is-clustered-or-sorted:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Clustering key - how the clustered index is clustered, or sorted&lt;/h3&gt;

&lt;p&gt;Apart from the notion of a primary key, for a clustered index, the &lt;em&gt;clustering
key&lt;/em&gt; defines the subset of columns that instruct the system on how to cluster
or sort the records.
A clustering key should be chosen to be as unique  as possible, but uniqueness
is not required (unlike for the primary key),since the system will add four bytes  called the uniquifier to
the clustering key if it is not indicated as being unique already.&lt;/p&gt;

&lt;h2 id=&#34;implications-of-heap-vs-clustered-index-for-dml:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Implications of heap vs. clustered index for DML&lt;/h2&gt;

&lt;p&gt;To compare the efficiency implications of DML statements on these two broad ways of physical data layout we have to look closer
at the nature of the operations we would like to perform.&lt;/p&gt;

&lt;p&gt;It is fairly intuitive  to reason about  this if we keep in mind:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The physical layout of a heap vs. a
clustered index.&lt;/li&gt;
&lt;li&gt;Indexes should stay up to date after the operation completes.&lt;/li&gt;
&lt;li&gt;Is the operation on one or on multiple records.&lt;/li&gt;
&lt;li&gt;What piece of data is required for the fastest retrieval of records.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;the-single-index-primary-key-as-clustering-key-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;The single index, primary key as clustering key scenario.&lt;/h3&gt;

&lt;p&gt;It turns out that it is generally the best choice to use a clustered index
instead of a heap, for the scenario where you need only one index on
one or more indexing columns. For this scenario, a clustered index takes up
less space, performs better overall, and releases space better when records
get deleted.&lt;/p&gt;

&lt;p&gt;The short answer to why this is the case is that for a clustered index, the
data is the index, so lookups on the clustering key finds the relevant records
directly (after climbing the index B-tree), and alterations affecting the clustering key requires alterations to
one structure - the clustered index. For a heap plus one nonclustered index
however, lookups given the index key is a two-step process. First the nonclustered
index is queried to find the RID, the uniquely identifying key for each heap
row corresponding to the clustering key, and
then the RID is used to fetch the record from the heap.
In addition to this, although alterations involving the index key might not require much work on the heap,
the nonclustered index needs to be updated - again a two-step process.&lt;/p&gt;

&lt;p&gt;Nevertheless, there is a great case to be made for choosing a heap over a
clustered index, which we&amp;rsquo;ll get to shortly, but for the scenario as explained
above (one index on the clustering key), a clustered index is the better choice.&lt;/p&gt;

&lt;p&gt;See this &lt;a href=&#34;http://technet.microsoft.com/en-us/library/cc917672.aspx&#34;&gt;Microsoft best practices&lt;/a&gt; white paper, but be
warned that it is proving the superiority of choosing a clustered index in
favor of a heap in a scenario
where a clustered index is the  best choice.
Do not be fooled by this article into thinking heaps are overall inferior to
clustered indexes, as a casual reading might lead you into believing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at the various DML operations quickly on clusterd index and heap&amp;hellip;&lt;/p&gt;

&lt;h4 id=&#34;insert:810306c57b772cfaef22459bbb55c3d7&#34;&gt;INSERT&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;INSERT&lt;/em&gt; into a heap is simply a case of appending where there is space
available in a data page (or creating a new page at the end). After this, a
second write operation is required - insertion of the index key into the
B-tree  of the nonclustered index.&lt;/p&gt;

&lt;p&gt;Now for a clustered index, the new record
must be inserted in the correct location. If this correct location is on a
data page that is full, that page needs to be split in two .
This might be more time-consuming than the simple insert on the heap, but for
a clustered index there is no second write operation to maintain the index -
the data and index are one. Note that this might even turn out to make a
clustered index perform better overall since it only requires one write
operation.&lt;/p&gt;

&lt;p&gt;Any additional nonclustered indexes on a clustered index or heap would take
roughly the same time to update or keep in sync with the actual data, so we
can ignore their maintenance penalty when comparing.&lt;/p&gt;

&lt;p&gt;But surely we should be able to gain performance with inserts by choosing
either of the two table layouts,   given that heaps
and clustered indexes differ so fundamentally?
In this particular scenario they are  equally good because both
options still require that indexes be kept up to date. If we were to choose a
heap and define absolutely no nonclustered index on it we will gain the fastest
insert performance since inserting would simply be adding data on anywhere
where there is space.&lt;/p&gt;

&lt;p&gt;There is actually a great use case for this: logs and other archival type of
storage that do not require immediate querying.&lt;/p&gt;

&lt;p&gt;But optimisation is a tricky problem, since even for this use case, if many
concurrent inserts are expected, it might   very well be better to choose a
clustered index instead, on some non unique but range-like clustering key that
will jump around a bit, to
achieve an effect of inserting in different locations to prevent everything
from trying to insert in the same data page all the time (as would be the
case for a heap).&lt;/p&gt;

&lt;h4 id=&#34;select-update-and-delete:810306c57b772cfaef22459bbb55c3d7&#34;&gt;SELECT, UPDATE and DELETE&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;SELECT&lt;/em&gt;, &lt;em&gt;UPDATE&lt;/em&gt; and &lt;em&gt;DELETE&lt;/em&gt; first requires locating where the records to
operate on need to be physically
found, followed by the actual action on the data.&lt;/p&gt;

&lt;p&gt;Assuming that the operation simply applies to data given a single primary key
value that is the clustering key, the finding
or first part of this operation
is slightly  less efficient for a heap compared to  a clustered index. For
a clustered index, after the tree is climbed the information is there and
ready to be retrieved, while for a heap, after climbing the tree of the
nonclustered index, you only get the RID, and then require a second
operation to (all be it directly) get at the data in the heap - one additional
level of indirection.&lt;/p&gt;

&lt;p&gt;There is an option to &lt;em&gt;include&lt;/em&gt; columns of the heap or clustered index in the
nonclustered indexes. The effect of this is that, after climbing the tree of
the nonclustered index, those &lt;em&gt;included&lt;/em&gt; column&amp;rsquo;s data is immediately
available - a mini clustered index in the form of a nonclustered index with
included columns. All very straight forward and unambiguous  wouldn&amp;rsquo;t you say?&lt;/p&gt;

&lt;p&gt;Except for &lt;em&gt;SELECT&lt;/em&gt;, the efficiency of the second part, the action part of the operation
can vary much more between heap and clustered index.
For &lt;em&gt;UPDATE&lt;/em&gt;, if the column being updated happens to be one or more of the
columns comprising the clustering key, and the table is a clustered index, then
in order to keep the data  sorted, the system might have to
do page splits.
This in turn mean that potentially, large amounts of data need to be copied around.
For a heap this is never the case, and the columns  can simply be updated in
place - order is of no importance.&lt;/p&gt;

&lt;p&gt;For both heap and clustered index, if any of the columns are part of any
defined nonclustered indexes then altering them might have nonclustered index
maintenance time as a further performance penalty.&lt;/p&gt;

&lt;p&gt;Fortunately, for both heap and clustered index, if the columns being updated are not part
of the clustering key the efficiency of the action part of the operation is nearly similar.&lt;/p&gt;

&lt;p&gt;Performing a &lt;em&gt;DELETE&lt;/em&gt; on a heap or clustered index should be simply a case of marking that record
as deleted and making the space available for potential future inserts. For a
clustered index, no index maintenance is yet again needed while for the heap,
the nonclustered index needs updating.&lt;/p&gt;

&lt;h2 id=&#34;welcome-to-the-real-world-where-tree-climbing-is-to-be-avoided-the-multiple-indexes-scenario:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Welcome to the real world, where tree climbing is to be avoided - the multiple indexes scenario&lt;/h2&gt;

&lt;p&gt;The single index, primary key as clustering key and lookup scenario described earlier
might appear  early on, and a lot in most models, but very soon you
will also want to efficiently query on other columns on wider  (more columns)
and deeper (more rows) tables.&lt;/p&gt;

&lt;p&gt;To prevent full table scans, you start adding nonclustered indexes,
and this is where heaps start to become the more attractive alternative.&lt;/p&gt;

&lt;p&gt;Suppose for a moment that your  table (let&amp;rsquo;s call it table &lt;em&gt;T&lt;/em&gt;) that became wide and deep overnight is a
clustered index, and the primary key (&lt;em&gt;K&lt;/em&gt;) is also the clustering key.&lt;/p&gt;

&lt;p&gt;Each additional nonclustered index (&lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip;) on &lt;em&gt;T&lt;/em&gt; will store in its leaf nodes the
values of &lt;em&gt;K&lt;/em&gt;.
This means that a query on &lt;em&gt;T&lt;/em&gt; utilising  some nonclustered index &lt;em&gt;N&lt;/em&gt;  results
in a tree climb of &lt;em&gt;N&lt;/em&gt; that yields some value of &lt;em&gt;K&lt;/em&gt;. Following this we
require another tree climb of the clustered index that
is &lt;em&gt;T&lt;/em&gt;, given a value for &lt;em&gt;K&lt;/em&gt;, and only then is the actual data reached.&lt;/p&gt;

&lt;p&gt;On the other hand, suppose  now that your wide and deep table  &lt;em&gt;H&lt;/em&gt; is a heap
instead, with one or more nonclustered indexes &lt;em&gt;N1&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt;, &amp;hellip; and so on.
This time, each nonclustered index &lt;em&gt;N&lt;/em&gt; will store at the leaf node the RID
of the relevant row, and not simply  yet another key into a further index.
This means that if some query on &lt;em&gt;H&lt;/em&gt; utilise one of the nonclustered indexes,
only one tree climb of that nonclustered index is required, after which the
RID is obtained, and unlike a clustering key, a RID represents the
physical position of the record in the heap, and thus can be directly accessed
- no further tree climbing required.&lt;/p&gt;

&lt;p&gt;For a more in-depth look at this, and some hard numbers comprising a
compelling case,  do yourself a favor and read Markus
Wienand&amp;rsquo;s fine article,
&lt;a href=&#34;http://use-the-index-luke.com/blog/2014-01/unreasonable-defaults-primary-key-clustering-key&#34;&gt;Unreasonable Defaults: Primary Key as Clustering
Key&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary-and-closing-thoughts-optimising-performance-is-an-interesting-and-very-relevant-problem:810306c57b772cfaef22459bbb55c3d7&#34;&gt;Summary and closing thoughts - Optimising performance is an  interesting and very relevant problem&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;Premature optimisation is the root of all evil&amp;quot;
 -- Donald E. Knuth
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As much as I concur with that statement, especially how it applies to code, I do think that a good understanding
of the options available to you when turning a data model into an actual
database schema can proactively prevent vicious      cycles of poor performing
monster database servers.
Yes, there is a lot of things one can do, and the precise case where one
technique or option would be the better option is hard to identify, but the
better your understanding of the internals, the more likely you are to get it
right first time,
and the more it will start to happen that you are writing a query and you
suddenly realise that a specific index would benefit that query tremendously.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve rambled a bit, but to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are two main choices for the physical layout of data for tables.&lt;/li&gt;
&lt;li&gt;If only one index on the primary key is required it&amp;rsquo;s probably the best to choose a clustered index.&lt;/li&gt;
&lt;li&gt;For a many index scenario choose a heap.&lt;/li&gt;
&lt;li&gt;For best insert performance on high  loads choose a heap with no nonclustered indexes.&lt;/li&gt;
&lt;li&gt;Use the include columns feature of nonclustered indexes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Today there are exciting new alternative data storage technologies like fully
in memory databases, distributed systems such  as Hadoop, Google&amp;rsquo;s BigTable
approach, and document oriented noSQL
options such as ElasticSearch to name only a very few.
These alternative solutions to the problem of working with large data sets
have and will continue to be applied more and more, but if the last decade is
anything to go by,  the relational database is going to stick around for quite
some time still, so investing time into learning how to optimise it is
time well spent.&lt;/p&gt;

&lt;p&gt;The reason for SQL systems remaining central to all serious data storage
applications  is not by accident. There is a theoretical reason, routed in the
so-called CAP theorem.
For an overview of how the CAP theorem restricted the growth and adoption of
noSQL systems, have a look at
&lt;a href=&#34;http://use-the-index-luke.com/blog/2013-04/whats-left-of-nosql&#34;&gt;Whats left of NoSQL?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An interesting response to this is FoundationDB (see
&lt;a href=&#34;http://www.theregister.co.uk/Print/2012/11/22/foundationdb_fear_of_cap_theorem/&#34;&gt;NoSQL&amp;rsquo;s CAP theorem busters: We don&amp;rsquo;t drop ACID&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;An exciting emerging trend is to harness the strengths of both the traditional
RDBMS and the more recent big data distributed, more normalised data storage
technologies. For an interesting application of this, see
&lt;a href=&#34;http://msdn.microsoft.com/en-gb/magazine/dn802606.aspx&#34;&gt;Use Updatable Tables for Responsive Real-Time Reporting&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I hope this short info burst tickled your interest enough that you will go
ahead and look into all of it a bit more.
Personally I have been pleasantly surprised by the depth of this subject area.&lt;/p&gt;

&lt;p&gt;I can highly recommend the book
&lt;a href=&#34;http://sql-performance-explained.com/?utm_source=UTIL&amp;amp;utm_medium=main&amp;amp;utm_campaign=second&#34;&gt;SQL Performance Optimisation&lt;/a&gt; for an in-depth look at this
subject, and the
&lt;a href=&#34;http://use-the-index-luke.com/&#34;&gt;Use The Index Luke&lt;/a&gt; site.&lt;/p&gt;

&lt;p&gt;This article also appears on &lt;a href=&#34;http://www.inivit.com/blog/&#34;&gt;Inivit&amp;rsquo;s blog&lt;/a&gt; along with some other fine posts from former colleagues.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>